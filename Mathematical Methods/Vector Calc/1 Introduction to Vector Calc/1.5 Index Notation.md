## [[1.5 Index Notation]]

Einstein Index Notation is heavily linked to the study of Tensors, but an adaption of it can make equations much more concise, outside of its rigorous definition in Tensor Algebra.

For this we will use a suffix **Latin letter** index on the complicated variable in question. This is a very useful way to work with every element together, for example.

The scalar dot product,

$$
\bolda + \boldb = a_ib_i = a_1b_1+a_2b_2+a_3b_3,
$$

Matrix multiplication,

$$
AB = A_{ij}B_{ji}.
$$

Whenever a index appears twice in the same term, we implicitly sum the term over the extent of that index.

However **the same index cannot appear three times in the same term**! We do not have a meaning for $a_{jj}b_j$ whereas we do for $a_{ij}b_j$ (this is the $i^\mathrm {th}$ entry of the matrix-vector product $A\boldb$) and we do for $a_{jj}b_i$ - this means: sum over the diagonal elements of the matrix $A$ to form the scalar quantity $\mathrm {trace}(A)$ and then multiply the vector $\boldb$ by this scalar quantity. So the result is (the $i^\mathrm {th}$ component of) a vector:

$$
a_{jj}b_i
= (a_{11} + a_{22} + a_{33}) b_i
= \big ((a_{11} + a_{22} + a_{33}) \boldb \big )_i
= ( \text {trace}(A)\boldb )_i.
$$

For any vector $\boldx$ we will denote its components by $x_i$, and matrices $A$ will have entries (components) $a_{ij}$, with the first index labelling the rows and the second index labelling columns in the usual way. The transpose $A^T$ of a matrix $A$ therefore has $ij^{\mathrm {th}}$ entry $(A^T)_{ij}=(A)_{ji}=a_{ji}$.

We will make use of two special pieces of notation:

### [[Kronecker Delta]]

The ‘Kronecker delta’ symbol $\delta _{ij}$ which can be defined straightforwardly by

$$
\delta_{ij} = \begin{cases}
1 &\text { if } i = j  \\
0 &\text { if } i \ne j\\
\end{cases}
$$

So, for example, $\delta_{ij} a_j = a_i$ and $\delta_{jj} = \delta_{ii} = 3$ - the sum of the diagonal entries which are all 1s.

This is very useful in "picking" an element from a list and could be considered the discrete form of the [[Dirac Delta]].

### [[Levi-Civita Symbol]] aka The _Alternating Tensor_ 

The [[Levi-Civita Symbol]] defined as,

$$
\delta_{ij} = \begin{cases}
+1 &\text { if } ijk \text { is one of 123, 231, 312} \\
-1 & \quad \text { if } ijk \text { is one of 321, 132, 213} \\
\phantom {+}0 & \quad \text { otherwise, i.e. if two index letters are the same. }
\end{cases}
$$

This behaviour shown graphically as,

![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Permutation_indices_3d_numerical.svg/420px-Permutation_indices_3d_numerical.svg.png)

> This is closely related the handedness of coordinate systems and the sign of permutations.
> 
> In fact this final one is the generalised definition of the symbol in higher dimensions. [Wikipedia](https://en.wikipedia.org/wiki/Levi-Civita_symbol#Generalization_to_n_dimensions).

This enables us to write determinants, and therefore vector products in a very compact way, for example

$$
\mathrm {det}(A) = \epsilon _{ijk} \, a_{1i} \, a_{2j} \, a_{3k}.
$$

Note that each index $i$, $j$, $k$ appears exactly twice, meaning that we are asked to sum over all three indices and we therefore get a scalar quantity (which the determinant is). Although this is a sum of 27 separate terms, from the definition of $\epsilon _{ijk}$ only six of them can be non-zero. Check that this gives the result you expect.

Whereas $\delta _{ij}$ is symmetric, i.e. $\delta _{ij}=\delta _{ji}$, we can see from the definition directly that $\epsilon _{ijk}$ is anti-symmetric in each pair of indices, i.e. exchanging any pair of $i,j$ and $k$ introduces a minus sign, e.g. $\epsilon _{jik}=-\epsilon _{ijk}$. Finally we note the very useful relationship between $\delta _{ij}$ and $\epsilon _{ijk}$:

$$
\epsilon_{ijk} \, \epsilon_{i\ell m} 
=
\delta_{j\ell} \, \delta_{km} - \delta_{jm} \, \delta_{k\ell}
$$

which allows us to simplify multiple vector products, as well as other expressions we’ll meet in a few lectures’ time.