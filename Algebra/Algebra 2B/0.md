algebranotes.ac
===============

#### 1 Rings

##### Groups and rings

**1.1** One way to look at a group – not the only way, and not always the best way – is to say that it is a set where you are allowed to mutiply elements together. There is more to it than that, but that’s a starting point. The corresponding basic informal idea of a ring is that in a ring you are allowed to multiply and you are allowed to add. Again, there’s more to it than that. So we begin by going back a step and looking at groups first. We start with a few formal definitions.

**1.2 Definition.** A _binary operation_ on a set $S$ is a function

$$ *\colon S\times S\to S. $$

Binary operations that we are interested in tend to be called addition, denoted by $+$, or multiplication, denoted by either $\cdot$ or nothing at all. In this context, by ancient tradition one writes $a*b$ rather than $*(a,b)$ (Polish notation): thus $7+6$ rather than $+(7,6)$ and $a\cdot b$ or $ab$ rather than $\cdot (a,b)$.

**1.3 Definition.** A _group_ is a pair $(G,*)$, where $G$ is a set, $*$ is a binary operation on $G$ and the following axioms hold:

*   • (associative) $(a*b)*c=a*(b*c)$ for all $a,\,b,\,c\in G$.
    
*   • (identity) There is an element $e\in G$ with the property that $e*a=a=a*e$ for all $a\in G$.
    
*   • (inverse) For each $a\in G$ there exists $b\in G$ such that $a*b=b*a=e$.
    

Often, in fact usually, we know what the group operation $*$ is, and then we don’t need to mention it and we just refer to the group as $G$ rather as $(G,*)$.

**1.4** In a group, both the identity element and inverses are unique.

*   (i) if $e, e’\in G$ are two elements satisfying the identity property from I.3, then $e*e’=e’$ because $e$ is an identity and $e*e’=e$ because $e’$ is. So $e=e’$.
    
*   (ii) Given $a\in G$, if $b, b’\in G$ are both elements satisfying the inverse property for $a$ in I.3, then
    
    $$ b=b*e=b*(a*b’)=(b*a)*b’=e*b’=b’. $$
    

This unique element $b$ from (ii) is called the _inverse_ of $a$. It is often denoted by $a^{-1}$, especially if $*$ has been denoted by $\cdot$.

**1.5** A neat way to say this is that a group has three operations: there is a binary operation (usually called _the_ group operation), $*$, which takes two elements of $G$ and gives you a third one; a unary operation, ${}^{-1}$, which takes one element of $G$ and gives you a second one; and there is a nullary operation, $e$, which takes no element of $G$ and gives you a first one. Then there are the group axioms, which tells you how these operations interact.

**1.6 Definition.** A group $(G,*)$ is _abelian_ (or _commutative_) if $a*b=b*a$ for all $a,\,b\in G$.

The binary operation in an abelian group is often written as $+$, in which case the identity element is denoted $0$ and the inverse of an element $a\in G$ is denoted $-a$.

**1.7 Definition.** A subset $H$ of a group $G$ is called a _subgroup_ of $G$ if and only if it is closed under all the group operations. That means that $1\in H$ (so, in particular, $H\neq \varnothing$); that $a^{-1}\in H$ if $a\in H$; and that $a*b\in H$ if $a,\,b\in H$.

In other words, $H$ is a subgroup if it is a subset of $G$ that is also a group (with the same operation $*$).

**1.8** If we have been given a non-empty subset $H\subseteq G$ then we can check whether it is a subgroup in one shot: $H$ is a subgroup if and only if

$$ \forall \; a, b\in H, \text { we have } a*b^{-1}\in H. $$

To see that this works, suppose we have this condition. First choose $a\in H$ and take $b=a$. That tells us that $1\in H$, so now we can start again and take $a=1$. That tells us that if $b\in H$ then $b^{-1}\in H$. Finally, if we want to show that $a*b\in H$ we take $c=b^{-1}$ and then the criterion tells us that $H\ni a*c^{-1}=a*b$.

In the other direction, if $H$ is a subgroup and $a,\,b\in H$ then $b^{-1}\in H$ and hence $a*b^{-1}\in H$, by I.3.

**1.9** Groups are symmetries. There may be other ways of looking at particular ones, but all groups are symmetry groups of something. It isn’t so easy to give an overarching description of a ring. The basic idea of a ring is that you are allowed to add and subtract and multiply, but not necessarily divide: addition is commutative, but multiplication might not be. Then there have to be rules about how multiplication and addition interact. So, looking at I.5, we expect two binary operations, one unary operation (subtraction, or rather minus), and one or two nullary operations ($0$ and, optionally, $1$).

**1.10** The basic example of some things that you can add and multiply by not necessarily divide is the integers. You can’t expect to be able to divide an integer by $2$, not if you want the answer to be an integer. So $\ZZ$ should be one basic example of a ring. In that case multiplication is commutative. Another good example is the ring of polynomials with (say) real coefficients in one variable, $\RR [t]$. You can add two of these and you can multiply them, but you can’t divide: $\frac {1}{t}$ isn’t a polynomial. In fact, these two turn out to have much more in common than just being rings: they are very similar rings. In other words, a polynomial may be nothing like an integer, but polynomials collectively behave remarkably like integers collectively.

If you want an example of a noncommutative ring, consider the set of (say) $2\times 2$ (say) real matrices, $M_2(\RR )$. Here you have addition and multiplication, and you can take inverses as long as the determinant isn’t zero; but sometimes it is zero.

##### Basics of rings

**1.11 Definition.** A _ring_ is a triple $(R,+,\cdot )$, where $R$ is a set with two binary operations called addition (denoted $+$) and multiplication (denoted $\cdot$, or just nothing), such that the following axioms hold:

*   (i) (addition) $(R,+)$ is an abelian group.
    
*   (ii) (associative) The binary operation $\cdot$ is associative.
    
*   (iii) (distributive) Multiplication is _distributive over addition_, from the left or the right: that is
    
    $\seteqnumber{0}{}{0}$
    
    $$ \begin{align*} a\cdot (b+c) & = (a\cdot b)+(a\cdot c) & \text {for all}\,\,a,\,b,\,c\in R; \\ (b+c)\cdot a & = (b\cdot a)+(c\cdot a) & \text {for all}\,\,a,\,b,\,c\in R. \end{align*} $$
    
*   (iv) (identity) There is a _multiplicative identity_, an element $1\in R$ such that $1\cdot a=a\cdot 1=a$ for all $a\in R$.
    

We write $0$ for the (unique) additive identity, and $-a$ for the (unique) additive inverse of $a\in R$.

**1.12** The last axiom (identity) in I.11 is in a sense optional. Many writers omit it, and allow rings without $1$. The disadvantage is that when you want to talk about a ring with a $1$ (also called a unital ring), you have to say so; and that happens a lot, so you would rather keep the simpler phrasing for the commonest case. Here we have made the opposite decision, and the disadvantage is that we now have no name for a ring without a $1$, i.e. something that satisfies I.11(i)–(iii) but not necessarily I.11(iv).

**1.13** We often omit $\cdot$ and write $ab$ instead of $a\cdot b$. Sometimes we will need to wite $0_R$ and $1_R$ for the identities in $R$, to distinguish them from identities in other rings or groups.

As with groups, we will very often know what the operations are and just talk about the ring $R$, rather than calling it $(R,+,\cdot )$.

For simplicity we often avoid brackets when there is no ambiguity. Here the same conventions hold as for real numbers: $\cdot$ has priority over $+$. For example $ab+ac$ stands for $(a\cdot b)+(a\cdot c)$ and not $(a\cdot (b+a))\cdot c$. One also writes $a^2$ for $a\cdot a$ and (see II.22 below) $2a$ for $a+a$ and so on.

**1.14 Lemma.** In any ring $(R,+,\cdot )$, we have

*   (i) $a\cdot 0=0$ and $0 = 0\cdot a$ for all $a\in R$; and
    
*   (ii) $a\cdot (-b)=-(a\cdot b)$ and $-(a\cdot b) = (-a)\cdot b$ for all $a, b\in R$.
    

Proof: For (i), let $a\in R$. Since $0$ is an additive identity, one of the distributive laws gives

$$ a\cdot 0 = a\cdot (0+0) = a\cdot 0 + a\cdot 0. $$

Adding $-(a\cdot 0)$ on the left on both sides gives

$$ -(a\cdot 0)+a\cdot 0 = -(a\cdot 0) + a\cdot 0+a\cdot 0. $$

The left hand side is zero, and the additive associativity gives that the right hand side is

$$ (-(a\cdot 0)+a\cdot 0)+a\cdot 0= 0 + a\cdot 0 = a\cdot 0 $$

as required. The second identity is similar.

To prove (ii), note that

$$ a\cdot b+a\cdot (-b)=a\cdot (b+(-b))=a\cdot 0=0. $$

This means that $a\cdot (-b)$ is the additive inverse of $ab$: that is, $a\cdot (-b)=-(a\cdot b)$. Again, the second identity is similar.

**1.15 Definition.** Let $R$ be a ring. An element $a\in R$ is called a _unit_ if it has a multiplicative inverse: i.e. if there exists $b\in R$ such that $a\cdot b=b\cdot a=1$.

**1.16 Lemma.** Let $R$ be a ring. Then the multiplicative identity is unique, and if $a\in R$ is a unit then the multiplicative inverse of $a$ is unique.

Proof: The same argument as in I.4 still works.

**1.17** Notice that if $R$ is a ring then $(R,\cdot )$ is _never_ a group, except in the trivial case where $R=\{0\}$ and $0=1$. This is because $0$ cannot possibly have a multiplicative inverse, because of I.14(i).

The set of units in $R$ is denoted by $R^*$ or sometimes $R^\times$. It is easy to see that $R^*$ does form a group under multiplication (but it behaves very badly under addition: the sum of two units does not have to be a unit): in fact, there is nothing to check because associativity and identity are already in the ring axioms.

##### Examples and types of rings

**1.18 Example.** Some examples of rings and their units are:

*   (i) $R=\ZZ$; then $\ZZ ^*=\{\pm 1\}$.
    
*   (ii) $R=\CC$; then $\CC ^*=\{z\in \CC \mid z\neq 0\}$.
    
*   (iii) $R=M_2(\RR )$; then $R^*=\{A\in M_2(\RR )\mid \det A\neq 0\}$.
    

Notice how different these three are. In (i) there are very few units, but more than just $1$. In (ii) everything is a unit, except for $0$ of course. In (iii) most elements are units, but there are still exceptions.

**1.19 Definition.** A ring $R$ is a _commutative ring_ if $a\cdot b=b\cdot a$ for all $a,\,b\in R$.

One does not say “abelian ring”! In contexts where only commutative rings are important, one sometimes drops the word “commutative”, and explicitly says “noncommutative ring” where necessary, but that will not be done here.

**1.20 Definition.** A ring $R$ is an _integral domain_ if it is a commutative ring in which $0\neq 1$, such that if $a,\,b\in R$ and $ab=0$, then $a=0$ or $b=0$.

Sometimes an integral domain is just called a domain.

**1.21 Definition.** A ring $R$ is a _division ring_ if $0\neq 1$, and every non-zero element is a unit.

That is, $a^{-1}$ exists as long as $a\neq 0$. Note that a division ring is not required to be commutative.

**1.22 Definition.** $R$ is a _field_ if it is a commutative division ring.

**1.23** Informally, in a field you can add and multiply and divide by anything that isn’t actually zero, and mutiplication is commutative.

Fields are often called $K$ (German Körper). Every field $K$ is an integral domain: if $a,\, b\in K$ and $ab=0$, then if $a\neq 0$ we have $b=1\cdot b = a^{-1}ab=a^{-1}\cdot 0=0$.

**1.24 Example.** We start with a few familiar examples.

*   (i) Every field is a commutative ring. In particular $\QQ$, $\RR$, and $\CC$ are commutative rings.
    
*   (ii) Division rings need not be commutative, so division rings need not be fields.
    
*   (iii) The ring $\ZZ$ is an integral domain (that’s why they are called “integral”), but it is not a division ring, so it is not a field.
    
*   (iv) The set $\RR [t]$ of polynomials in one variable with real coefficients is also an integral domain, but not a field.
    
*   (v) The commutative ring $\ZZ /4\ZZ$ consists of the integers with arithmetic mod 4. It is a commutative ring, but it is not an integral domain because $2\times 2=4=0$ but $2\neq 0$.
    
*   (vi) On the other hand, $\ZZ /5\ZZ$ is a field; in fact $\ZZ /p\ZZ$ is a field, called $\FF _p$, whenever $p$ is prime.
    

**1.25** There is also a field with $4$ elements, called $\FF _4$, which we shall meet later but it is not the same as $\ZZ /4\ZZ$. They are both commutative rings and they both have four elements, but they are otherwise different.

**1.26 Example.** For any ring $R$, let $M_n(R)$ denote the set of all $n\times n$ matrices with coefficients in the ring $R$. Then $M_n(R)$ is a ring with respect to usual addition and multiplication of square matrices. However, even if $R$ is a commutative ring, $M_n$ is not commutative, unless $n=1$ (when it is just $R$ in a very light disguise).

We saw this ring in I.18(iii) in the case $R=\RR$.

**1.27 Example.** Again take $R$ to be a ring and $t$ to be a variable, but now consider the set $R[[t]]$ of _formal power series_:

$$ R[[t]]=\{f=\sum _{k=0}^\infty a_kt^k\mid a_k\in R\}. $$

We do not care whether these converge (that is the meaning of the word “formal”). Indeed, it makes no sense to ask whether they converge, because $R$ might be any ring at all: $M_5(\ZZ /26\ZZ [s])$, for example. There is no sense in which two such things can be said to be close to one another, so there is no notion of convergence. If it happens that, say, $R=\CC$, then we could ask whether they converge, but we don’t. It doesn’t matter, because we never assign an actual value to $t$.

The element $a_k\in R$ is called the _coefficient of $t^k$ in $f$_.

$R[[t]]$ is a ring: addition and multiplication of power series are defined as usual by

$\seteqnumber{0}{}{0}$

$$ \begin{eqnarray*} \sum _{k=0}^\infty a_kt^k+\sum _{k=0}^\infty b_kt^k&= & \sum _{k=0}^\infty (a_k+b_k)t^k \\ \left (\sum _{k=0}^\infty a_kt^k\right )\cdot \left (\sum _{k=0}^\infty b_kt^k\right ) & = & \sum _{k=0}^\infty \left (\sum _{i+j=k}a_ib_j\right ) t^k\\ &=& a_0b_0 + (a_1b_0+a_0b_1)t + (a_2b_0+a_1b_1+a_0b_2)t^2 + \cdots \end{eqnarray*} $$

As $R$ is an abelian group with respect to the ring addition it follows readily that $(R[[t]],+)$ is an abelian group in which $0 = 0+0t+0t^2+\cdots$ is the zero element. To see that $(R[[t]],+,\cdot )$ is a ring, it remains to see that the multiplication is associative and that the distributive laws hold. For this, let

$$ f= \sum _{k=0}^\infty a_kt^k, \quad g=\sum _{k=0}^\infty b_kt^k,\quad h=\sum _{k=0}^\infty c_kt^k $$

be power series. The coefficient of $t^n$ in the product $(fg)h$ is

$$ \sum _{i+j+k=n}(a_ib_j)c_k $$

which (as multiplication in $R$ is associative) is the same as

$$ \sum _{i+j+k=n}a_i(b_jc_k), $$

which is the coefficient of $t^n$ in $f(gh)$. It follows that $(fg)h=f(gh)$, so multiplication in $R[[t]]$ is associative. Finally we check the distributive laws. The coefficent of $t^n$ in $f(g+h)$ is

$$ \sum _{i+j=n}a_i(b_j+c_j)=\sum _{i+j=n}a_ib_j+\sum _{i+j=n}a_ic_j $$

which equals the coefficient of $t^n$ in $fg+fh$, so $f(g+h)=fg+fh$. Similary one proves that $(g+h)f=gf+hf$. This completes the proof that $(R[t],+,\cdot )$ is a ring.

Notice that we did not need $R$ to be a commutative ring. It is easy to check that $R[t]$ is commutative if and only if $R$ is commutative.

**1.28 Example.** Let $R$ be a ring and let $t$ be a variable. Let $d\ge 0$ be a non-negative integer. A _polynomial $f$ over $R$ of degree $d$_ is a formal expression

$$ f=\sum _{k=0}^d a_kt^k = a_0+a_1t+a_2t^2+a_3t^3 + \cdots +a_dt^d, $$

with $a_k\in R$ for $0\leq k\leq d$ and $a_d\neq 0$.

We let $R[t]_d$ denote the set of all polynomials of degree $d$, and we set

$$ R[t]=\{0\}\cup \bigcup _{d=0}^\infty R[t]_d. $$

Thus $1+t$, $39-62t+3t^{19}$ and $94$ are all elements of $\ZZ [t]$, and so is $0$, but $\frac {1}{t}$ is not and neither is $e^t$.

The degree of a polynomial is the highest $k$ such that $a_k\neq 0$, i.e. the highest power of $t$ that actually occurs: a polynomial of degree $0$ is called a non-zero constant (it is just a non-zero element of $R$). The degree of the polynomial $0$ is not defined – sometimes one can save some writing by declaring that it is $0$, or that it is $-1$ or even $-\infty$, but only temporarily. $0$ is also called a constant, so the constants are just the elements of $R$.

A polynomial is just a special kind of formal power series, one for which all the coefficients of index $>d$ are zero, so we have defined addition and multiplication of formal power series exactly as we did for polynomials in I.27. The only difference being that we may replace the sums to $\infty$ by sums to some finite number. It follows immediately that $R[t]$ is a ring.

**1.29** Two formal power series $\sum _{k=0}^\infty a_kt^k$ and $\sum _{k=0}^\infty b_kt^k$ coincide if and only if $(a_k)=(b_k)$: the variable $t$ is superfluous. We could forget it and simply write the list of coefficients instead. But in fact one more often does the opposite: given an ordered list of ring elements (say numbers) it is often useful to organise them into a power series. This is especially common in probability, where it is called a generating function.

##### Subrings

**1.30 Definition.** A nonempty subset $S$ of a ring $R$ is called a _subring_ if and only if, for any $a,\,b\in S$ we have both $a-b\in S$ and $ab\in S$.

**1.31 Lemma.** Let $S$ be a subset of a ring $(R,+,\cdot )$. Then $S$ is a subring of $R$ if and only if $(S,+,\cdot )$ is satisfies I.11(i)–(iii) of a ring.

In other words, a subring is a subset that happens to be a ring, but not necessarily with a $1$. This is an inconvenience of our convention that “ring” means “unital ring”, but it is what we want.

**1.32 Example.** We have seen several examples of subrings already.

*   (i) For any ring $R$, both $\{0\}$ and $R$ are subrings of $R$.
    
*   (ii) The ring $\ZZ$ is a subring of $\QQ$ which is a subring of $\RR$ which is a subring of $\CC$, under the usual operations of addition and multiplication.
    
*   (iii) The even integers $2\ZZ$ are a subring of $\ZZ$. By Lemma I.31 the even integers therefore form a ring but not necessarily with a $1$: and indeed $1$ is odd. (But $0$ is even, a fact that seems to be poorly understood.) So a subring does not have to be a ring in the full sense of satisfying I.11(iv) as well.
    
*   (iv) If $R$ is any ring and $t$ is a variable, then $R$ is a subring of $R[t]$ which is a subring of $R[[t]]$. In this case, the subrings really are rings.
    
*   (v) The Gaussian integers $\ZZ [i]=\{a+bi \in \CC \mid a,\, b\in \ZZ \}$ form a subring of the field $\CC$: also $1=1+0i\in \ZZ [i]$, so $\ZZ [i]$ is a ring.
    

The notation $\ZZ [i]$ is not incompatible with the notation for polynomial rings: unlike $t$, $i$ is not a variable, but in any case,

$$ \sum _{k=0}^da_k i^k = \left (\sum _{l=0}^{\lceil d/4\rceil }(a_{4l}-a_{4l+2})\right )+i \left (\sum _{l=0}^{\lceil d/4\rceil }(a_{4l+1}-a_{4l+3})\right ) \in \ZZ [i]. $$

**1.33 Lemma.** If a subring $S$ of an integral domain $R$ contains the element $1\in R$, then $S$ is an integral domain.

##### Ideals and quotient rings

**1.34** We begin with a bit of set theory. Let $X$ be a set: recall that a _relation_ $\sim$ on $X$ is simply a subset $\Sigma \subset X\times X$ and we write $x\sim y$ if (and only if) $(x,y)\in \Sigma$. An _equivalence_ relation on $X$ is a relation $\sim$ that is reflexive, symmetric and transitive, and in that case the _equivalence class_ of an element $x\in X$ is the (nonempty) set $[x]=\{y\in X \mid x\sim y\}$ of elements that are equivalent to $x$. Every element lies in a unique equivalence class, and any two distinct equivalences classes are disjoint subsets of $X$; we say that the equivalence classes _partition_ the set $X$. The set of equivalence classes $\{[x]\mid x\in X\}$ is denoted $X/{\sim }$.

If $X$ has some extra structure, such as being a group or a ring, we cannot expect $X/{\sim }$ to inherit that structure unless $\sim$ satisfies some extra conditions. If $X=G$ is a group, the condition is that $\sim$ should be given by a normal subgroup $H\vartriangleleft G$, so $x\sim y\iff xH=yH$. The next step is to work out what the corresponding thing is for rings. Fortunately, we already know some examples.

**1.35 Example.** For any $n\in \NN$, consider the subset $n\ZZ = \{nm\in \ZZ \mid m\in \ZZ \}$ of integers that are divisible by $n$. There is an equivalence relation $\sim _n$ on $\ZZ$ defined by

$$ a\sim b \iff n \vert (b-a) \iff b-a\in n\ZZ . $$

Any integer $m$ can be written in the form $m=qn+r$ for a unique $0\leq r<n$, in which case $[m]=[r]$. Therefore the set $\ZZ /{\sim }$ of equivalence (or _congruence_) classes is simply $\{[0],[1],\ldots ,[n-1]\}$.

We will denote this set by $\ZZ /n\ZZ$ (or $\ZZ /n$). We can define addition and multiplication in $\ZZ /n\ZZ$ by

$$ [a]+[b]:= [a+b]\quad \text { and }\quad [a]\cdot [b]:= [a\cdot b]. $$

This says simply that we add and multiply the representatives $a$ and $b$ in $\ZZ$, and then take the equivalence class of the result. This is the familiar modular arithmetic: if we know that we are working in $n\ZZ$ we may as well drop the square brackets and use $a$ instead of $[a]$, just remembering that $a$ and $a+n$ then become different names for the same thing.

With this definition, $\ZZ /n\ZZ$ becomes a ring: if $n$ is prime, it is actually a field. We can allow the case $n=0$, in which case $\sim$ is trivial and the new ring is just $\ZZ$ again, and we can allow $n<0$ because $n\ZZ =(-n)\ZZ$. We can also allow $n=1$ we get the trivial ring with one element, in which $0=1$.

**1.36 Definition.** Let $R$ be a ring and let $\sim$ be an equivalence relation on $R$. We say that $\sim$ is a _congruence_ if for all $a,\, b,\, a’,\, b’\in R$, we have

$$ a\sim a’\text { and }b\sim b’ \imp a+b\sim a’+b’ \text { and }a\cdot b\sim a’\cdot b’. $$

The equivalence classes of a congruence $\sim$ are called _congruence classes_.

This condition says that one can add or multiply any two equivalence classes $[a], [b]\in R/{\sim }$ by first adding or multiplying any representatives of the equivalence classes in the ring $R$, and then taking the congruence class of the result. This is exactly what happens in I.35.

**1.37 Theorem.** Let $\sim$ be a congruence on a ring $R$. Define addition and multiplication on the set $R/{\sim }$ of equivalence classes as follows: for $a, b\in R$, define

$$ [a]+[b]:= [a+b]\quad \text { and }\quad [a]\cdot [b]:= [a\cdot b]. $$

Then $(R/{\sim },+,\cdot )$ is a ring with zero element $[0]$, and it is commutative if $R$ is commutative.

Proof: We first check that addition and multiplication are well-defined for equivalence classes. For this, consider alternative representatives of the equivalence classes $[a]$ and $[b]$, say $a’\in R$ satisfying $[a]=[a’]$ and $b’\in R$ satisfying $[b]=[b’]$. Then

$\seteqnumber{0}{}{0}$

$$ \begin{align*} [a’]+[b’] & = [a’ + b’] &\text { by definition} \\ & = [a+b] & \text { by the congruence property} \\ & = [a]+[b]&\text { by definition,} \end{align*} $$ and similarly

$\seteqnumber{0}{}{0}$

$$ \begin{align*} [a’]\cdot [b’] & = [a’ \cdot b’] &\text { by definition} \\ & = [a\cdot b] & \text { by the congruence property} \\ & = [a]\cdot [b]&\text { by definition} \end{align*} $$ as required. This means that addition and multiplication define binary operations on the set $R/{\sim }$ of equivalence classes.

We now check that all the ring axioms hold.

*   (i) $(R/{\sim },+)$ is an abelian group. We could check each condition by hand, but it is easier to note that $[0]$ is a subgroup of $(R,+)$ and furthermore (because $(R,+)$ is an abelian group) it is a normal subgroup: so $(R/{\sim },+)$ is nothing but the quotient group $(R,+)/[0]$.
    
*   (ii) To check that $(R/{\sim },\cdot )$ is associative, note that for $a, b, c\in R$ we have
    
    $$ ([a]\cdot [b])\cdot [c]=[ab]\cdot [c]=[(ab)c]=[a(bc)]= [a]\cdot [bc]=[a]\cdot ([b]\cdot [c]). $$
    
*   (iii) To check that $R/{\sim }$ satisfies the distributive laws, note that for $a, b, c\in R$ we have
    
    $\seteqnumber{0}{}{0}$
    
    $$ \begin{eqnarray*} [c]\cdot ([a]+[b]) = [c]\cdot [a+b] & = & [c(a+b)] \\ & = & [ca+cb] \\ & = & [ca]+[cb] \\ & = &[c]\cdot [a]+[c]\cdot [b]. \end{eqnarray*} $$
    
    One proves that $([a]+[b])\cdot [c] = [a]\cdot [c]+[b]\cdot [c]$ similarly.
    
*   (iv) $[1]\in R/{\sim }$ is a multiplicative identity because
    
    $$ [a]\cdot [1] = [a\cdot 1] = [a] = [1\cdot a]=[1]\cdot [a]. $$
    

Finally, if $R$ is commutative then $[a]\cdot [b] = [a\cdot b] = [b\cdot a] = [b]\cdot [a]$, so $R/{\sim }$ is commutative.

**1.38 Definition.** A nonempty subset $I$ of a ring $R$ is called an _ideal_ in $R$ if and only if $I$ is a subgroup of $(R,+)$ such that $ra\in I$ and $ar\in I$ for any $a\in I$ and $r\in R$.

**1.39** To check that $I$ is an ideal, it is enough to check that

*   (i) $a-b\in I$ if $a,\,b\in I$, and
    
*   (ii) $ra\in I$ and $ar\in I$ if $a\in I$ and $r\in R$.
    

This is a short cut similar to I.8.

**1.40** What we have defined is a two-sided ideal. It is also possible, and occasionally useful, to define a left ideal to be a subgroup $L$ of $(R,+)$ for which $ra\in L$ whenever $r\in R$ and $a\in L$ (but perhaps $ar\not \in L$). Obviously if $R$ is a commutative ring, left ideal, right ideals and two-sided ideals are all the same thing.

If $R$ is a commutative ring, there is a slightly different way to think about ideals: an ideal $I$ in a commutative ring $R$ is a non-empty subset that is closed under taking linear combinations with $R$ coefficients. That is

$$ \forall \; a,\, b\in I,\,\text { and } \forall \; r,\,s\in R \text { we have } ra+sb\in I. $$

This is just a rewording of I.38 in the commutative case.

**1.41 Example.** Let $R$ be a commutative ring and let $a\in R$. The set $aR=\{ar \mid r\in R\}$, also sometimes denoted $\latt {a}$ if the ring $R$ is clear from the context, is an ideal in $R$. It is called the _ideal generated by_ $a$, and every ideal of this form is called a _principal ideal_.

**1.42 Lemma.** Let $\sim$ be a congruence relation on a ring $R$. Then $[0]$ is an ideal in the ring $R$. Moreover:

*   (i) for $a, b\in R$, we have $a\sim b \iff a-b\in [0]$; and
    
*   (ii) the congruence classes of $\sim$ are the cosets of $[0]$, i.e., $[a]=a+[0]$ for all $a\in R$.
    

**1.43 Proposition.** Let $I$ be an ideal in $R$, and define $\sim$ on $R$ by $a\sim b$ if and only if $a-b\in I$. Then $\sim$ is a congruence relation in which the equivalence classes are the cosets of $I$ in $R$, i.e., we have $[a]=a+I$ for all $a\in R$. In particular, $[0]=I$.

Proof: We first show that $\sim$ is an equivalence relation. Let $a,\, b,\, c\in R$. Then $a-a = 0 \in I$, so $a\sim a$, so $\sim$ is reflexive. If $a\sim b$ then $a-b\in I$ and hence $b-a= -(a-b)\in I$ because $(I,+)$ is a group according to Definition I.38. This gives $b\sim a$, so $\sim$ is symmetric. Finally if $a\sim b$ and $b\sim c$ then $a-b,b-c\in I$. As $I$ is closed under addition, it follows that $(a-b)+(b-c)=a-c\in I$ and hence $a\sim c$. This shows that $\sim$ is transitive, so $\sim$ is an equivalence relation.

To prove that $\sim$ is a congruence, let $a, b, a’, b’\in R$ and suppose that $a\sim a’$ and $b\sim b’$. Then $a-a’,b-b’\in I$. Since $I$ is an ideal, we have

$$ (a+b)-(a’+b’)=(a-a’)+(b-b’)\in I $$

by the first defining property of an ideal, so $a+b\sim a’+b’$. Finally, by adding $0 = -ab’+ab’$ below, we get

$$ ab-a’b’=ab+\big [-ab’+ab’\big ]-a’b’=a(b-b’)+(a-a’)b’\in I $$

by the second defining property of an ideal, so $ab\sim a’b’$ as required.

For $a\in R$, the equivalence class of $a$ is

$\seteqnumber{0}{}{0}$

$$ \begin{eqnarray*} [a] := \{b\in R \mid b\sim a\} & = & \{b\in R \mid b-a\in I\} \\ & = & \{b\in R \mid \exists \; i\in I \text { such that }b-a=i\} \\ & = & \{a+i \mid i\in I\} \\ & = & a+I \end{eqnarray*} $$

as claimed.

**1.44** Proposition I.43 says that ideals determine congruence relations, and it provides the converse to Lemma I.42. These results together establish a one-to-one correspondence between congruences on a ring and ideals in that ring. We may therefore change our point of view when considering quotient rings: then next definition simply rewrites the definition of the quotient ring $R/{\sim }$ constructed in Theorem I.37 directly in terms of the ideal $I$ associated to the congruence class $\sim$.

**1.45 Definition.** Let $I$ be an ideal in a ring $R$. The _quotient ring_ $R/I$ is the set

$$ R/I=\{a+I\mid a\in R\} $$

of cosets of $I$ in $R$. Addition and multiplication in the ring $R/I$ are defined by

$\seteqnumber{0}{}{0}$

$$ \begin{eqnarray*} (a+I)+(b+I) & = & (a+b)+I \\ (a+I)\cdot (b+I) & = & (a\cdot b) + I. \end{eqnarray*} $$

That is, we add and multiply cosets by choosing representatives of them and doing the addition and multiplication in $R$, and then taking the coset of the result.

**1.46 Example.** We already know several examples of quotient rings.

*   (i) $n\ZZ$ is an ideal in $\ZZ$, so the example in I.35 is a case of this. That explains why we chose the notation $\ZZ /n\ZZ$.
    
*   (ii) For a ring $R$, consider the polynomial ring $R[t]$ and look at the ideal $\latt {t^2}=\{t^2f \in R[t]\mid f\in R[t]\}$. The corresponding equivalence relation is given by $f\sim g\iff f-g\in \latt {t^2}\iff t^2 \vert f-g$.
    

Any polynomial $f$ can be written in the form $f=t^2h+at+b$ for unique $a,\,b\in R$ (and $h\in R[t]$), so $[f]=[ax+b]$ for some unique $a,\,b\in R$. Therefore $R[t]/\latt {t^2} =\{[at+b] \mid a,\,b\in R\}$, and addition and multiplication are given by

$$ [at+b]+[ct+d]=[(a+c)t+(b+d)] $$

and

$$ [at+b]\cdot [ct+d]=[act^{2}+(ad+bc)t+bd]=[(ad+bc)t+bd]. $$

respectively. In other words we work with polynomials and then discard all the terms involving $t^2$, because that is zero in $R[t]/\latt {t^2}$.

**1.47** I.46(ii) provides a respectable way to say the thing that you said when learning calculus: “$\varepsilon ^2$ is small so we’ll ignore it”. You were actually working in $R[\varepsilon ]/\latt {\varepsilon ^2}$.