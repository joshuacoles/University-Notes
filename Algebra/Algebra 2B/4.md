#### 5 The structure of linear operators

**5.1 Definition.** Let $V$ be a vector space over $K$: from V.3 onwards we assume it is finite-dimensional, of dimension $n$.

An _endomorphism_ of $V$ is a linear map $\alpha \colon V\to V$ (also called a _linear operator_). The set of all endomorphisms of $V$ is denoted $\End (V)$.

**5.2 Lemma.** The operations $+$ (defined by $(\alpha +\beta )(v)=\alpha (v)+\beta (v)$) and $\circ$ (composition) make $\End (V)$ into a ring: in fact, if we identify $\lambda \in K$ with the map $v\mapsto \lambda v$, then $\End (V)$ becomes a $K$-algebra.

**5.3 Proposition.** Any choice of basis $\bde =(e_1,\ldots ,e_n)$ of $V$ determines a ring isomorphism $\eta _\bde \colon M_{n\times n}(K)\to \End (V)$, given by

$$ \eta _\bde (A)(\sum _i x_ie_i)=\sum _i\sum _j A_{ij}x_je_i. $$

**5.4** In other words, $A$ is the matrix representing the endomorphism $\alpha =\eta _\bde (A)$ with respect to the given basis $\bde =(e_1,\ldots ,e_n)$.

We have set this up so that matrices multiply vectors from the left. As a result, when we do matrix multiplication we really ought to write our vectors as column vectors. If we need to write a vector out, though, it is usually typographically easier to write it as a row vector.

**5.5** Given a polynomial $f=\sum _{i=0}^d a_it^i\in K[t]$, and an endomorphism $\alpha \in \End (V)$, we may write

$$ f(\alpha ) = a_0\id +a_1\alpha +a_2\alpha ^2+\cdots + a_d\alpha ^d $$

for the endomorphism obtained by substituting $\alpha$ for $t$ (and formally replacing $t^0=1$ by the identity element $\id \in \End (V)$). The map $K[t]\to \End (V)$ defined by $f\mapsto f(\alpha )$ is a ring homomorphism, which we call $\ev _\alpha$.

**5.6** Similarly, given a polynomial $f=\sum _{i=0}^d a_it^i\in K[t]$, and an matrix $A\in M_{n\times n}(K)$, we may write

$$ f(A) = a_0\II _n+a_1A+a_2A^2+\cdots + a_dA^d $$

for the matrix obtained by substituting $A$ for $t$ (and formally replacing $t^0=1$ by the identity element $\II _n\in M_{n\times n}(K)$). The map $K[t]\to M_{n\times n}(K)$ defined by $f\mapsto f(\alpha )$ is a ring homomorphism, which we call $\ev _A$.

**5.7** The evaluations in V.5 and V.6 are compatible with the isomorphism $\eta _\bde$ from Proposition V.3, in the sense that if $V$ is of dimension $n$ and $\bde$ is a basis for $V$, then $f(\eta _\bde (A))=\eta _\bde (f(A))$.

In other words, evaluating a polynomial at an endomorphism and then expressing as a matrix is the same as expressing the endomorphism as a matrix and then evaluating the polynomial at that matrix.

**5.8 Lemma.** The kernel of the ring homomorphism $\ev _\alpha$ (or $\ev _A$) is not the zero ideal.

Proof: The dimension of $M_{n\times n}(K)$ is $n^2$ so the list $\II _n,A,A^2,\ldots , A^{n^2}$ must be linearly dependent. If $a_0,\ldots ,a_{n^2}\in K$ (not all zero) satisfy $a_0\II _n+a_1A+\cdots +a_{n^2}A^{n^2}=0$, then the polynomial $f=\sum _{i=0}^{n^2} a_it^i$ satisfies $\ev _A(f) = 0$, so $f\in \Ker (\ev _A)$ is a nonzero element of the kernel. The argument for $\ev _\alpha$ is exactly the same.

**5.9 Definition.** A polynomial $f=a_0+a_1t+\cdots +a_d t^d\in K[t]$ is said to be _monic_ if its leading coefficient $a_d$ is equal to $1$.

**5.10 Lemma.** If $I$ is an ideal of $K[t]$ then there is a unique monic polynomial $m$ of least degree in $I$, and $I$ is the ideal generated by $m$.

Proof: This follows from the proof of Theorem III.16. The ideal $I$ is principal, and is generated by any element of least degree. If $m’\in I$ is an element of $I$ of least degree, with leading term $a’_d\in K^*$, then $m={a’_d}^{-1}m$.

**5.11 Definition.** The _minimal polynomial_ or _minimum polynomial_ of $\alpha \colon V\to V$ is the monic polynomial $m_\alpha \in K[t]$ of lowest degree such that $m_\alpha (\alpha )=0$, i.e. the monic generator of $\Ker (\ev _\alpha )$. Similarly, the minimum polynomial of $A\in M_{n\times n}(K)$ is the monic polynomial that generates $\Ker (\ev _A)$.

**5.12 Proposition.** If $A$ and $A’$ both represent the same endomorphism $\alpha \in \End (V)$, with respect to different bases of $V$, then $m_A=m_{A’}=m_\alpha$.

**5.13 Example.**

*   (i) If $\alpha =\lambda \id$ then $p(\alpha )=0$ where $p(t)=t-\lambda$, so $m_\alpha (t)=t-\lambda$.
    
*   (ii) If $A=\begin {pmatrix} 0 &1 \\ 1 & 0 \end {pmatrix}$ then $A^2=\II _2$, aso $p(A)=0$ where $p(t)=t^2-1$. As $A$ is not a diagonal matrix, we have that $q(A)\neq 0$ for any $q=t-\lambda$ of degree $1$. Hence $m_A(t)= t^2-1$.
    

**5.14 Lemma.** Let $\alpha \in \End (V)$ and suppose that $p\in K[t]$ is such that $p(\alpha )=0$. Then every eigenvalue of $\alpha$ is a root of $p$. In particular every eigenvalue of $\alpha$ is a root of $m_\alpha$.

Proof: Let $v\neq 0$ be an eigenvector of eigenvalue $\lambda \in K$ and suppose $p(t)=\sum _{i=0}^k a_it^i$. Then $p(\alpha )=0$ gives

$$ 0= p(\alpha )\,v = (a_0\id +a_1\alpha +\cdots +a_k\alpha ^k)v = (a_0+a_1\lambda +\cdots +a_k\lambda ^k)v = p(\lambda )v. $$

As $v\neq 0$ it follows that $p(\lambda )=0$.

**5.15 Definition.** The _characteristic polynomial_ of $\alpha \in \End (V)$ is $\chi _\alpha (t)= \det (\alpha - t\id )$. The characteristic polynomial of $A\in M_{n\times n}(K)$ is $\chi _A(t)=\det (A-t\II _n)$: by the definition of determinant of an endomorphism, it follows that $\chi _\alpha =\chi _A$ if $A$ represents $\alpha$ with respect to some basis.

**5.16** The definition of $\det (\beta )$ for $\beta \in \End (V)$ is that it is the determinant of any matrix that represents $\beta$. There is a coordinate-free way to define $\det (\beta )$ but it is (slightly) beyond the scope of this course.

**5.17 Definition.** Let $\alpha \in \End (V)$ be an endomorphism, or $A\in M_{n\times n}(K)$ be a matrix. In either case _algebraic multiplicity_ of an eigenvalue $\lambda$, denoted $\am (\lambda )$, is the multiplicity of $\lambda$ as a root of $\chi _\alpha (t)$. The _geometric multiplicity_ of $\lambda$, denoted $\gm (\lambda )$, is the dimension of the eigenspace $E_\alpha (\lambda )=\Ker (\alpha -\lambda \id )= E_A(\lambda )=\Ker (A-\lambda \II _n)$.

**5.18 Proposition.** For any eigenvalue we have $\am (\lambda ) \geq \gm (\lambda )$.

Proof: We may assume $\lambda =0$. We need to show that if $\dim \Ker (\alpha )=k$ then $t^k|\chi _\alpha (t)$. Choose a basis of $\Ker (\alpha )$ and extend it to a basis of $V$. With respect to this basis, the endmorphism $\alpha -t\id$ has matrix

$$ A-t\II _n=\begin {pmatrix} -t\II _k & X \\ 0 &Y \end {pmatrix} $$

whose determinant is $\pm t^k\det Y$.

**5.19 Theorem.** (Cayley–Hamilton Theorem) For any $A\in M_n(K)$ we have $\chi _A(A)=0\in M_n(K)$. Equivalently, for any linear map $\alpha \colon V\to V$ we have $\chi _\alpha (\alpha )=0\in \End (V)$.

**5.20** This is a very nontrivial assertion. It is true that $\det (A-A\II _n)=\det (0)=0\in K$ but the left-hand side is not $\chi _A(A)$: in fact, it is not even a matrix. For $n=2$, the matrix $A=\begin {pmatrix} a & b \\ c & d\end {pmatrix}$ has $\chi _A(t) = \det \begin {pmatrix} a-t & b \\ c & d-t\end {pmatrix} = t^2-(a+d)t+(ad-bc)$, so the Cayley–Hamilton Theorem is the generalisation to arbitrary $n$ of the calculation

$\seteqnumber{0}{}{0}$

$$ \begin{eqnarray*} \chi _A(A) & = & A^2-(a+d)A + (ad-bc)\cdot \II _2 \\ & = & \begin {pmatrix} a^2+bc & ab+bd \\ ca+cd & bc+d^2\end {pmatrix} -\begin {pmatrix} a^2+ad & ab+bd \\ ac+cd & ad+d^2\end {pmatrix}+ (ad-bc) \begin {pmatrix} 1& 0 \\ 0 &1\end {pmatrix}\\ & = & \begin {pmatrix} 0 & 0 \\ 0 & 0\end {pmatrix}. \end{eqnarray*} $$

**5.21 Corollary.** The minimal polynomial $m_\alpha$ divides the characteristic polynomial $\chi _\alpha$. In fact the roots of $m_\alpha$ are precisely the eigenvalues of $\alpha$.

Proof: The Cayley–Hamilton theorem says that $\chi _\alpha \in \Ker (\ev _\alpha )$, and $\Ker (\varphi _\alpha )=\latt {m_\alpha }$, so $m_\alpha |\chi _\alpha$. Therefore every root of $m_\alpha$ is a root of $\chi _\alpha$, and hence an eigenvalue of $\alpha$. Conversely, every eigenvalue of $\alpha$ is a root of $m_\alpha$ by Lemma V.14.

**5.22** When working over $\CC$, Corollary V.21 says that if $\lambda _1, \ldots ,\lambda _k$ are the distinct eigenvalues of $\lambda$ and $\chi _\alpha (t)=(\lambda _1-t)^{r_1}\cdots (\lambda _k-t)^{r_k}$, then $m_\alpha (t)=(t-\lambda _1)^{s_1}\cdots (t-\lambda _k)^{s_k}$, with $1\leq s_i\leq r_i$ for all $1\leq i\leq k$.

**5.23** Proof of the Cayley–Hamilton Theorem: Suppose $\chi _A(t)=\det (A-t\II _n)=a_0+a_1t+\cdots +a_nt^n$. We must show that $\chi _A(A)=a_0\II _n+a_1A+\cdots +a_nA^n$ is equal to the zero matrix. Notice that

$$ \det (\II _n-tA)=a_0t^n+a_1t^{n-1}+\cdots +a_n $$

and recall the adjugate formula for determinants:

$$ \adj (\II _n-tA)\cdot (\II _n-tA)=\det (\II _n-tA)\II _n. $$

We can regard this as an equation in the ring of formal power series $M_{n\times n}[[t]]$ (see I.27). In this ring, $\II _n-tA$ is a unit: its inverse is $\sum _{i=0}^\infty A^i t^i$, as one checks by multiplying. Hence in $M_{n\times n}[[t]]$ we have

$$ \adj (\II _n-tA)=\det (\II _n-tA)\II _n\cdot \sum _{i=0}^\infty A^i t^i=(a_0t^n+a_1t^{n-1}+\cdots +a_n)\cdot \sum _{i=0}^\infty A^i t^i $$

But $\adj (\II _n-tA)$ is a matrix whose entries are determinants of $(n-1)\times (n-1)$ matrices, and the entries of those matrices (they are submatrices of $\II _n-tA$ are linear in $t$. So each entry of $\adj (\II _n-tA)$ is a polynomial of degree at most $n-1$, so we can write

$$ \adj (\II _n-tA)=\sum _{i=0}^{n-1}B_it^i\in M_{n\times n}[t]\subset M_{n\times n}[[t]]. $$

Now we have the equation

$$ \sum _{i=0}^{n-1}B_it^i=(a_0t^n+a_1t^{n-1}+\cdots +a_n)\cdot \sum _{i=0}^\infty A^i t^i $$

in $M_{n\times n}[[t]]$, and comparing the coefficients of $t^n$ gives

$$ 0=a_nA^n+a_{n-1}A^{n-1}+\cdots +a_0, $$

as required.

##### Invariant subspaces

**5.24 Definition.** For a linear operator $\alpha \colon V\to V$, we say that a subspace $W$ of $V$ is $\alpha$-_invariant_ if $\alpha (W) \subseteq W$.

If $W$ is $\alpha$-invariant, then the _restriction_ of $\alpha$ to $W$ is the linear operator $\alpha |_W\colon W\to W$ given by $\alpha |_W(w)=\alpha (w)$.

**5.25 Example.**

*   (i) The subspaces $\{0\}$ and $V$ are always $\alpha$-invariant.
    
*   (ii) Let $\lambda$ be an eigenvalue of $\alpha$. If $v$ is an eigenvector for $\lambda$, then the $1$-dimensional subspace $Kv$ is $\alpha$-invariant.
    
*   (iii) For any $\theta \in \RR$, the linear operator $\alpha \colon \RR ^{3}\to \RR ^3$ that rotates every vector by $\theta$ radians anticlockwise around the $z$-axis has $V_1:=\RR e_1\oplus \RR e_2$ and $V_2:=\RR e_3$ as $\alpha$-invariant subspaces. The restriction $\alpha \vert _{V_1}\colon V_1\to V_1$ is simply rotation by $\theta$ radians in the plane, while $\alpha \vert _{V_2}\colon V_2\to V_2$ is the identity on the real line. Notice that the matrix for $\alpha$ in the basis $e_1, e_2, e_3$ is the “block” matrix
    
    $$ A=\begin {pmatrix} \cos \theta & -\sin \theta & 0 \\ \sin \theta & \cos \theta & 0 \\ 0 &0 & 1 \end {pmatrix}. $$
    
    Notice that this matrix has two square non-zero blocks (the top left $2\times 2$ matrix and the bottom right $1\times 1$ matrix). These two blocks are precisely the matrices for the linear maps $\alpha \vert _{V_1}$ and $\alpha \vert _{V_2}$ in the given bases on $V_1$ and $V_2$ respectively.
    

**5.26 Definition.** For $1\leq i\leq k$, let $V_i$ be a vector space and let $\alpha _i\in \End (V_i)$. The _direct sum_ of $\alpha _1, \dots , \alpha _k$ is the linear map

$$ (\alpha _1\oplus \cdots \oplus \alpha _k)\colon \bigoplus _{1\leq i\leq k} V_i\to \bigoplus _{1\leq i\leq k} V_i $$

defined as follows: each $v\in \bigoplus _{1\leq i\leq k} V_i$ can be written uniquely in the form $v=v_1+\dots + v_k$ for some $v_i\in V_i$, and we define

$$ (\alpha _1\oplus \cdots \oplus \alpha _k)(v_1+\dots + v_k):=\alpha _1(v_1)+\dots + \alpha _k(v_k). $$

**5.27** For $1\leq i\leq k$, let $A_i\in M_{n_i}(K)$ be the matrix for a linear map $\alpha _i$ with respect to some basis $\cB _i$ of $V_i$. Then the matrix for the direct sum $\alpha _1\oplus \cdots \oplus \alpha _k$ with respect to the basis $\cB _1\cup \cB _2\cup \cdots \cup \cB _k$ of $\bigoplus _{1\leq i\leq k} V_i$ is the _direct sum_ (block matrix)

$$ A_1\oplus \cdots \oplus A_k=\begin {pmatrix} A_1 & & & \\ & A_2 & & \\ & & \ddots & \\ & & & A_k \end {pmatrix} $$

(zeros everywhere else in the matrix) of the matrices $A_1, \dots , A_k$.

**5.28 Lemma.** For $\alpha \in \End (V)$, suppose $V=V_1\oplus V_2\oplus \cdots \oplus V_k$ where $V_1,\ldots ,V_k$ are $\alpha$-invariant subspaces. For $1\leq i\leq k$, write $\alpha _i:=\alpha |_{V_i}\in \End (V_i)$. Then

$$ \alpha =\alpha _1\oplus \cdots \oplus \alpha _k\in \bigoplus _{i=1}^k \End (V_i) $$

Moreover, the minimum polynomial $m_\alpha$ is the unique monic polynomial of least degree that is divisible by all the $m_{\alpha _i}$; that is, the least common multiple of $m_{\alpha _1}, \dots , m_{\alpha _k}$.

Proof: The first clause is easy: each $v\in V$ can be written uniquely as $v=v_1+\cdots +v_k$ for $v_i\in V_i$, and

$$ \alpha (v) = \alpha (v_1)+\cdots +\alpha (v_k) = \alpha _1(v_1)+\cdots +\alpha _k(v_k) $$

where $\alpha _i(v_i)\in V_i$.

For the claim about the minimum polynomial, we claim first that any $f\in K[t]$ satisfies

$$ f(\alpha ) = f(\alpha _1)\oplus f(\alpha _2)\oplus \cdots \oplus f(\alpha _k). $$

Indeed, for $i>0$ and $v=v_1+\dots + v_k$ we have $\alpha ^i(v_1+\dots + v_k) = \alpha ^i_1(v_1)+\dots + \alpha ^i_k(v_k)$, so $\alpha ^i = \alpha _1^i\oplus \dots \oplus \alpha ^i_k$. For any scalar $c\in K$, it follows that $c\alpha ^i = c\alpha _1^i\oplus \dots \oplus c\alpha ^i_k$. In view of the definition of addition in $\End (V)$ (see Lemma V.2) that is all we need to check.

Now $m_\alpha$ divides $f$ if and only if $f(\alpha ) = 0$ (by Definition V.11 the multiples of $m_\alpha$ are the elements of $\Ker (\ev _\alpha )$), and this holds if and only if $f(\alpha _i)= 0$ for all $1\leq i\leq k$. But again by Definition V.11, that holds if and only if $m_{\alpha _i}\vert f$ for all $1\leq i\leq k$.

**5.29** What we have just shown is that $f\in \latt {m_\alpha }$ if and only if $f\in \latt {m_{\alpha _i}}$ for all $i$. In other words, $\latt {m_\alpha }=\bigcap _{i=1}^k \latt {m_{\alpha _i}}$. So intersection of ideals is the algebraic way of saying “lcm”.

##### Jordan blocks

**5.30** Consider first the special case where $K=\CC$ (or some other algebraically closed field) and $\alpha \in \End (V)$ has only one eigenvalue $\lambda \in K$. Then from V.22 we know that $\chi _\alpha (t) = (\lambda -t)^n$ and $m_\alpha (t) = (t-\lambda )^r$ for some $r\leq n$.

**5.31 Definition.** If $\alpha \in \End (V)$ and $v\in V$, the _cyclic subspace generated by $v$_ $Z_\alpha (v)$ is the subspace spanned by all the $\alpha ^i(v)$ for $i\ge 0$; equivalently

$$ Z_\alpha (v) = \big \{p(\alpha )v\in V \mid p\in K[t]\big \}. $$

**5.32** By construction, $Z_\alpha (v)$ is an $\alpha$-invariant subspace of $V$. If $v\in E_\alpha (\lambda )$ (i.e. if $\alpha (v)=\lambda v$), then $Z_\alpha (v)=K v$.

If one stops at $\alpha ^r(v)$ the resulting space is known to numerical analysts as the $r$th _Krylov space_.

**5.33 Proposition.** Let $\alpha \in \End (V)$ be such that $\chi _\alpha (t) =(\lambda -t)^n$ and $m_\alpha (t) = (t-\lambda )^r$. For any nonzero vector $v\in V$, define $e:=e(v)\in \NN$ to be the smallest positive integer such that $(\alpha -\lambda \id )^e v=0$. For $1\le j\le e$ we define $v_j=(\alpha -\lambda \id )^{e-j}v$ (in particular $v_e=v$).

Then $(v_1,v_2,\ldots ,v_e)$ is a basis for $W=Z_\alpha (v)$, and with respect to this basis, the matrix for the linear map $\beta =\alpha \vert _W\in \End (W)$ is the $e\times e$ matrix

$$ J(\lambda ,e)=\begin {pmatrix} \lambda & 1 & & & \\ & \lambda & 1 & & \\ & & \ddots & \ddots & \\ & & & \lambda & 1 \\ & & & & \lambda \end {pmatrix}. $$

Moreover, $E_\beta (\lambda )=K v_1$, and we have $m_\beta (t)=(t-\lambda )^e=\chi _\beta (t)$.

Proof: First of all, since $m_\alpha (t)=(t-\lambda )^r$, we have that $(\alpha -\lambda \id )^r(v)=m_\alpha (\alpha )(v)=0$, so the number $e$ does exist.

The fact that the $v_j$ form a basis for $W$ is an easy exercise.

For the shape of the matrix, we compute that $\alpha (v_1)=\lambda v_1+(\alpha -\lambda \id )v_1= \lambda v_1+(\alpha -\lambda \id )^e v=\lambda v_1$, and for $2\leq i\leq e$ we have $\alpha (v_i)=\lambda v_i+(\alpha -\lambda \id )v_i= \lambda v_i+v_{i-1}=v_{i-1}+\lambda v_i$. From this we may read off the columns of $J(\lambda ,e)$. The rest is straightforward.

**5.34 Definition.** We call $J(\lambda ,e)$ a _Jordan block_ of $\alpha$.

**5.35 Example.** The linear operator $\alpha \colon \CC ^2\to \CC ^2$ given by the matrix $A=\begin {pmatrix} 3/2 & 1/2 \\ -1/2 & 1/2 \end {pmatrix}$ satisfies $\chi _\alpha (t) =(1-t)^2$ and $m_\alpha (t) = (t-1)^2$. Following Proposition V.33 we first compute an eigenvector $v_1$ for $\lambda =1$, by solving $(A-\II _2)v_1 = 0$: that is, $\begin {pmatrix}1/2 & 1/2 \\ -1/2 & -1/2 \end {pmatrix} v_1 = \begin {pmatrix} 0 \\ 0 \end {pmatrix}$. One solution to this is $v_1 = (1,-1)$ (see V.4, where we agreed to write row vectors even when we shouldn’t). Next, we solve $(A-\II )v_2 = v_1$: that is, $\begin {pmatrix}1/2 & 1/2 \\ -1/2 & -1/2 \end {pmatrix} v_2 = \begin {pmatrix} 1 \\ -1 \end {pmatrix}$. One solution is $v_2 = (0,2)$. The matrix required to change basis so that $A$ can be written in the form of Proposition V.33 is the matrix whose columns are $v_1$ and $v_2$, namely $P = \begin {pmatrix} 1 & 0 \\ -1 & 2 \end {pmatrix}$, and one may check that

$$ P^{-1}AP = \begin {pmatrix} 1 & 0 \\ 1/2 & 1/2 \end {pmatrix} \begin {pmatrix} 3/2 & 1/2 \\ -1/2 & 1/2 \end {pmatrix} \begin {pmatrix} 1 & 0 \\ -1 & 2 \end {pmatrix} = \begin {pmatrix} 1 & 1 \\ 0 & 1 \end {pmatrix} = J(1,2). $$

**5.36 Theorem.** Let $\alpha \in \End (V)$ be such that $\chi _\alpha (t) =(\lambda -t)^n$ and $m_\alpha (t) = (t-\lambda )^r$. Then there exists a basis for $V$ with respect to which $\alpha$ has matrix

$$ \JNF (\alpha )= \begin {pmatrix} J(\lambda ,e_1) & & & \\ & J(\lambda ,e_2) & & \\ & & \ddots & \\ & & & J(\lambda ,e_m) \end {pmatrix}=J(\lambda ,e_1)\oplus \cdots \oplus J(\lambda ,e_m), $$

where $m=\gm (\lambda )$ is the number of Jordan blocks; $r=\max \{e_1,\ldots ,e_m\}$; and $n=e_1+\cdots +e_m$.

Proof: Assume for now that we can find non-zero $v_1,\ldots ,v_m\in V$ such that

$$ V=Z_\alpha (v_1)\oplus \cdots \oplus Z_\alpha (v_m), $$

and put $W_j:=Z_\alpha (v_j)$ and $e_j=\dim W_j$. Because $W_j$ is $\alpha$-invariant, we may put $\alpha _j=\alpha |_{W_j}$, and then $\alpha =\alpha _1\oplus \cdots \oplus \alpha _m$ by Lemma V.28. We then apply Proposition V.33 to each $W_j$.

That gives the required form for the matrix $A$ and also gives $m_{\alpha _j} = (t-\lambda )^{e_j}$ for $1\leq j\leq m$. Moreover, by assumption we can write any $v\in V$ as $v=w_1+\cdots +w_m\in V$ for $w_j\in W_j$, so if $v\in E_\alpha (\lambda )$ then

$$ \alpha _1(w_1)+\cdots +\alpha _m(w_m)=\alpha (v) = \lambda (v) = \lambda w_1+\cdots +\lambda w_m $$

and thus $\alpha _j(w_j)=\lambda w_j$ for $1\leq j\leq m$. Then $E_\alpha (\lambda )=E_{\alpha _1}(\lambda )\oplus \cdots \oplus E_{\alpha _m}(\lambda )$. By Proposition V.33, we have $\dim E_{\alpha _i}(\lambda )=1$, so

$$ m = \dim E_{\alpha _1}(\lambda ) +\cdots + \dim E_{\alpha _m}(\lambda ) = \dim E_\alpha (\lambda ) = \gm (\lambda ). $$

By Lemma V.28, we know that $m_\alpha (t)$ is the least common multiple of the $m_{\alpha _j}(t)$, but Proposition V.33 shows that $m_{\alpha _j}(t)=(t-\lambda )^{e_i}$, so $m_\alpha (t)=(t-\lambda )^{\max _j e_j}$ as claimed.

Finally, $n=e_1+\cdots +e_m$ because $\dim V=\dim Z_\alpha (v_1)+\cdots +\dim Z_\alpha (v_m)$.

It remains to show that such $v_1,\ldots ,v_m$ exist. We establish this by induction on $r$.

If $r=1$, then $\alpha =\lambda \id$, and we may simply take $m=n$ and $v_1,\ldots ,v_m$ to be any basis for $V$. In this case $e=1$ and each $Z_\alpha (v_j)$ is $1$-dimensional.

Now suppose that $r\geq 2$ and that the claim holds for smaller values of $r$. Consider the $\alpha$-invariant subspace

$$ W=(\alpha -\lambda \id )V = \{(\alpha -\lambda \id )(v) \in V \mid v\in V\}. $$

Notice that $(\alpha -\lambda \id )^{r-1}w=0$ for all $w\in W$ and the minimum polynomial of $\alpha |_{W}$ is $(t-\lambda )^{r-1}$. The inductive hypothesis gives vectors $(\alpha -\lambda \id )v_1,\ldots ,(\alpha -\lambda \id )v_l\in W\setminus \{0\}$ such that

$$ W= Z_\alpha ((\alpha -\lambda \id )v_1)\oplus \cdots \oplus Z_\alpha ((\alpha -\lambda \id )v_l). $$

Put $\beta _j=\alpha |_{Z_\alpha (v_j)}$. Proposition V.33 shows that $E_{\beta _j}(\lambda )=K w_j$ where $w_j=(\alpha -\lambda \id )^{e_j-1}v_j\in Z_\alpha ((\alpha -\lambda \id )v_j)$, for some $e_j\geq 2$. As before, it follows that $w_1,\ldots ,w_l$ is a basis for $E_{\alpha |_W}(\lambda )$. We can extend this to a basis $(w_1,\ldots ,w_l,v_{l+1},\ldots ,v_m)$ for $E_\alpha (\lambda )\subseteq Z_\alpha (v_1)+\cdots +Z_\alpha (v_l)+K v_{l+1}+\cdots + K v_m$.

But in fact we claim that

$$ V=Z_\alpha (v_1)\oplus \cdots \oplus Z_\alpha (v_l)\oplus Z_\alpha (v_{l+1})\oplus \cdots \oplus Z_\alpha (v_m). $$

Since $v_{l+1},\ldots ,v_m$ are eigenvectors for $\lambda$, this is the same as saying that

$$ V=Z_\alpha (v_1)\oplus \cdots \oplus Z_\alpha (v_l)\oplus (K v_{l+1}\oplus \cdots \oplus K v_m). $$

The right hand side is by definition contained in the left. For the opposite inclusion, let $v\in V$. Then $(\alpha -\lambda \id )v\in W$, so by the choice of $v_1,\ldots ,v_l$ there exist $p_1,\ldots ,p_l\in K[t]$ such that

$$ (\alpha -\lambda \id )v=p_1(\alpha )(\alpha -\lambda \id )v_1+ \cdots + p_l(\alpha )(\alpha -\lambda \id )v_l. $$

We rearrange this as $(\alpha -\lambda \id )(v-(p_1(\alpha )v_1+\cdots + p_e(\alpha )v_e))=0$, thus finding an eigenvector

$$ v-(p_1(\alpha )v_1+\cdots +p_l(\alpha )v_l)\in E_\alpha (\lambda )\subseteq Z_\alpha (v_1)+\cdots +Z_\alpha (v_l)+K v_{l+1}+\cdots + K v_m. $$

Since $(p_1(\alpha )v_1+\cdots +p_l(\alpha )v_l)\in Z_\alpha (v_1)+\cdots +Z_\alpha (v_l)$, we have shown that $V\subseteq Z_\alpha (v_1)+\cdots Z_\alpha (v_l)+K v_{l+1}+\cdots + K v_m$.

What we have not yet shown is that this is a direct sum. It is just about clear from the construction, but it is also easy to give a formal proof, as follows: suppose that for some polynomials $q_j\in K[t]$ and some constants $b_{l+i}\in K$ we have

$$ q_1(\alpha )v_1+\cdots +q_l(\alpha )v_l+b_{l+1}v_{l+1}+ \cdots +b_mv_m=0. $$

Then we apply $(\alpha -\lambda \id )$ to both sides, so as to get a relation in $W$, namely

$$ 0=q_1(\alpha )(\alpha -\lambda \id )v_1+\cdots + q_l(\alpha )(\alpha -\lambda \id )v_l. $$

But $W$ does decompose as a direct sum (that was the induction hypothesis), so $(\alpha -\lambda \id )q_j(\alpha )v_j=0$ for $1\leq i\leq l$. Thus $q_j(\alpha )v_j$ is an eigenvector that lies in $Z_\alpha (v_j)$, so it must be a multiple of $w_j$. Since $w_1,\ldots ,w_l$ are linearly independent, it follows that $q_j(\alpha )v_j=0$ for $1\leq j\leq l$.

But now all that is left is $b_{l+1}v_{l+1}+\cdots +b_mv_m=0$, and as $v_{l+1},\ldots ,v_m$ are linearly independent, it follows that $b_{l+1}=\ldots =b_m=0$. This finishes the proof.

**5.37 Example.** For a complex vector space $V$ of dimension 4, suppose that $\alpha \in \End (V)$ has $m_\alpha (t)=(t-5)^2$ and $\chi _\alpha (t)=(t-5)^4$. Since the degree of $m_\alpha (t)$ is $2$, we must have at least one largest block $J(5,2)$, so the possible decompositions of the 4-dimensional space $V$ are $J(5,2)\oplus J(5,2)$ and $J(5,2)\oplus J(5,1)\oplus J(5,1)$. If we know in addition that $\gm (5)=3$ then we must have three blocks, so the second possibility applies.

##### Primary decomposition

**5.38** If $\alpha \in \End (V)$ has more than one eigenvalue we aim to choose a basis in which the matrix for $\alpha$ is a block matrix as in V.27), and Lemma V.28 tells us that we can achieve this by writing $V$ as the direct sum of $\alpha$-invariant subspaces.

Our goal now is to produce one such decomposition of $V$. The key is to factor the minimal polynomial $m_\alpha$ in the ring $K[t]$ as a product of irreducible factors. We begin with the case where the minimal polynomial has two coprime factors.

In this subsection, $K$ is any field.

**5.39 Proposition.** Let $\alpha \in \End (V)$ be a linear operator whose minimal polynomial is $m_\alpha =q_1q_2$, where $q_1,\, q_2$ are monic and coprime. Put $V_i=\Ker (q_i(\alpha ))$. Then

*   (i) the subspaces $V_i$ are $\alpha$-invariant and $V=V_1\oplus V_2$; and
    
*   (ii) if we put $\alpha _i=\alpha |_{V_i}$ then $\alpha =\alpha _1\oplus \alpha _2$ and $m_{\alpha _i}=q_i$.
    

Proof: We first produce a decomposition into images, instead of kernels. We define $W_1=\Image (q_2(\alpha ))$ and $W_2=\Image (q_1(\alpha ))$, and to prove modified versions of (i) and (ii) with $W_i$ in place of $V_i$.

We first prove that $W_i$ are $\alpha$-invariant and $V=W_1\oplus W_2$. The $\alpha$-invariance follows because Since $q_i(\alpha )$ commutes with $\alpha$. Since $q_1$ and $q_2$ are coprime, Lemma III.20 gives $f_1,\,f_2\in K[t]$ such that $1=f_1q_1+f_2q_2$, so

$$ \id =f_1(\alpha )q_1(\alpha )+f_2(\alpha )q_2(\alpha )=q_2(\alpha )f_2(\alpha )+q_1(\alpha )f_1(\alpha ), $$

and therefore for any $v\in V$ we have

$$ v=\id (v)=q_2(\alpha )\big (f_2(\alpha )(v)\big )+q_1(\alpha )\big (f(\alpha )(v)\big )\in W_1+W_2, $$

so $V=W_1+W_2$. To see that the sum is direct, suppose $v\in W_1\cap W_2$, say $v=q_1(\alpha )(v_1)=q_2(\alpha )(v_2)$. Then using the equation above, we have that

$\seteqnumber{0}{}{0}$

$$ \begin{eqnarray*} v & = & f_1(\alpha )q_1(\alpha )(v)+f_2(\alpha )q_2(\alpha )(v) \\ & = & [f_1(\alpha )q_1(\alpha )q_2(\alpha )](v_2)+ [f_2(\alpha )q_2(\alpha )q_1(\alpha )](v_1) \\ & = & [f_1(\alpha )m_\alpha (\alpha )](v_2)+ [f_2(\alpha )m_\alpha (\alpha )](v_1) \\ & = & 0. \end{eqnarray*} $$

Hence $W_1\cap W_2=\{0\}$ and $V=W_1\oplus W_2$.

For the modified version of (ii), the fact that $\alpha _i=\alpha |_{W_i}$ satisfy $\alpha =\alpha _1\oplus \alpha _2$ follows from Lemma V.28.

For the statement about the minimal polynomial, let us take $i=1$ for simplicity. Let $f\in K[t]$. Then $m_{\alpha _1}|f$ if and only if $f(\alpha _1)(w)=0$ for all $w\in W_1$. But $\alpha _1(w)=\alpha (w)$, by definition of $\alpha _1$, and we may write $w\in W_1$ as $q_2(v)$ for some $v\in V$, so the statement becomes $f(\alpha )\big (q_2(\alpha )(v)\big )=0$ for all $v\in V$.

That is equivalent to $m_\alpha |fq_2$, because $m_\alpha$ is the minimum polynomial. Since $m_\alpha =q_1q_2$, we can write this as $q_1|f$, and that shows that $q_1$ is the minimum polynomial of $\alpha _1$.

Finally, we go back to kernels: we claim that in fact $W_i=V_i$ for $i=1,\,2$. Again we may assume $i=1$. Since each $v\in V$ satisfies $q_1(\alpha )q_2(\alpha )(v)=m_\alpha (\alpha )(v)=0$, we have that $W_1=\Image (q_2(\alpha ))\subseteq \Ker (q_1(\alpha ))=V_1$. Since the spaces are of finite dimension, we will have $W_1=V_1$ if we can show that $\dim W_1=\dim V_1$. For this we use rank-nullity, which gives

$$ \dim \Ker (q_1(\alpha )) + \dim \Image (q_1(\alpha )) = \dim V = \dim W_1+\dim W_2 $$

and we get the result by subtracting $\dim \Image (q_1(\alpha )) = \dim W_2$ from both sides.

**5.40 Theorem.** (Primary Decomposition Theorem) Let $\alpha \in \End (V)$ be a linear operator and write $m_\alpha =p_1^{n_1}\cdots p_k^{n_k}$, where $p_1,\dots ,p_k$ are the distinct monic irreducible factors of $m_\alpha$ in $K[t]$. Let $q_i=p_i^{n_i}$ and let $V_i=\Ker (q_i(\alpha ))$. Then:

*   (i) the subspaces $V_1,\ldots ,V_k$ are $\alpha$-invariant and $V=V_1\oplus \cdots \oplus V_k$; and
    
*   (ii) the maps $\alpha _i=\alpha |_{V_i}$ for $1\leq i\leq k$ satisfy $\alpha =\alpha _1\oplus \cdots \oplus \alpha _k$ and $m_{\alpha _i}=q_i$.
    

Proof: We use induction on $k$. For $k=1$, we have $m_\alpha = p_1^{n_1} = q_1$. Then

$$ V_1 = \Ker \big (q_1(\alpha )\big ) = \Ker \big (m_\alpha (\alpha )\big ) =V $$

because $m_\alpha (\alpha )$ is the zero map by Definition V.11. This proves the case $k=1$. For $k\geq 2$, suppose the result holds for any linear operator whose minimal polynomial has fewer than $k$ distinct irreducible factors. Suppose now that $m_\alpha = p_1^{n_1}\cdots p_k^{n_k}$. Define $r_1=p_1^{n_1}\cdots p_{k-1}^{n_{k-1}}$ and $r_2=p_k^{n_k}$, so $m_\alpha = r_1r_2$. Note that $r_1$ and $r_2$ are coprime, so Proposition V.39 gives

$$ V=\Ker \big (r_1(\alpha )\big )\oplus \Ker \big (r_2(\alpha )\big ), $$

where $\beta _i:=\alpha |_{\Ker (r_i(\alpha ))}$ satisfies $\alpha =\beta _1\oplus \beta _2$ and $m_{\beta _i} = r_i$ for $1\leq i\leq 2$. In particular, $\beta _1$ is a linear operator on $\Ker (r_1(\alpha ))$ whose minimal polynomial $r_1=p_1^{n_1}\cdots p_{k-1}^{n_{k-1}}$ has $k-1$ irreducble factors, so by induction there exist $\beta _1$-invariant subspaces such that

$$ \Ker \big (r_1(\alpha )\big ) = V_1\oplus \cdots \oplus V_{k-1} $$

and maps $\alpha _i=\beta _1|_{V_i}$ for $1\leq i\leq k-1$ that satisfy $\beta _1=\alpha _1\oplus \cdots \oplus \alpha _{k-1}$ and $m_{\alpha _i}=p_i^{n_i}$.

Since $\beta _1:=\alpha |_{\Ker (r_1(\alpha ))}$ and $\alpha _i=\beta _1|_{V_i}$, we have that $\alpha _i=\alpha |_{V_i}$ for $1\leq i\leq k-1$. Defining $V_k:= \Ker \big (r_2(\alpha )\big )$ gives $V=V_1\oplus \cdots \oplus V_k$, and if we set $\alpha _k:=\beta _2|_{V_k} = \alpha \vert _{V_k}$, then we have that $\alpha _i = \alpha \vert _{V_i}$ and $\alpha =\alpha _1\oplus \cdots \oplus \alpha _k$ for $1\leq i\leq k$. It remains to note that $m_{\alpha _i}=p_i^{n_i}$ for all $1\leq i\leq k$.

**5.41 Corollary.** A linear map $\alpha \in \End (V)$ is diagonalisable over $K$ if and only if

$$ m_\alpha (t)=(t-\lambda _1)(t-\lambda _2)\cdots (t-\lambda _k) $$

for distinct $\lambda _1,\ldots ,\lambda _k\in K$.

Proof: Suppose first that $\alpha \in \End (V)$ is diagonalisable with distinct eigenvalues $\lambda _1,\ldots ,\lambda _k$. Let $V_i=E_\alpha (\lambda _i)$ be the $\lambda _i$-eigenspace, of dimension $n_i$. Pick a basis $\cV _i$ for $V_i$: then $\cV =\bigcup _{i=1}^k\cV _i$ is a basis for $V$, with respect to which $\alpha$ has matrix

$$ A=\begin {pmatrix} \lambda _1\II _{n_1} & & & \\ & \lambda _2\II _{n_2} & & \\ & & \ddots & \\ & & & \lambda _k\II _{n_k} \end {pmatrix}. $$

Then $m_{A_i}(t)=t-\lambda _i$, so (by Lemma V.28) $m_A(t)=(t-\lambda _1)(t-\lambda _2)\cdots (t-\lambda _k)$.

For the converse, we apply Theorem V.40 with $q_i=t-\lambda _i$ for $1\leq i\leq k$ to obtain

$$ V = \Ker (\alpha -\lambda _1\id )\oplus \cdots \oplus \Ker (\alpha -\lambda _k\id ) = E_\alpha (\lambda _1)\oplus \cdots \oplus E_\alpha (\lambda _k), $$

so $V$ must therefore have a basis consisting of eigenvectors of $\alpha$, i.e., $\alpha$ is diagonalisable.

##### Jordan normal form

**5.42** Suppose that $\alpha \in \End (V)$ and assume that $K$ is large enough to contain all the eigenvalues of $\alpha$ (for example, that is the case if $K=\CC$). Then we can decompose the minimal polynomial as

$$ m_\alpha (t)=(t-\lambda _1)^{s_1}\cdot (t-\lambda _2)^{s_2}\cdots (t-\lambda _k)^{s_k} $$

where $\lambda _1,\ldots ,\lambda _k$ are the distinct eigenvalues of $\alpha$ (recall that the roots of $m_\alpha$ are exactly the eigenvalues of $\alpha$).

The Primary Decomposition Theorem V.40 implies that

$$ V=\Ker (\alpha -\lambda _1\id )^{s_1}\oplus \cdots \oplus \Ker (\alpha -\lambda _k\id )^{s_k} $$

is a decomposition of $V$ as a direct sum of $\alpha$-invariant subspaces.

**5.43 Definition.** Let $\alpha \colon V\to V$ be a linear map with eigenvalue $\lambda$. A nonzero vector $v\in V$ is a _generalised eigenvector_ with respect to $\lambda$ if $(\alpha -\lambda \id )^sv=0$ for some positive integer $s$. The _generalised $\lambda$-eigenspace_ of $V$ is

$$ G_\alpha (\lambda ) = \big \{v\in V\mid (\alpha -\lambda \,\id )^sv=0\,\text { for some }s\in \NN \big \}. $$

In particular, $E_\alpha (\lambda )\subseteq G_\alpha (\lambda )$.

**5.44 Lemma.** Let $s$ be the multiplicity of the eigenvalue $\lambda$ as a root of $m_\alpha$. (This is neither the algebraic nor the geometric multiplicity!) Suppose $s’\ge s$. Then $G_\alpha (\lambda )=\Ker (\alpha -\lambda \,\id )^{s’}$.

The right hand side is contained in the left by Definition V.43. For the opposite inclusion, suppose $m_\alpha (t)=(t-\lambda _1)^{s_1} (t-\lambda _2)^{s_2}\cdots (t-\lambda _k)^{s_k}$, and put $V_i=\Ker (\alpha -\lambda _i\id )^{s_i}$ and $\alpha _1=\alpha |_{V_i}$. By the Primary Decomposition Theorem V.40 we have $V=V_1\oplus \cdots \oplus V_k$, and $m_{\alpha _i}(t)=(t-\lambda _i)^{s_i}$. Now suppose that $\lambda =\lambda _i$, and $j\neq i$. Then $\Ker (\alpha _j-\lambda _i\id )=\{0\}$ because $\lambda _j\neq \lambda _i$ is the only eigenvalue of $\alpha _j$.

If $v\in G_\alpha (\lambda )$ we may decompose it as $v=v_1+\cdots +v_k$ with $v_i\in V_i$. If $(\alpha -\lambda _i\id )^{s’}v=0$ then $(\alpha _1-\lambda _i\id )^{s’}v_1+\cdots + (\alpha _k-\lambda _i\id )^{s’}v_k=0$, but these are components in a direct sum so each $(\alpha _j-\lambda _i\id )^{s’}v_j=0$. That implies $v_j=0$ for $j\neq i$, so $v\in V_i$.

We deduce that $G_\alpha (\lambda _i)\subseteq \Ker (\alpha -\lambda _i\id )^{s_i}$; but $(\alpha -\lambda _i\id )^{s_i}v=0$ clearly implies that $(\alpha -\lambda _i\id )^{s’}v=0$ for any $s’\geq s_i$, it follows that $G_\alpha (\lambda _i)\subseteq \Ker (\alpha - \lambda _i\id )^{s’}$ as required.

**5.45** Lemma V.44 implies in particular that $G_\alpha (\lambda )=\Ker (\alpha -\lambda \,\id )^{\am (\lambda )}$. This is useful for calculating $G_\alpha (\lambda )$ as it is easier to determine $\chi _\alpha (t)$ than $m_\alpha (t)$.

**5.46 Theorem.** Suppose that the characteristic and minimal polynomials are $\chi _\alpha (t) = \prod _{1\leq i\leq k} (\lambda _i-t)^{r_i}$ and $m_\alpha (t) = \prod _{1\leq i\leq k} (t-\lambda _i)^{s_i}$ respectively. Then

$$ V=G_\alpha (\lambda _1)\oplus \cdots \oplus G_\alpha (\lambda _k), $$

and if $\alpha =\alpha _1\oplus \cdots \oplus \alpha _k$ is the corresponding decomposition of $\alpha$, then $\chi _{\alpha _i}(t)=(\lambda _i-t)^{r_i}$ and $m_{\alpha _i}(t)=(t-\lambda _i)^{s_i}$.

Proof: Almost everything follows directly from the Primary Decomposition Theorem V.40 and Lemma V.44. It remains to prove that $\chi _{\alpha _i}(t)=(\lambda _i-t)^{r_i}$. To see this, Corollary V.21 shows that the roots of $m_{\alpha _i}$ are exactly the eigenvalues of $\alpha _i$, so $\chi _{\alpha _i}(t)= (\lambda _i-t)^{r’_i}$ for some positive integer $r’_i$. We have that $\alpha =\alpha _1\oplus \cdots \oplus \alpha _k$ from Theorem V.40, and hence, writing $l_i=\dim G_\alpha (\lambda _i)$, we get a block decomposition $A=A_1\oplus \cdots \oplus A_k$ where $A_i\in M_{l_i}(K)$ is any matrix for the map $\alpha _i$. Therefore

$\seteqnumber{0}{}{0}$

$$ \begin{align*} (\lambda _1-t)^{r_1}\cdots (\lambda _k-t)^{r_k} & = \chi _\alpha (t) & \\ & = \det (A-t\II _n) & \\ & = \det \big (A_1\oplus \cdots \oplus A_k - t(\II _{l_1}\oplus \cdots \oplus \II _{l_k})\big ) & \\ & = \det \big ( (A_1-t\II _{l_1})\oplus \cdots \oplus (A_k-t\II _{l_k})\big ) & \\ & = \det (A_1-t\II _{l_1}) \cdot \det (A_2-t\II _{l_2}) \cdots \det (A_k-t\II _{l_k}) & \\ & = \chi _{\alpha _1}(t)\cdots \chi _{\alpha _k}(t) & \\ & = (\lambda _1-t)^{r’_1}\cdots (\lambda _k-t)^{r’_k} & \end{align*} $$ since the determinant of a direct sum equals the product of the determinants. Comparing exponents gives $r’_i=r_i$ for $i=1,\ldots ,k$ as required.

**5.47 Theorem.** (Jordan Normal Form Theorem) Suppose $\alpha \in \End (V)$ and $\chi _\alpha (t)=(\lambda _1-t)^{r_1}\cdots (\lambda _k-t)^{r_k}$ (in particular, $K$ contains all the eigenvalues of $\alpha$). Then there exists a basis of $V$ such that the matrix $A$ for $\alpha$ expressed in this basis is

$$ \JNF (\alpha )= \JNF (\alpha _1)\oplus \cdots \oplus \JNF (\alpha _k), $$

(in the notation of Theorem V.36, where $\alpha _i=\alpha |_{G_\alpha (\lambda _i)}$.

Proof: This now follows immediately from the Primary Decomposition Theorem V.40, which gives $V=G_\alpha (\lambda _1)\oplus \cdots \oplus G_\alpha (\lambda _k)$ with the corresponding decomposition $\alpha =\alpha _1\oplus \cdots \oplus \alpha _n$.

It is easy to see that the Jordan blocks in $\JNF (\alpha )$ are unique up to the order in which we write the blocks.