[[next](MA10210se24.html)] [[prev](MA10210se22.html)] [[prev-tail](MA10210se22.html#tailMA10210se22.html)] [[tail](#tailMA10210se23.html)] [[up](MA10210ch5.html#MA10210se23.html)]

### 5.3 [Multiplicities of eigenvalues and diagonalisation](MA10210.html#QQ2-30-44)

Note that not all linear operators/forwardslash matrices are diagonalisable. We already saw an instance of this in Example [5.2.4](MA10210se22.html#x29-42008r4). A linear operator can fail to be diagonalisable is because it does not have sufficient many independent eigenvectors. For instance, when the characteristic polynomial of the operator has no roots and so no eigenvectors (see Example [5.2.4](MA10210se22.html#x29-42008r4)). In this sections, we will discuss when a linear operator/matrix is diagonalisable using multiplicities of its eigenvalues.

Example 1.  

$$
\begin{align}
 & & A=\begin{pmatrix} \begin{matrix}3 & 1 \\ 0 & 3\end{matrix} \end{pmatrix}\in M_{2 , 2}\left( ℂ \right) & \text{} \\ & & \left(\Delta\right)_{A}\left( t \right)=\begin{vmatrix} \begin{matrix}3-t & 1 \\ 0 & 3-t\end{matrix} \end{vmatrix}=\left(\left( 3 - t \right)\right)^{2}, & \text{}
\end{align}
$$

so $\lambda=3$ is the only eigenvalue

$$
\begin{align}
ker \left( A - 3 I \right) = ker \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} = \left<\begin{matrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} \end{matrix}\right> ,
\end{align}
$$

1-dimensional. Therefore there are not 2 independent eigenvectors, so no eigenbasis.

Alternatively, argue that since the only eigenvalue of $A$ is $3$ , the only diagonal matrix that $A$ could possibly be similar to would be $D=\begin{pmatrix} 3 & 0 \\ 0 & 3 \end{pmatrix}$ . But in this case

$$
P D P^{- 1} = D
$$

for all $P$ , so $A$ is not similar to $D$ , and hence not diagonalisable.

In this section, we will make precise how the non-diagonalisability of Example [5.3.1](#x30-44001r1) can be explained in terms of there being “too few” eigenvectors with eigenvalue 3.

#### [Independence of eigenvectors](MA10210li1.html#QQ2-30-45)

Observe: if $dimV=n$ and $\phi:V\rightarrow V$ , then to have an eigenbasis we just need $n$ independent eigenvectors.

Theorem 2.  
If $v_{1},\dots ,v_{m}$ are eigenvectors of $\phi:V\rightarrow V$ with distinct eigenvalues $\left(\lambda\right)_{1},\dots ,\left(\lambda\right)_{m}$ , then $v_{1},\dots ,v_{m}$ are linearly independent.

Proof.

Suppose for contradiction that there is a non-trivial dependence. Then there is a shortest one, which (after reordering) can suppose is

$$
a_{1} v_{1} + \dots  + a_{k} v_{k} = 0
$$

with all $a_{i}\neq 0$ .

As $v_{i}\neq 0$ for all $i$ , we must have $k\geq 2$ . Then

$$
\begin{align}
0 & = & \phi\left( a_{1} v_{1} + \dots  + a_{k} v_{k} \right) & \text{} \\ & = & a_{1}\phi\left( v_{1} \right)+\dots +a_{k}\phi\left( v_{k} \right) & \text{} \\ & = & a_{1}\left(\lambda\right)_{1}v_{1}+\dots +a_{k}\left(\lambda\right)_{k}v_{k}. & \text{}
\end{align}
$$

But also

$$
\begin{align}
0 & = & \left(\lambda\right)_{k}\left( a_{1} v_{1} + \dots  + a_{k} v_{k} \right) & \text{} \\ & = & a_{1}\left(\lambda\right)_{k}v_{1}+\dots +a_{k}\left(\lambda\right)_{k}v_{k}. & \text{}
\end{align}
$$

Therefore

$$
a_{1} \left( \left(\lambda\right)_{k} - \left(\lambda\right)_{1} \right) v_{1} + \dots  + a_{k - 1} \left( \left(\lambda\right)_{k} - \left(\lambda\right)_{k - 1} \right) v_{k - 1} = 0 ,
$$

which is a shorter non-trivial dependence, since $\left(\lambda\right)_{k}\neq\left(\lambda\right)_{i}$ for all $i<k$ , a contradiction. □

Corollary 3.  
If $\left(\Delta\right)_{\phi}\left( t \right)=\left( \left(\lambda\right)_{1} - t \right)\dots \left( \left(\lambda\right)_{n} - t \right)$ with $\left(\lambda\right)_{1},...,\left(\lambda\right)_{n}$ distinct, then $\phi:V\rightarrow V$ is diagonalisable.

Proof. By the assumption, we have $n=dimV$ (using Lemma [4.5.5](MA10210se20.html#x26-39009r5)) and the eigenvalues are $\left(\lambda\right)_{1},\dots ,\left(\lambda\right)_{n}$ . Note that each eigenvalue $\left(\lambda\right)_{i}$ has at least one eigenvector $v_{i},\forall i$ . By Theorem [5.3.2](#x30-45001r2), the eigenvectors $v_{1},\dots ,v_{n}$ are linearly independent. Hence they form a maximal independent list, as $dimV=n$ , and thus an eigenbasis. Therefore $\phi$ is diagonalisable. □

#### [Multiplicities and diagonalisability](MA10210li1.html#QQ2-30-46)

Suppose $\lambda$ is an eigenvalue of $\phi:V\rightarrow V$ ( $V$ finite dimensional).

Definition 4.  

(a)

The algebraic multiplicity $a.m.\left( \lambda \right)$ is the largest power of $\left( \lambda - t \right)$ that divides $\left(\Delta\right)_{\phi}\left( t \right)$ .

(b)

The geometric multiplicity $g.m.\left( \lambda \right)$ is $dimE_{\phi}\left( \lambda \right)$

Remark 5.  

1.

$dimE_{\phi}\left( \lambda \right)=\text{max \# indep.  }\lambda\text{-eigenvectors}.$

2.

If $\lambda$ is a root of $\left(\Delta\right)_{\phi}\left( t \right)$ , then $t-\lambda$ divides $\left(\Delta\right)_{\phi}\left( t \right)$ . So $a.m.\left( \lambda \right)\geq 1$ , as every eigenvalue is a root of $\left(\Delta\right)_{\phi}\left( t \right)$ .

3.

$g.m.\left( \lambda \right)\geq 1$ , as every eigenvalue has at least one eigenvector $v\neq 0$ .

Proposition 6.  

$$
a.m. \left( \lambda \right) \geq g.m. \left( \lambda \right)
$$

Proof. Choose a basis $v_{1},\dots ,v_{k}$ for $E_{\phi}\left( \lambda \right)$ and extend it to a basis $v_{1},\dots ,v_{n}$ of $V$ . Since

$$
\phi \left( v_{i} \right) = \lambda v_{i} , i = 1 , \dots  , k ,
$$

the matrix representing $\phi$ with respect to this basis is of the form

$$
\begin{align}
 \begin{pmatrix} \begin{matrix}\lambda & & 0 \\ & \ddots & \\ 0 & & \lambda\end{matrix} & * \\ ̲ & ̲ \\ 0 & A^{′} \end{pmatrix}
\end{align}
$$

So by the formula of the determinant of a block triangular matrix (see Proposition [4.1.3](MA10210se16.html#x22-35012r3))

$$
\left(\Delta\right)_{\phi} \left( t \right) = \left(\left( \lambda - t \right)\right)^{k}  \det  \left( A^{′} - t I_{n - k} \right)
$$

and thus

$$
a.m. \left( \lambda \right) \geq k = g.m. \left( \lambda \right) . \square
$$

Theorem 7.  
An operator $\phi:V\rightarrow V$ is diagonalisable if and only if $\left(\Delta\right)_{\phi}\left( t \right)$ is a product of linear factors and $a.m.\left( \lambda \right)=g.m.\left( \lambda \right)$ for all eigenvalues $\lambda$ .

Proof. Let $n$ be the dimension of $V$ .

( $\Rightarrow$ ) Suppose the diagonal matrix

$$
\begin{align}
D = \begin{pmatrix} \left(\lambda\right)_{1} & & 0 \\ & \ddots & \\ 0 & & \left(\lambda\right)_{n} \end{pmatrix}
\end{align}
$$

represents $\phi$ in the eigenbasis $v_{1},\dots ,v_{n}$ . Then

$$
\begin{align}
\left(\Delta\right)_{\phi}\left( t \right) & = & \left(\Delta\right)_{D}\left( t \right) & \text{} \\ & = & \left( \left(\lambda\right)_{1} - t \right)\dots \left( \left(\lambda\right)_{n} - t \right), & \text{}
\end{align}
$$

and so

$$
\begin{align}
a.m.\left( \lambda \right) & =\#\left\{ i : \left(\lambda\right)_{i} = \lambda \right\} & & \\ & =\#\left\{ v_{i} : v_{i} \text{ is a } \lambda \text{-eigenvector} \right\} & & \\ & =g.m.\left( \lambda \right). & \square
\end{align}
$$

( $\Leftarrow=$ ) Suppose

$$
\left(\Delta\right)_{\phi} \left( t \right) = \left(\left( \left(\lambda\right)_{1} - t \right)\right)^{k_{1}} \dots  \left(\left( \left(\lambda\right)_{r} - t \right)\right)^{k_{r}}
$$

with $\left(\lambda\right)_{1},\dots ,\left(\lambda\right)_{r}$ distinct.

Then

$$
k_{1} + \dots  + k_{r} = deg \left(\Delta\right)_{\phi} \left( t \right) = dim V = n
$$

and

$$
k_{i} = a.m. \left( \left(\lambda\right)_{i} \right) = g.m. \left( \left(\lambda\right)_{i} \right)
$$

by assumption.

For each $i$ choose a basis $v_{1}^{\left( i \right)},\dots ,v_{k_{i}}^{\left( i \right)}$ for $E_{\phi}\left( \left(\lambda\right)_{i} \right)$ and combine them to a list $v_{1}^{\left( 1 \right)},\dots ,v_{k_{r}}^{\left( r \right)}$ of $n$ vectors. Suppose

$$
\sum_{i = 1}^{r} \sum_{j = 1}^{k_{i}} a_{i}^{\left( i \right)} v_{j}^{\left( i \right)} = 0 .
$$

Let

$$
w_{i} = \sum_{j = 1}^{k_{i}} a_{i}^{\left( i \right)} v_{j}^{\left( i \right)} ,
$$

which is a vector in $\in E_{\phi}\left( \left(\lambda\right)_{i} \right)$ . Then $w_{i}$ is either $0$ or a $\left(\lambda\right)_{i}$ -eigenvector and

$$
\begin{align}
\sum_{i = 1}^{r}w_{i}=0. & & & \text{}
\end{align}
$$

By Theorem [5.3.2](#x30-45001r2), the linear dependence is not possible unless $w_{i}=0,\forall i$ . But then for any $i$ : $a_{j}^{\left( i \right)}=0$ for all $j$ , as $v_{1}^{\left( i \right)},\dots ,v_{k_{i}}^{\left( i \right)}$ are a basis. So the combined list is a basis and thus an eigenbasis. Hence $\phi$ is diagonalisable.

Example 8.  
Let

$$
\begin{align}
A = \begin{pmatrix} 3 & 0 & 0 \\ 0 & 5 & 1 \\ 0 & 0 & 5 \end{pmatrix}
\end{align}
$$

The eigenvalues are $3$ and $5$ with $a.m.\left( 3 \right)=1$ and $a.m.\left( 5 \right)=2$ . Note that

$$
\begin{align}
\left( A - 5 I \right) = \begin{pmatrix} -2 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix}
\end{align}
$$

and so $ker\left( A - 5 I \right)=\langle e_{2} \rangle,$ 1-dimensional. Therefore $g.m.\left( 5 \right)=1\neq a.m.\left( 5 \right)$ and so $A$ is not diagonalisable.

[[next](MA10210se24.html)] [[prev](MA10210se22.html)] [[prev-tail](MA10210se22.html#tailMA10210se22.html)] [[front](MA10210se23.html)] [[up](MA10210ch5.html#MA10210se23.html)]