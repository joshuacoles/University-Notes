### 3 [Polynomials and Matrices](MA10209-web.html#QQ2-7-13)

For these, we wish to work over a general ’field of scalars’ such as $ℚ , ℝ , ℂ$ (but not $ℤ$ ). Abstractly, a **field** is a set $𝔽$ equipped with two binary operations: sum ( $+$ ) and product ( $\cdot$ or nothing) satisfying the following ’extended’ field axioms:

F1

$+$ is associative: $\forall x , y , z \in 𝔽$ $\left( x + y \right) + z = x + \left( y + z \right)$

F2

There is an additive identity $0 \in 𝔽$ s.t. $0 + x = x = x + 0$

F3

Every $x \in 𝔽$ has an additive inverse $- x$ such that $x + \left( - x \right) = 0 = - x + x$ .

F3x

$\forall x \in 𝔽$ $- \left( - x \right) = x$

F4

$+$ is commutative $\forall x , y \in 𝔽$ $x + y = y + x$

With just (F1)-(F4), we say $𝔽$ (with $+ , 0$ ) is an **additive group**.

F5

$\cdot$ is distributive over $+$ : $\forall a , x , y \in 𝔽$ 
$$
\begin{align}
 & & a\cdot\left( x + y \right)=a\cdot x+a\cdot y & \text{} \\ & & \left( x + y \right)\cdot a=x\cdot a+y\cdot a & \text{}
\end{align}
$$

F5x

$\forall a , x \in 𝔽$ 
$$
\begin{align}
 & & a\cdot 0=0=0\cdot a & \text{} \\ & & a\cdot\left( - x \right)=-a\cdot x & \text{} \\ & & \left( - x \right)\cdot a=-x\cdot a & \text{}
\end{align}
$$

F6

$\cdot$ is associative $\forall x , y , z \in 𝔽$ $\left( x \cdot y \right) \cdot z = x \cdot \left( y \cdot z \right)$

F7

There is a multiplicative identity $1 \in 𝔽$ , $\forall x \in 𝔽$ $1 \cdot x = x = x \cdot 1$ .

F8

$\cdot$ is commutative $\forall x , y \in 𝔽$ $x \cdot y = y \cdot x$

F9

$1 \neq 0$ and $\forall x \in 𝔽 \backslash \left\{ 0 \right\}$ there is a multiplicative inverse $x^{- 1}$ , s.t. $x^{- 1} \cdot x = 1 = x \cdot x^{- 1}$ .

F9x

$\forall x \in 𝔽 \backslash \left\{ 0 \right\}$, $\left(\left( x^{- 1} \right)\right)^{- 1} = x$ .

If (F1)-(F7) hold, we say $𝔽$ is a **ring** (some add ’with $1$ ’ for emphasis). If (F1)-(F8) hold, we say $𝔽$ is a **commutative ring**.

Example 3.1.  
  
$ℤ$ , $ℤ_{n}$ are commutative rings. $ℚ , ℝ$ and $ℂ$ are fields, also $ℤ_{p}$ is a field, for $p$ prime.

In a commutative ring $R$ , we say the **cancellation law** holds if any of the following equivalent properties holds (for all $a , x , y \in R$ )

(i)

$\left( a x = a y \text{ and } a \neq 0 \right) \Rightarrow x = y$

(ii)

$\left( a x = 0 \text{ and } a \neq 0 \right) \Rightarrow x = 0$

(iii)

$a x = 0 \Rightarrow \left( a = 0 \text{ or } x = 0 \right)$

(iv)

$\left( a \neq 0 \text{ and } x \neq 0 \right) \Rightarrow a x \neq 0$

Here (ii), (iii), (iv) are logically equivalent, while (i) $\Rightarrow$ (ii) by setting $y = 0$ . For (ii) $\Rightarrow$ (i):

$$
\begin{align}
ax=ay & \Rightarrow & ax-ay=a\left( x - y \right)=0 & \text{} \\ & \Rightarrow & x-y=0\text{by (ii)} & \text{} \\ & \Rightarrow & x=y & \text{}
\end{align}
$$

The cancellation law holds in any field, by multiplying $a x = a y$ through by $a^{- 1}$ and using (F6, 7, 9). However, the cancellation law also holds in $ℤ$ , so is weaker than (F9).

#### 3.1 [Polynomials](MA10209-web.html#QQ2-7-14)

Definition 3.2.  
  
Let $𝔽$ be any field. A polynomial in a variable $X$ with coefficients in $𝔽$ is a formal expression

$$
f = \sum_{n = 0}^{\in fty} a_{n} X^{n} ,
$$

where the coefficients $a_{n} \in 𝔽$ and all but finitely many $a_{n}$ are zero. Two polynomials are equal if and only if their coefficients are equal. We write $𝔽 \left[ X \right]$ for the set of all polynomials. We can write $f$ as $f \left( X \right)$ if the variable needs to be specified, but usually it isn’t.

In practice, we write polynomials more simply using the rules in a commutative ring such as

$$
X^{0} = 1 , X^{1} = X , 1 X^{n} = X^{n} , 0 X^{n} = 0
$$

and we omit all large powers with coefficients $a_{n} = 0$ , e.g.

$$
f = 2 + X + 5 X^{3} = 2 X^{0} + 1 X^{1} + 0 X^{2} + 5 X^{3}
$$

and implicitly $a_{n} = 0$ , for $n \geq 4$ .

We add and multiply polynomials as if $X$ were an unknown element of $𝔽$ (including possibly $X = 0$ , so there is no $X^{- 1}$ ). Thus, if $g = \sum_{n = 0}^{\in fty} b_{n} X^{n}$ , then

$$
f + g = \sum_{n = 0}^{\in fty} \left( a_{n} + b_{n} \right) X^{n} , f \cdot g = \sum_{n = 0}^{\in fty} \left(\sum_{k = 0}^{n} a_{k} b_{n - k}\right) X^{n} .
$$

For example, $\left( X + 1 \right) + \left( X + 2 \right) = 2 X + 3$ , while $\left( X + 1 \right) \left( X + 2 \right) = X^{2} + 3 X + 2$ .

Every $a \in 𝔽$ determines a constant polynomial with $a_{0} = a$ and $a_{n} = 0$ for $n > 0$ . These add and multiply as in $𝔽$ . The constant polynomials $0$ and $1$ are the additive and multiplicative identities in $𝔽 \left[ X \right]$ . Also $- f$ is the polynomial with coefficients $- a_{n}$ for all $n \in ℕ_{0}$ . As a consequence $𝔽 \left[ X \right]$ is a commutative ring (but not a field). In fact, the only invertible polynomials are the non-zero constant polynomials.

= Lecture 20 =

Remark.  
  
A polynomial $f \in 𝔽 \left[ X \right]$ determines an ’evaluation’ function

$$
𝔽 \rightarrow 𝔽 : \alpha \rightarrow tail f \left( \alpha \right) = \sum_{n = 0}^{\in fty} a_{n} \left(\alpha\right)^{n}
$$

that is, setting $X = \alpha$ .

However, we don’t consider $f$ to be equal to this function, despite the notation. For example, if $𝔽 = ℤ_{p}$ , then the set of functions $\left(𝔽\right)^{𝔽}$ is finite of size $p^{p}$ , but $𝔽 \left[ X \right]$ is always infinite, e.g. over $ℤ_{2}$

$$
1 , X , X^{2} , X^{3} , 1 + X , 1 + X + X^{2} , \dots 
$$

are all different elements of $ℤ_{2} \left[ X \right]$ , but don’t all define different functions. Indeed, recall that, by Fermat’s Little Theorem (Prop. [2.20](MA10209-webse2.html#x6-11008r20)), we know $\left(\alpha\right)^{p} = \alpha$ for all $\alpha \in ℤ_{p}$ .

Definition 3.3.  
  
For any non-zero $f \in 𝔽 \left[ X \right]$ :

*   the **leading term** is $a_{k} X^{k}$ if $a_{k} \neq 0$ but $a_{n} = 0$ for $n > k$ .
*   then say $k$ is the **degree** of $f$ , written $deg f = k$ and say $f$ is **monic** if $a_{k} = 1$
*   by convention $deg \left( 0 \right) = - \in fty < n$ for any $n \in ℕ_{0}$ and $0$ is not monic.

Note that, for any non-zero $f$ , with leading coefficient $a_{k}$ , the polynomial $p = a_{k}^{- 1} f$ is monic and $f = a_{k} p$ . For example, $f = 2 X^{2} - 3 X + 1 = 2 \left( X^{2} - \frac{3}{2} X + \frac{1}{2} \right)$ .

Lemma 3.4.  
  
For non-zero $f , g \in 𝔽 \left[ X \right]$ , $f g \neq 0$ and further,

$$
deg \left( f g \right) = deg \left( f \right) + deg \left( g \right) .
$$

Proof. Since $f , g$ are non-zero we can write

$$
\begin{align}
f & =a_{0}+a_{1}X+\dots +a_{m}X^{m},\text{with }a_{m}\neq 0\text{} & & \\ g & =b_{0}+b_{1}X+\dots +b_{n}X^{n},\text{with }b_{n}\neq 0\text{} & & 
\end{align}
$$

Thus $deg g = n$ and the leading term of $f g$ is $a_{m} b_{n} X^{n + m}$ and $a_{m} b_{n} \neq 0$ , because the cancellation law holds in $𝔽$ . Thus $f g \neq 0$ because it has a non-zero coefficient and $deg \left( f g \right) = m + n$ , as required. □

Note that this means that the cancellation law holds in $𝔽 \left[ X \right]$ . Note also that the convention $deg \left( 0 \right) = - \in fty$ is consistent, provided we also agree that $- \in fty + n = - \in fty$ for $n \in ℕ_{0}$ or $n = - \in fty$.

Definition 3.5.  
  
For $f , g \in 𝔽 \left[ X \right]$ , $f$ **divides** $g$ (write $f | g$ ) if $f h = g$ , for some $h \in 𝔽 \left[ X \right]$ .

As for $ℤ$ , ’divides’ is reflexive and transitive, but not symmetric. Also we can get an equivalence relation by setting $f sim g$ iff $f | g$ and $g | f$ .

Lemma 3.6.  
  
For $f , g , h \in 𝔽 \left[ X \right]$ .

(a)

if $f | g \neq 0$ , then $deg \left( f \right) \leq deg \left( g \right)$ .

(b)

$1 | f$ and $f | 0$ but $0 | f \Rightarrow f = 0$ and $f | 1 \Rightarrow f \in \left(𝔽\right)^{*} = 𝔽 \backslash \left\{ 0 \right\}$ as a constant polynomial.

(c)

if $f , g \neq 0$ and $f | g$ and $g | f$ , then $f = \lambda g$ for some $\lambda \in \left(𝔽\right)^{*}$

(d)

if $h | f$ and $h | g$ , then $h | \lambda f + \mu g$ for any $\lambda , \mu \in 𝔽 \left[ X \right] .$

Proof. 

(a)

if $f h = g \neq 0$ , then $f , h \neq 0$ , and $deg \left( g \right) = deg \left( f \right) + deg \left( h \right)$ (Lemma [3.4](#x7-14003r4)), so $deg \left( g \right) \geq deg \left( f \right)$ as $deg \left( h \right) \geq 0$ .

(b)

$f = 1 \cdot f$ and $0 = f \cdot 0$ . However, if $f = 0 \cdot h$ , then $f = 0$ and, if $1 = f \cdot h$ , then $deg \left( f \right) + deg \left( h \right) = deg \left( 1 \right) = 0$ , so $deg \left( f \right) = deg \left( h \right) = 0$ . Hence $f , h \in 𝔽$ are constant polynomials and indeed $f , h \in \left(𝔽\right)^{*}$ as they are invertible.

(c)

if $g = \mu f$ and $f = \lambda g$ for some $\lambda , \mu \in 𝔽 \left[ X \right]$ , then $g = \lambda \mu g$ and so $\lambda \mu = 1$ , by cancellation law. Hence $\lambda$ (and $\mu$ ) $\in \left(𝔽\right)^{*}$ as in (b).

(d)

if $f = h x$ and $g = h y$ for $x , y \in 𝔽 \left[ X \right]$ , then $\lambda f + \mu g = \left( \lambda x + \mu y \right) h$ , as required.

□

Definition 3.7.  
  
For $f , g \in 𝔽 \left[ X \right]$ , say $h$ is **a greatest common divisor** ( $gcd$ ) of $f , g$ if

$h | f$ and $h | g$ and, for all $x \in 𝔽 \left[ X \right]$ , if $x | f$ and $x | g$ , then $x | h$ .

Say $k$ is a **least common multiple** ( $lcm$ ) of $f , g$ if

$f | k$ and $g | k$ and, for all $x \in 𝔽 \left[ X \right]$ , if $f | x$ and $g | x$ , then $k | x$ .

Say $f , g$ are **coprime** if $1$ is a $gcd$ of $f , g$ , that is, $x | f \text{ and } x | g \Leftrightarrow x \in \left(𝔽\right)^{*}$.

Just as for the integers $ℤ$ , because “divides” is reflexive and transitive, we can shorten these definitions to

*   $h$ is a gcd of $f , g$ when $\forall x \in 𝔽 \left[ X \right] : x | f \text{ and } x | g \Leftrightarrow x | h$
*   $k$ is an lcm of $f , g$ when $\forall x \in 𝔽 \left[ X \right] : f | x \text{ and } g | x \Leftrightarrow k | x$

but note that we have not yet shown that gcd’s and lcm’s exist.

= Lecture 21 =

If $h$ and $h^{′}$ are both $gcd$ s of $f , g$ , then $h^{′} | h$ , because $h$ is gcd and $h^{′}$ is a common divisor and similarly $h | h^{′}$, so that $h^{′} = \lambda h$ for some $\lambda \in \left(𝔽\right)^{*}$ (Lemma [3.6](#x7-14005r6)(c)). Conversely, we can replace a gcd $h$ by $h^{′} = \lambda h$ , for $\lambda \in \left(𝔽\right)^{*}$, and it will remain a gcd. Thus we can always make a gcd unique by requiring that it is monic. Similar remarks apply to lcm’s.

Proposition 3.8 (Division Lemma for Polynomials).  
  
Let $f , g \in 𝔽 \left[ X \right]$ with $g \neq 0$ . There are unique $q , r \in 𝔽 \left[ X \right]$ such that $f = q g + r$ and $deg r < deg g$ (including $r = 0$ ).

Proof. If $deg f < deg g$ (inc. $f = 0$ ), then $q = 0$ , $r = f$ is a solution and is unique otherwise $deg \left( q g \right) \geq deg g$ . For $deg f \geq deg g$ proceed by induction on $deg f$ . Suppose $f$ and $g$ have leading terms $a X^{m}$ and $b X^{n}$ respectively, so $a , b$ are non-zero and $m \geq n$ . As the coefficients are in a field $𝔽$ , $b$ is invertible, so we can consider

$$
f_{1} = f - \frac{a}{b} X^{m - n} \cdot g ,
$$

so the leading terms on the RHS cancel and $deg f_{1} < deg f$ . Hence, by the inductive hypothesis, $f_{1} = q_{1} g + r$ uniquely with $deg r < deg g$ , and so

$$
f = f_{1} + \frac{a}{b} X^{m - n} g = \left(\frac{a}{b} X^{m - n} + q_{1}\right) g + r
$$

and we have $q = \frac{a}{b} X^{m - n} + q_{1}$ , as required. □

Note that the proof gives a recursive method for calculation. With practice, this can be done in one line, matching each degree in turn. For example, if $f = X^{4} + 3$ and $g = 2 X^{2} + X + 2$ , then

$$
X^{4} + 3 = \left( 2 X^{2} + X + 2 \right) \left(\frac{1}{2} X^{2} - \frac{1}{4} X - \frac{3}{8}\right) + \frac{7}{8} X + 3 \frac{3}{4}
$$

Notice that we do need to allow coefficients to be in a field ( $ℚ$ in this case) for the computation to work.

Corollary 3.9.  
  
If $f \in 𝔽 \left[ X \right]$ and $\alpha \in 𝔽$ , then

$$
f = \left( X - \alpha \right) q + f \left( \alpha \right) \text{for some }q\in𝔽\left[ X \right]\text{}
$$

Hence, $\alpha$ is a root of $f$ , that is $f \left( \alpha \right) = 0$ , if and only if $\left( X - \alpha \right) | f$ .

Proof. By the Division Lemma (Prop. [3.8](#x7-14007r8)), $f = \left( X - \alpha \right) q + r$ for some $q , r \in 𝔽 \left[ X \right]$ with $deg r < deg \left( X - \alpha \right) = 1$ . Hence $r \in 𝔽$ is a constant polynomial and we see that $r = f \left( \alpha \right)$ by evaluating at $\alpha$ . The last part then follows immediately. □

Proposition 3.10 (Bezout’s Lemma for polynomials).  
  
For non-zero $f , g \in 𝔽 \left[ X \right]$ , let $h$ be a non-zero polynomial of lowest degree in the set

$$
S = \left\{\lambda f + \mu g : \lambda , \mu \in 𝔽 \left[ X \right]\right\} .
$$

Then $h$ is a $gcd$ of $f , g$ .

Proof. First, if $x | f$ and $x | g$ , then $x | h$ as $h \in S$ (Lemma [3.6](#x7-14005r6)(d)). Hence it remains to prove that $h | f$ and $h | g$ . Write $h = \lambda f + \mu g$ and $f = h q + r$ with $deg r < deg h$ , then

$$
r = f - h q = \left( 1 - \lambda q \right) f - \left( \mu q \right) g \in S .
$$

So can only have $deg r < deg h$ if $r = 0$ , i.e. $h | f$ . Similarly, $h | g$ . □

Corollary 3.11.  
  
In $𝔽 \left[ X \right]$ , $gcd$ s exist and we can find them by Euclid’s Algorithm.

Example 3.12.  
  
To find $gcd$ of $f = X^{9} - 1$ and $g = X^{6} - 1$

$$
\begin{align}
 & & \left( X^{9} - 1 \right)=\left( X^{6} - 1 \right)X^{3}+\left( X^{3} - 1 \right) & \text{} \\ & & \left( X^{6} - 1 \right)=\left( X^{3} - 1 \right)\left( X^{3} + 1 \right)+0\text{STOP} & \text{}
\end{align}
$$

Hence the (monic) gcd of $X^{9} - 1$ and $X^{6} - 1$ is $X^{3} - 1$ . In Euclid’s Algorithm, it can be helpful to rescale the polynomials to be monic, since this does not change the gcd.

Definition 3.13.  
  
A non-zero polynomial $f \in 𝔽 \left[ X \right]$ is **irreducible** iff $f \notin \left(𝔽\right)^{*}$ and if $f = g h$ then either $g \in \left(𝔽\right)^{*}$ and $h \in \left(𝔽\right)^{*}$. [We could say $deg f > 0$ and if $f = g h$ , then $deg g = 0$ or $deg h = 0$ ].

Compare this definition with when a positive $p \in ℤ$ is prime.

Proposition 3.14 (Euclid’s Lemma for polynomials).  
  
If $f \in 𝔽 \left[ X \right]$ and $f$ is irreducible, then

$$
f | g h \Rightarrow f | g \text{ or } f | h .
$$

Proof. Suppose $f | g h$ and $f \not| g$ . Since $f$ is irreducible, the common divisors of $f , g$ are just $\left(𝔽\right)^{*}$, so $1$ is a gcd. Hence we can write $\lambda f + \mu g = 1$ , so $\lambda f h + \mu g h = h$ and so, as $f | g h$ , we get $f | h$ , as required. □

Theorem 3.15 (Unique Factorisation ).  
  
Every non-zero $f \in 𝔽 \left[ X \right]$ can be written (uniquely up to order) as $f = a \prod_{i = 1}^{n} p_{i}$ , where $a \in \left(𝔽\right)^{*}$ and $p_{i}$ are monic and irreducible.

Proof. (Sketch). We can use the same argument as for $ℤ$ . Note that $a$ is just the leading coefficient of $f$ .

For existence: if $f \in \left(𝔽\right)^{*}$, then $n = 0$ . If $f$ is irreducible, then $n = 1$ and $p_{1} = f / a$ . Otherwise, $f = g h$ with $deg g$ and $deg h$ both $> 0$ and $< deg f$ , so we obtain a factorisation into irreducibles by induction on $deg f$

For uniqueness: use Euclid’s Lemma as for $ℤ$ . □

Remark.  
  
Since, over $ℂ$ , we have all the roots (by Fundamental Theorem of Algebra), the unique factorisation is

$$
f = a \prod_{i = 1}^{n} \left( X - \left(\alpha\right)_{i} \right)
$$

where $\left(\alpha\right)_{i}$ are the roots of $f$ . However, over $ℝ$ or $ℚ$ (or $ℤ_{p}$ ) irreducible factors may not be linear. For example, in $ℝ \left[ X \right]$ , we have $X^{3} - 1 = \left( X - 1 \right) \left( X^{2} + X + 1 \right)$ and both factors are irreducible, as $X^{2} + X + 1$ only has roots in $ℂ$ , so no linear factors in $ℝ \left[ X \right]$ .

= Lecture 22 =

#### 3.2 [Matrices](MA10209-web.html#QQ2-7-15)

Definition 3.16.  
  
An $n \times m$ matrix with coefficients in a field $𝔽$ is an array

$$
\begin{align}
A = \begin{pmatrix} a_{11} & \dots  & a_{1 m} \\ \vdots & & \vdots \\ a_{n 1} & \dots  & a_{n m} \end{pmatrix}
\end{align}
$$

with $n$ rows and $m$ columns and coefficients $a_{i j} \in 𝔽$ . We also write just $A = \left( a_{i j} \right)$ , when $n , m$ are understood. Two $n \times m$ matrices are equal when all coefficients are equal, that is, if $B = \left( b_{i j} \right)$ , then $A = B$ when $a_{i j} = b_{i j}$ for all $i , j$ .

Matrices of same size can be added and multiplied by a scalar $\lambda \in 𝔽$ coefficient-wise:

$$
A + B = \left( a_{i j} + b_{i j} \right) , \lambda A = \left( \lambda a_{i j} \right) .
$$

Thus $n \times m$ matrices form an additive group, as axioms (F1)-(F4) follow coefficient-wise. In particular the zero matrix has all coefficients $= 0$ . In fact, with scalar multiplication $n \times m$ matrices form a ‘vector space’ (see Algebra 1B).

The key operation is matrix multiplication: if $B = \left( b_{j k} \right)$ is an $m \times p$ matrix, then $A B = \left( u_{i k} \right)$ is an $n \times p$ matrix, where

$$
u_{i k} = \sum_{j = 1}^{m} a_{i j} b_{j k} .
$$

For example,

$$
\begin{align}
\begin{pmatrix} r & s & t \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = r x + s y + t z
\end{align}
$$

or

$$
\begin{align}
\begin{pmatrix} 1 & 2 & 0 \\ 0 & 3 & 4 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ 0 & 3 \\ 4 & 0 \end{pmatrix} = \begin{pmatrix} 1 & 8 \\ 16 & 9 \end{pmatrix}
\end{align}
$$

Lemma 3.17.  
  
Matrix multiplication is associative, i.e. if $C = \left( c_{k l} \right)$ is a $p \times q$ matrix, then

$$
\left( A B \right) C = A \left( B C \right) .
$$

Proof. We need to see that all the coefficients of $\left( A B \right) C$ are equal to those of $A \left( B C \right)$ . For the $i ℓ$ coefficient, that is

$$
\sum_{k = 1}^{p} \left(\sum_{j = 1}^{m} a_{i j} b_{j k}\right) c_{k ℓ} = \underset{j , k}{\sum} a_{i j} b_{j k} c_{k l} = \sum_{j = 1}^{m} a_{i j} \left(\sum_{k = 1}^{p} b_{j k} c_{k ℓ}\right)
$$

using the field axioms for coefficients, including the distributive (F5) and associative (F6) laws, and where the middle sum is over all pairs $\left( j , k \right) \in \left\{ 1 , \dots  , m \right\} \times \left\{ 1 , \dots  , p \right\}$. □

We also identify $\left(𝔽\right)^{m}$ with the set of $m \times 1$ matrices or ’column vectors’, in the natural way, that is,

$$
v = \left( v_{1} , \dots  , v_{m} \right) \in \left(𝔽\right)^{m} \text{ becomes } v = \begin{pmatrix} v_{1} \\ \vdots \\ v_{m} \end{pmatrix}
$$

Then an $n \times m$ matrix $A$ gives a map

$$
\left(\phi\right)_{A} : \left(𝔽\right)^{m} \rightarrow \left(𝔽\right)^{n} : v \rightarrow tail A v
$$

by matrix multiplication. In fact (see Algebra 1B), the maps that arise this way are precisely the **linear** maps $\phi : \left(𝔽\right)^{m} \rightarrow \left(𝔽\right)^{n}$ , that is, those that satisfy

$$
\phi \left( \lambda v + \mu w \right) = \lambda \phi \left( v \right) + \mu \phi \left( w \right)
$$

for all $\lambda , \mu \in 𝔽$ , $v , w \in \left(𝔽\right)^{m}$ .

Example 3.18.  
  
The matrix $A = \begin{pmatrix} a_{1} & b_{1} \\ a_{2} & b_{2} \end{pmatrix}$ with coefficients in $ℝ$ gives a map

$$
\begin{align}
\left(\phi\right)_{A} : ℝ^{2} \rightarrow ℝ^{2} : \begin{pmatrix} x \\ y \end{pmatrix} \rightarrow tail \begin{pmatrix} a_{1} & b_{1} \\ a_{2} & b_{2} \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} a_{1}x+b_{1}y \\ a_{2}x+b_{2}y \end{pmatrix} = x \begin{pmatrix} a_{1} \\ a_{2} \end{pmatrix} + y \begin{pmatrix} b_{1} \\ b_{2} \end{pmatrix} .
\end{align}
$$

In other words,

$$
\left(\phi\right)_{A} \left( x e_{1} + y e_{2} \right) = x \left(\phi\right)_{A} \left( e_{1} \right) + y \left(\phi\right)_{A} \left( e_{2} \right) ,
$$

where $e_{1} = \left( 1 , 0 \right)$ and $e_{2} = \left( 0 , 1 \right)$ are the standard unit vectors.

![No alt text was set. Please request alt text from the person who provided you with this resource.](./figures/pdf/MA10209-web-figure11.svg)

Note that $\left(\phi\right)_{A}$ determines $A$ as its columns are $\left(\phi\right)_{A} \left( e_{1} \right) = a$ and $\left(\phi\right)_{A} \left( e_{2} \right) = b$ .

More generally, the fact that $\left(\phi\right)_{A}$ is linear means that it maps all parallelogram grids to parallelogram grids, that is, $\left(\phi\right)_{A} \left( x v + y w \right) = x \left(\phi\right)_{A} \left( v \right) + y \left(\phi\right)_{A} \left( w \right)$ for $x , y \in ℝ$ and $v , w \in ℝ^{2}$ . This can be written as

$$
\begin{align}
\begin{pmatrix} a_{1} & b_{1} \\ a_{2} & b_{2} \end{pmatrix} \begin{bmatrix} \begin{pmatrix} v_{1} & w_{1} \\ v_{2} & w_{2} \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \end{bmatrix} = \begin{bmatrix} \begin{pmatrix} a_{1} & b_{1} \\ a_{2} & b_{2} \end{pmatrix} \begin{pmatrix} v_{1} & w_{1} \\ v_{2} & w_{2} \end{pmatrix} \end{bmatrix} \begin{pmatrix} x \\ y \end{pmatrix} ,
\end{align}
$$

which we see is a special case of the associativity of matrix multiplication.

Now return to a general $\left(\phi\right)_{A} : \left(𝔽\right)^{m} \rightarrow \left(𝔽\right)^{n}$ and consider a second map $\left(\phi\right)_{B} : \left(𝔽\right)^{p} \rightarrow \left(𝔽\right)^{m}$ , coming from an $m \times p$ matrix $B$ . Then the associativity equation

$$
A \left( B u \right) = \left( A B \right) u
$$

for any $u \in \left(𝔽\right)^{p}$ , thought of as a $p \times 1$ matrix, means that

$$
\left(\phi\right)_{A} \circ \left(\phi\right)_{B} = \left(\phi\right)_{A B}
$$

as functions $\left(𝔽\right)^{p} \rightarrow \left(𝔽\right)^{n}$ . Thus matrix multiplication corresponds to composition of maps, so that the associativity of composition means 
$$
\begin{align}
\left(\phi\right)_{\left( A B \right) C}=\left(\phi\right)_{A B}\circ\left(\phi\right)_{C} & =\left( \left(\phi\right)_{A} \circ \left(\phi\right)_{B} \right)\circ\left(\phi\right)_{C} & & \\ & =\left(\phi\right)_{A}\circ\left( \left(\phi\right)_{B} \circ \left(\phi\right)_{C} \right) & & \\ & =\left(\phi\right)_{A \left( B C \right)} & & 
\end{align}
$$

corresponding to the general associativity $\left( A B \right) C = A \left( B C \right)$ . Indeed, we can just write

$$
\left(\phi\right)_{A B C} = \left(\phi\right)_{A} \circ \left(\phi\right)_{B} \circ \left(\phi\right)_{C}
$$

without brackets on either side.

= Lecture 23 =

**Algebra of matrices**

**(a)** The set $M_{n} \left( 𝔽 \right)$ of $n \times n$ matrices with coefficients in $𝔽$ forms a ring under matrix addition and multiplication. The first case $M_{1} \left( 𝔽 \right) = 𝔽$ , but, if $n > 1$ , then $M_{n} \left( 𝔽 \right)$ is not commutative and does not satisfy the cancellation law, e.g.

$$
\begin{align}
\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}
\end{align}
$$

but

$$
\begin{align}
\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}
\end{align}
$$

i.e. the ’zero’ matrix $0 \in M_{2} \left( 𝔽 \right)$

**(b)** There is a multiplicative identity $I \in M_{n} \left( 𝔽 \right)$ given by

$$
\begin{align}
I = \left( \left(\delta\right)_{i j} \right) , \text{ where } \left(\delta\right)_{i j} = \left\{\begin{matrix} 1 & i=j\text{ (on diagonal)} \\ 0 & i\neq j\text{ (off diagonal)} \end{matrix}\right
\end{align}
$$

e.g.

$$
\begin{align}
I_{3} = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}
\end{align}
$$

Check: $I A = A = A I$ , for all $A \in M_{n} \left( 𝔽 \right)$ . Further, $\left(\phi\right)_{I} = \left(Id\right)_{\left(𝔽\right)^{n}}$ , as maps $\left(𝔽\right)^{n} \rightarrow \left(𝔽\right)^{n}$ , i.e. $I v = v$ .

**(c)** Say $A \in M_{n} \left( 𝔽 \right)$ is **invertible** iff there is $A^{- 1} \in M_{n} \left( 𝔽 \right)$ s.t.

$$
A^{- 1} A = I = A A^{- 1}
$$

Note that $A^{- 1}$ is unique. If $A$ is invertible, then $\left(\phi\right)_{A} : \left(𝔽\right)^{n} \rightarrow \left(𝔽\right)^{n}$ is a bijection with two-sided inverse $\left(\phi\right)_{A^{- 1}} : \left(𝔽\right)^{n} \rightarrow \left(𝔽\right)^{n}$ .

In fact (see Algebra 1B) for any $n \times m$ matrix $A$ , the map $\left(\phi\right)_{A} : \left(𝔽\right)^{m} \rightarrow \left(𝔽\right)^{n}$ is a bijection if and only if $m = n$ and $A$ is an invertible matrix.

Example 3.19 (Inverting a $2 \times 2$ matrix).  
  
For

$$
\begin{align}
A = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \in M_{2} \left( 𝔽 \right) ,
\end{align}
$$

consider

$$
\begin{align}
A^{′} = \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
\end{align}
$$

Then

$$
\begin{align}
A A^{′} = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix} = \begin{pmatrix} ad-bc & 0 \\ 0 & ad-bc \end{pmatrix} = \left( \det A \right) I
\end{align}
$$

where we define $\det A = a d - b c \in 𝔽$ .

Can check that also $A^{′} A = \left( \det A \right) I$ . Thus

$$
A^{- 1} = \left(\frac{1}{\det A}\right) A^{′}
$$

provided $\det A \neq 0$ , so that $\left(\left( \det A \right)\right)^{- 1}$ exists in $𝔽$ (here we need $𝔽$ to be a field).

In fact, $A$ invertible $\Leftrightarrow$ $\det A \neq 0$ and $\left( \Rightarrow \right)$ follows from two key identities

$$
\det I = 1
$$

and

$$
\det \left( A B \right) = \left( \det A \right) \left( \det B \right) ,
$$

which can be easily checked in the $2 \times 2$ case.

Example 3.20 ( $2 \times 2$ determinants).  
  
Suppose $𝔽 = ℝ$ . For $v , w \in ℝ^{2}$ , the area of the parallelogram spanning by $v , w$ is

$$
\begin{align}
V \left( v , w \right) = \left|v_{1} w_{2} - v_{2} w_{1}\right| = \left|\det B\right| , \text{for} B = \begin{pmatrix} v_{1} & w_{1} \\ v_{2} & w_{2} \end{pmatrix} .
\end{align}
$$

Hence, if $A \in M_{2} \left( ℝ \right)$ , then

$$
\begin{align}
V\left( A v , A w \right) & = & \left|\det \left( A B \right)\right| & \text{} \\ & = & \left|\det A\right|\cdot\left|\det B\right| & \text{} \\ & = & \left|\det A\right|V\left( v , w \right). & \text{}
\end{align}
$$

Thus $\left|\det A\right|$ is the area scaling factor for parallelograms (in fact, for area in general). Note that parallelogram $v , w$ came from the unit square of area $1$ by applying $B$ , so its area should indeed be $\left|\det B\right|$ .

= Lecture 24 =

**Geometry of matrices**

Definition 3.21.  
  
For any $n \times m$ matrix $A = \left( a_{i j} \right)$ , the **transpose** $A^{T}$ is the $m \times n$ matrix $b_{k l}$ where $b_{k l} = a_{l k}$ , e.g.

$$
\begin{align}
\left(\begin{pmatrix} a & b \\ c & d \end{pmatrix}\right)^{T} = \begin{pmatrix} a & c \\ b & d \end{pmatrix}
\end{align}
$$

Can check

$$
\left(\left( A^{T} \right)\right)^{T} = A , \left(\left( A + B \right)\right)^{T} = A^{T} + B^{T} , \left(\left( A C \right)\right)^{T} = C^{T} A^{T}
$$

for suitably sized $B , C$ .

If $v , w \in ℝ^{2}$ are identified with column vectors, i.e. $2 \times 1$ matrices, then

$$
v^{T} w = v_{1} w_{1} + v_{2} w_{2} = v \cdot w \text{(scalar product).}
$$

Thus

$$
\left(\left|v\right|\right)^{2} = v^{T} v = v_{1}^{2} + v_{2}^{2}
$$

where $\left|v\right|$ is Euclidean length and $\left|v - w\right|$ is Euclidean distance from $v$ to $w$ in $ℝ^{2}$ .

The same holds in $ℝ^{3}$ (where $\left(\left|v\right|\right)^{2} = v_{1}^{2} + v_{2}^{2} + v_{3}^{2}$ ) and can be extended to $ℝ^{n}$ (as definition). Also the angle $\theta$ between $v , w$ is defined by $v \cdot w = \left|v\right| \left|w\right| cos \theta$ .

Definition 3.22.  
  
A matrix $A \in M_{n} \left( ℝ \right)$ is **orthogonal** if $A^{T} A = I$ .

Note: in Algebra 1B, you will see this means that $A$ is invertible and $A^{- 1} = A^{T}$ , but this needs proof, i.e. that $A A^{T} = I$ .

Lemma 3.23.  
  
If $A \in M_{n} \left( ℝ \right)$ is orthogonal, then $\left(\phi\right)_{A} : ℝ^{n} \rightarrow ℝ^{n}$ preserves lengths and angles, that is, $\left|A v\right| = \left|v\right|$ and more generally, $\left( A v \right) \cdot \left( A w \right) = v \cdot w$ .

Proof. Write $v \cdot w$ as $v^{T} w$ and so

$$
\begin{align}
\left( A v \right)\cdot\left( A w \right) & = & \left(\left( A v \right)\right)^{T}Aw & \text{} \\ & = & \left( v^{T} A^{T} \right)Aw & \text{} \\ & = & v^{T}\left( A^{T} A \right)w & \text{} \\ & = & v^{T}Iw & \text{} \\ & = & v^{T}w & \text{} \\ & = & v\cdot w. & \text{}
\end{align}
$$

In particular,

$$
\left(\left|A v\right|\right)^{2} = \left( A v \right) \cdot \left( A v \right) = v \cdot v = \left(\left|v\right|\right)^{2} .
$$

□

Example 3.24.  
  
Which $A \in M_{2} \left( ℝ \right)$ are orthogonal? If

$$
\begin{align}
A = \begin{pmatrix} v_{1} & w_{1} \\ v_{2} & w_{2} \end{pmatrix}
\end{align}
$$

then

$$
\begin{align}
A^{T} A = \begin{pmatrix} v_{1} & v_{2} \\ w_{1} & w_{2} \end{pmatrix} \begin{pmatrix} v_{1} & w_{1} \\ v_{2} & w_{2} \end{pmatrix} = \begin{pmatrix} \left(\left|v\right|\right)^{2} & v\cdot w \\ w\cdot v & \left|w\right| \end{pmatrix}
\end{align}
$$

so

$$
A^{T} A = I \Leftrightarrow \left|v\right| = \left|w\right| = 1 \text{ and } v \cdot w = 0
$$

i.e. $v$ and $w$ are perpendicular unit vectors. Hence we must have $v = \left( cos \theta , sin \theta \right)$ for some angle $\theta$ and then $w$ is obtained by rotation through $\pm \frac{\pi}{2}$ , i.e. replacing $\theta$ by $\theta \pm \frac{\pi}{2}$ to get

$$
w = \left( - sin \theta , cos \theta \right) \text{or} w^{′} = \left( sin \theta , - cos \theta \right) = - w
$$

![No alt text was set. Please request alt text from the person who provided you with this resource.](./figures/pdf/MA10209-web-figure12.svg)

In other words,

$$
\begin{align}
A = \begin{pmatrix} cos\theta & -sin\theta \\ sin\theta & cos\theta \end{pmatrix} ,
\end{align}
$$

which is rotation through $\theta$ (and note that $\det A = \left(cos\right)^{2} \theta + \left(sin\right)^{2} \theta = 1$ ) or

$$
\begin{align}
A = \begin{pmatrix} cos\theta & sin\theta \\ sin\theta & -cos\theta \end{pmatrix} ,
\end{align}
$$

which is reflection in line of angle $\theta / 2$ with horizontal axis (and note that $\det A = - 1$ ). We can identify these transformations by applying $A$ to the unit square.

**$3\times 3$ determinants**

The volume of the parallelepiped spanned by $u , v , w \in ℝ^{3}$ is given by the ’scalar triple product’

$$
V \left( u , v , w \right) = \left|u \cdot v \times w\right| ,
$$

where

$$
u \cdot v \times w = u_{1} \left( v_{2} w_{3} - v_{3} w_{2} \right) + u_{2} \left( v_{3} w_{1} - v_{1} w_{3} \right) + u_{3} \left( v_{1} w_{2} - v_{2} w_{1} \right) .
$$

We could expect that this is the formula for $\det B$ , where

$$
\begin{align}
B = \begin{pmatrix} u_{1} & v_{1} & w_{1} \\ u_{2} & v_{2} & w_{2} \\ u_{3} & v_{3} & w_{3} \end{pmatrix} \in M_{3} \left( 𝔽 \right)
\end{align}
$$

and, in fact, this is correct. The first property is that

$$
\begin{align}
\det \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} = 1
\end{align}
$$

which is the volume of the unit cube.

It further follows (see Algebra 1B) that

*   $\det \left( A B \right) = \det A \det B$ , so as in the $2 \times 2$ case, $\left|\det A\right|$ is the volume scaling factor for the transformation
    
    $$
    \left(\phi\right)_{A} : ℝ^{3} \rightarrow ℝ^{3} : v \rightarrow tail A v .
    $$
    
*   $A \in M_{3} \left( 𝔽 \right)$ is invertible $\Leftrightarrow$ $\det A \neq 0$ (so that $\frac{1}{\det A}$ exists in $𝔽$ ). Then
    
    $$
    A^{- 1} = \frac{1}{\det A} adj A
    $$
    
    where the ’adjugate’ $adj A$ is given by an explicit formula using $2 \times 2$ determinants.

= Lecture 25 =