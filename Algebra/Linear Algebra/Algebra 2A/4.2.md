#### 4.2 The spectral theorem

##### 4.2.1 Eigenvalues and eigenvectors

Recall from Chapter 5 of Algebra 1B:

###### Definitions

. Let $V$ be a vector space over $\F$ and $\phi \in L(V)$.

An _eigenvalue_ of $\phi$ is a scalar $\lambda \in \F$ such that there is a _non-zero_ $v\in V$ with

$\seteqnumber{0}{4.}{5}$

$$ \phi (v)=\lambda v. $$

Such a vector $v$ is called an _eigenvector of $\phi$ with eigenvalue $\lambda$_.

The _$\lambda$-eigenspace $E_{\phi }(\lambda )$ of $\phi$\_ is given by

$\seteqnumber{0}{4.}{5}$

$$ E\_{\phi }(\lambda ):=\ker (\phi -\lambda \id \_V)\leq V. $$

- Remark. Thus $E_{\phi }(\lambda )$ consists of all eigenvectors of $\phi$ with eigenvalue $\lambda$ along with $0$.

###### Definition

. Let $V$ be a finite-dimensional vector space over $\F$ and $\phi \in L(V)$.

The _characteristic polynomial $\Delta _{\phi }$ of $\phi$\_ is given by

$\seteqnumber{0}{4.}{5}$

$$ \Delta \_{\phi }(\lambda ):=\det (\phi -\lambda \id \_V)=\det (A-\lambda \I ), $$

where $A$ is the matrix of $\phi$ with respect to some (any!) basis of $V$.

We know that $\Delta _{\phi }(\lambda )$ is a polynomial in $\lambda$ with coefficients in $\F$ and $\deg \Delta _{\phi }=\dim V$. It is important to us because:

- [[Lemma 4.7]]. A scalar $\lambda \in \F$ is an eigenvalue of $\phi$ if and only if $\Delta _{\phi }(\lambda )=0$, that is, $\lambda$ is a root of $\Delta _{\phi }$.

When $\F =\C$, the Fundamental Theorem of Algebra ensures that the characteristic polynomial has at least one root so so we conclude:

- [[Corollary 4.8]]. Let $\phi$ be a linear operator on a finite-dimensional _complex_ vector space $V$. Then $\phi$ has an eigenvalue.

- Remark. When $\F =\R$ things are more problematic: consider $\phi :\R ^2\to \R ^2$ given by $\phi (x_1,x_2)=(-x_2,x_1)$. Thus $\phi =\phi _A$ for

  $\seteqnumber{0}{4.}{5}$

  $$ A= \begin{pmatrix} 0&-1\\1&0 \end {pmatrix}. $$

  Then $\Delta _{\phi }(\lambda )=\lambda ^2+1$ which has no real roots at all!

##### 4.2.2 Invariant subspaces and adjoints

###### Definition

. Let $V$ be a vector space and $\phi \in L(V)$.

A subspace $U\leq V$ is _$\phi$-invariant_ if $\phi (U)\leq U$, that is, $\phi (u)\in U$, for all $u\in U$.

Note that, in this case, $\phi$ restricts to give a linear operator on $U$: $\phi _{|U}\in L(U)$.

###### Example

. Any eigenspace of $\phi$ is $\phi$-invariant: if $v\in E_{\phi }(\lambda )$, then $\phi (v)=\lambda v\in E_{\phi }(\lambda )$, since $E_{\phi }(\lambda )$ is a subspace and so closed under scalar multiplication.

Here are two ways to find invariant subspaces.

- [[Lemma 4.9]]. Let $\phi ,\psi \in L(V)$ and suppose that

  - • $\psi \circ \phi =\phi \circ \psi$ (say that $\phi$ and $\psi$ _commute_).
  - • $U=E_{\phi }(\lambda )$ is an eigenspace of $\phi$.

  Then $U$ is $\psi$-invariant.

- Proof. Let $u\in U$ so that $\phi (u)=\lambda u$. Take $\psi$ of both sides to get $\psi (\phi (u))=\lambda \psi (u)$. But $\psi (\phi (u))=\phi (\psi (u))$ so this reads:

  $\seteqnumber{0}{4.}{5}$

  $$ \phi (\psi (u))=\lambda \psi (u). $$

  Thus $\psi (u)\in E_{\phi }(\lambda )$ as required. □

- [[Lemma 4.10]]. Let $V$ be a finite-dimensional6 inner product space and $\phi \in L(V)$.

  Let $U\leq V$ be a $\phi$-invariant subspace. Then $U^{\perp }$ is $\phi ^{*}$-invariant.

- Proof. Let $v\in U^{\perp }$ so that $\ip {u,v}=0$, for all $u\in U$. Then

  $\seteqnumber{0}{4.}{5}$

  $$ \ip {u,\phi ^{\*}(v)}=\ip {\phi (u),v}=0, $$

  for all $u\in U$, since $\phi (u)\in U$ also. Thus $\phi ^{*}(v)\in U^{\perp }$ as required. □

These two constructions come together for an interesting class of operators:

###### Definition

. Let $V$ be a finite-dimensional inner product space. A linear operator $\phi \in L(V)$ is _normal_ if it commutes with its adjoint: $\phi ^{*}\circ \phi =\phi \circ \phi ^{*}$.

###### Examples

.

We now have:

- [[Proposition 4.11]]. Let $V$ be a finite-dimensional inner product space and $\phi \in L(V)$.

  Suppose that:

  - • $\phi$ is normal;
  - • $U\leq V$ is an eigenspace of $\phi$.

  Then $U^{\perp }$ is $\phi$-invariant.

- Proof. Since $\phi$ is normal, $\phi$ and $\phi ^{*}$ commute whence $U$ is $\phi ^{*}$-invariant by [[Lemma 4.9]]. But now [[Lemma 4.10]] tells us that $U^{\perp }$ is $(\phi ^{*})^{*}=\phi$-invariant. □

6 We only need this hypothesis to ensure that $\phi ^{*}$ exists.

##### 4.2.3 The spectral theorem for normal operators

Recall from Algebra 1B (Definition 1 of §5.2):

###### Definition

. Let $V$ be a finite-dimensional vector space. A linear operator $\phi \in L(V)$ is _diagonalisable_ if $V$ has a basis of eigenvectors of $\phi$.

This means that we have a basis $\lst {v}1n$ of $V$ and scalars $\lst \lambda 1n\in \F$ with

$\seteqnumber{0}{4.}{5}$

$$ \phi (v_i)=\lambda \_iv_i, $$

for $\bw 1in$. Equivalently, the matrix $A$ of $\phi$ with respect to $\lst {v}1n$ is _diagonal_

$\seteqnumber{0}{4.}{5}$

$$ \label {eq:19} A= \begin{pmatrix} \lambda \_1&&\\&\ddots &\\&&\lambda \_n \end {pmatrix}, $$

with the eigenvalues as diagonal entries.

- Exercise.7 Let $\phi \in L(V)$ be diagonalisable and $\lst {v}1n$ be a basis of eigenvectors with eigenvalues $\lst \lambda 1n$. If $\lambda$ is an eigenvalue of $\phi$ then

  $\seteqnumber{0}{4.}{6}$

  $$ E\_{\phi }(\lambda )=\Span {v_i\st \lambda \_i=\lambda }. $$

  Thus $\lambda$ appears $\dim E_{\phi }(\lambda )$ times in the list $\lst {\lambda }1n$.

###### Definition

. Let $V$ be a finite-dimensional inner product space. A linear operator $\phi \in L(V)$ is _orthogonally diagonalisable_ if $V$ has an _orthonormal_ basis of eigenvectors.

The Big Question we are going to answer is:

> When is a linear operator orthogonally diagonalisable?

We begin by observing that there is a necessary condition:

- [[Proposition 4.12]]. Let $V$ be a finite-dimensional inner product space over $\F$ and $\phi \in L(V)$ an orthogonally diagonalisable linear operator. Then:

  - (1) If $\F =\C$, $\phi$ is normal.
  - (2) If $\F =\R$, $\phi$ is self-adjoint.

- Proof. Let $\lst {u}1n$ is an orthonormal basis of eigenvectors of $\phi$. The matrix $A$ of $\phi$ with respect to $\lst {u}1n$ has the form (4.6) so that, by [[Proposition 4.3]](2), $\phi ^{*}$ has matrix

  $\seteqnumber{0}{4.}{6}$

  $$ A^{\dagger }= \begin{pmatrix} \overline {\lambda \_1}&&\\&\ddots &\\&&\overline {\lambda \_n} \end {pmatrix} $$

  which is also diagonal. In particular, if $F=\R$ so that each $\lambda _i=\overline {\lambda _i}$, $A=A^T$ whence $\phi =\phi ^{*}$. Otherwise said, $\phi$ is self-adjoint.

  On the other hand, when $\F =\C$, we have $A^{\dagger }A=AA^{\dagger }$ (diagonal matrices commute) so that $\phi ^{*}\circ \phi =\phi \circ \phi ^{*}$, that is, $\phi$ is normal. □

In both cases, the converse is also true. We do the complex case first.

- [[Theorem 4.13]] (Spectral theorem for normal operators). Let $V$ be a finite-dimensional _complex_ inner product space and $\phi \in L(V)$ a linear operator. Then $\phi$ is orthogonally diagonalisable if and only if $\phi$ is normal.

- Proof. The forward implication is [[Proposition 4.12]].

  We prove the reverse implication by induction on $\dim V$. Thus our inductive hypothesis at $n$ is that the theorem holds when $\dim V\leq n$.

  First suppose that $\dim V=1$ and let $u\in V$ have unit length so that $u$ is an orthonormal basis all on its own. Then any $\phi \in L(V)$ has $\phi (u)\in V$ so that $\phi (u)=\lambda u$, for some $\lambda \in \F$. Thus $u$ is an eigenvector of $\phi$ and the inductive hypothesis holds at $n=1$.

  Now assume that the hypothesis holds at $n-1$ and suppose that $\dim V=n$ and $\phi$ is normal. We make three observations:

  - (1) Since $\F =\C$, $\phi$ has an eigenvalue $\lambda$ by [[Corollary 4.8]]. We let $U\leq V$ be the $\lambda$-eigenspace.
  - (2) Let $\lst {u}1k$ be an orthonormal basis of $U$, which exists by [[Corollary 3.8]]. Of course, each $u_i$ is an eigenvector of $\phi$ with eigenvalue $\lambda$.
  - (3) Since $\phi$ is normal, [[Proposition 4.11]] tells us that $U^{\perp }$ is $\phi$-invariant. Moreover, $\dim U^{\perp }=\dim V-\dim U<n$ so we can try to apply the inductive hypothesis to $\phi _{|U^{\perp }}\in L(U^{\perp })$. For this, we need $\phi _{|U^{\perp }}$ to be normal. However, $U^{\perp }$ is $\phi ^{*}$-invariant by [[Lemma 4.10]] so that $\phi ^{*}_{|U^{\perp }}\in L(U^{\perp })$. Moreover, for $w_1,w_2\in U^{\perp }$,

    $\seteqnumber{0}{4.}{6}$

    $$ \ip {\phi ^{_}\_{|U^{\perp }}(w_2),w_1}=\ip {\phi ^{_}(w*2),w_1} =\ip {w_2,\phi (w_1)}=\ip {w_2,\phi *{|U^{\perp }}(w\_{1})} $$

    so that $(\phi _{|U^{\perp }})^{*}=\phi ^{*}_{|U^{\perp }}$. Since $\phi$ and $\phi ^{*}$ commute, so do their restrictions to $U^{\perp }$ and we conclude that $\phi _{|U^{\perp }}$ is indeed normal. We can therefore apply the inductive hypothesis to get an orthonormal basis $\lst {u}{k+1}n$ of $U^{\perp }$ consisting of eigenvectors of $\phi _{|U^{\perp }}$:

    $\seteqnumber{0}{4.}{6}$

    $$ \phi \_{|U^{\perp }}(u_j)=\lambda \_ju_j, $$

    or, equivalently,

    $\seteqnumber{0}{4.}{6}$

    $$ \phi (u_j)=\lambda \_ju_j, $$

    for $\bw {k+1}jn$.

  We conclude that $\lst {u}1n$ is an orthonormal basis of $V$ with each $u_i$ an eigenvector of $\phi$. Thus the inductive hypothesis is true at $n$ and so always. □

- Remark. The only place in the argument where we used $\F =\C$ was at step (1) to see that $\phi$ has an eigenvalue at all. In the next section, we will show that real self-adjoint operators also have eigenvalues and so prove the spectral theorem in that case also.

7 Question 2 on sheet 7.

##### 4.2.4 The spectral theorem for real self-adjoint operators

To prove the spectral theorem in the real case, we need the following lemma which is interesting in its own right.

- [[Lemma 4.14]]. Let $V$ be an inner product space8 and $\phi \in L(V)$ be self-adjoint.

  - (1) Any eigenvalue of $\phi$ is _real_.
  - (2) If $v,w\in V$ are eigenvectors of $\phi$ with eigenvalues $\lambda \neq \mu$ then $v\perp w$.

- Proof. Let $v,w\in V$ be eigenvectors of $\phi$ with eigenvalues $\lambda ,\mu$. Since $\phi$ is self-adjoint, we have

  $\seteqnumber{0}{4.}{6}$

  $$ \label {eq:20} \ip {\phi (v),w}=\ip {v,\phi (w)}. $$

  However,

  $\seteqnumber{0}{4.}{7}$

  $$ \begin{align*} \ip {\phi (v),w}&=\ip {\lambda v,w}=\bar {\lambda }\ip {v,w}\\ \ip {v,\phi (w)}&=\ip {v,\mu w}=\mu \ip {v,w} \end{align*} $$ so that (4.7) reads

  $\seteqnumber{0}{4.}{7}$

  $$ (\bar {\lambda }-\mu )\ip {v,w}=0. $$

  Now, for (1), take $v=w$ so that $\lambda =\mu$ to get $(\bar {\lambda }-\lambda )\norm {v}^2=0$ so that $\lambda =\bar {\lambda }$: thus $\lambda \in \R$.

  Now, if $\lambda \neq \mu$, we get $(\lambda -\mu )\ip {v,w}=0$ giving $\ip {v,w}=0$, settling (2). □

We apply this to find the missing ingredient for the real spectral theorem.

- [[Proposition 4.15]]. A self-adjoint operator $\phi$ on a real, finite-dimensional inner product space $V$ has an eigenvalue.

- Proof. Choose an orthonormal basis $\lst {u}1n$ of $V$ and let $A$ be the matrix of $\phi$ with respect to $\lst {u}1n$. Then, by [[Proposition 4.3]](2), $A=A^T$

  Now view $A$ as a _complex_ matrix (with real entries) and let $\phi _A^{\C }:\C ^n\to \C ^n$ be the complex linear map given by matrix multiplication with $A$. Observe:

  - 1. $A=A^T=A^{\dagger }$ so that $\phi ^{\C }_A$ is self-adjoint for the dot inner product on $\C ^n$.
  - 2. Thus $\phi ^{\C }_A$ has an eigenvalue by [[Proposition 4.8]] and, further, that eigenvalue $\lambda$ is real by [[Lemma 4.14]].
  - 3. Otherwise said, $\lambda$ is a root of the characteristic polynomial $\Delta _{\phi _A^{\C }}$ of $\phi _A^{\C }$. But

    $\seteqnumber{0}{4.}{7}$

    $$ \Delta _{\phi \_A^{\C }}(\lambda )=\det (A-\lambda \I )=\Delta _{\phi }(\lambda ) $$

    so that $\lambda$ is an eigenvalue of $\phi$.

  □

We now have:

- [[Theorem 4.16]] (Spectral theorem for real self-adjoint operators). Let $V$ be a real, finite-dimensional inner product space and $\phi \in L(V)$ a linear operator. Then $\phi$ is orthogonally diagonalisable if and only if $\phi$ is self-adjoint.

- Proof. The proof is exactly the same as that of the complex spectral [[Theorem 4.13]] except that we use [[Proposition 4.15]] as a replacement for [[Corollary 4.8]] in step 1. □

- Remark. A serious application of these ideas is to Quantum Mechanics. Here a physical system is modelled by an inner product space $V$ (usually infinite-dimensional) and the _observables_ (things we can measure like position, momentum or spin) by self-adjoint operators. This is a probabilistic theory where all we can hope to find out is the _expected value_ of an observable when the system is in a state $v\in V$. However, there are _pure states_ where we can measure $\phi$ with certainty: these are the eigenvectors of $\phi$ and the corresponding eigenvalue is what we measure in that state.

  Let $\lst {u}1n$ be an orthonormal basis of eigenvectors of $\phi$ with eigenvalues $\lst \lambda 1n$. Then the probability that we will get a measurement of $\lambda _i$ when in state $v$ is

  $\seteqnumber{0}{4.}{7}$

  $$ \frac {\abs {\ip {u_i,v}}^{2}}{\norm {v}^{2}}. $$

  Note that this lies between $0$ and $1$ by Cauchy–Schwarz (3.2) and the different probabilities sum to $1$ by the Bessel equality ([[Proposition 3.6]](2)).

  It follows (exercise!) that the expected value of a measurement of $\phi$ when in state $v$

  $\seteqnumber{0}{4.}{7}$

  $$ \frac {\ip {\phi (v),v}}{\norm {v}^2}. $$

  We need the theory of _Hilbert spaces_ (see MA40256) to make sense of all this in the infinite-dimensional case.

8 We do _not_ demand that $V$ be finite-dimensional.

##### 4.2.5 The spectral theorem for symmetric and Hermitian matrices

The spectral theorem can be reformulated as a result about symmetric and Hermitian matrices:

- [[Theorem 4.17]] (Spectral theorem for symmetric/hermitian matrices).
- - (1) Let $A\in M_{n\times n}(\R )$ be symmetric. Then there is an orthogonal matrix $P\in \rO (n)$ such that $P^{-1}AP$ is diagonal.
  - (2) Let $A\in M_{n\times n}(\C )$ be Hermitian. Then there is an unitary matrix $P\in \rU (n)$ such that $P^{-1}AP$ is diagonal.

- Proof. In either case, $\phi _A:\F ^n\to \F ^n$ is self-adjoint with respect to the dot inner product and so, by the relevant spectral theorem, orthogonally diagonalisable. So let $\cB =\lst {u}1n$ be an orthonormal basis of eigenvectors of $\phi _A$ with eigenvalues $\lst \lambda 1n$.

  Contemplate $\phi _{\cB }:\F ^n\to \F ^n$ for which $\phi _{\cB }(e_i)=u_i$. Then $\phi _{\cB }=\phi _P$ where $P\in M_{n\times n}(\F )$ is the matrix whose columns are $\lst \bu 1n$.

  Observe:

  - • The columns of $P$ are orthonormal so that $P$ is orthogonal/unitary.
  - • $\phi _{P^{-1}AP}=\phi _P^{-1}\circ \phi _A\circ \phi _P$ has eigenvectors $\lst {e}1n$ with eigenvalues $\lst \lambda 1n$:

    $\seteqnumber{0}{4.}{7}$

    $$ \phi \_P^{-1}\circ \phi \_A\circ \phi \_P(e_i)=\phi \_P^{-1}(\phi \_A(u_i))=\lambda \_i\phi \_P^{-1}(u_i) =\lambda \_ie_i. $$

  Otherwise said, $P^{-1}AP$ is the diagonal matrix

  $\seteqnumber{0}{4.}{7}$

  $$ \begin{pmatrix} \lambda \_1&&\\&\ddots &\\&&\lambda \_n \end {pmatrix}. $$

  □

###### Example

. Let $A$ be the Hermitian matrix

$\seteqnumber{0}{4.}{7}$

$$ A= \begin{pmatrix} 1&i\\-i&1 \end {pmatrix}. $$

**Problem**: Find a unitary matrix $P$ such that $P^{-1}AP$ is diagonal.

**Solution**: $P$ will have for columns an orthonormal basis of eigenvectors of $A$. Then $P^{-1}AP$ will be diagonal with the corresponding eigenvalues as entries.

First we find the eigenvalues:

$\seteqnumber{0}{4.}{7}$

$$ \det (A-\lambda \I )= \begin{vmatrix} 1-\lambda &i\\-i&1-\lambda \end {vmatrix} =\lambda ^2-2\lambda =\lambda (\lambda -2) $$

so that the eigenvalues are $0$ and $2$.

Corresponding eigenvectors are guaranteed to be orthogonal by [[Lemma 4.14]] so all we have to do is find unit length eigenvectors to get an orthonormal basis.

For eigenvalue $0$, we must solve $A\bx =0$ which we readily do to get

$\seteqnumber{0}{4.}{7}$

$$ \bx =\mu \begin{pmatrix} 1\\i \end {pmatrix} $$

and taking $\mu =1/\sqrt {2}$ we get a unit eigenvector

$\seteqnumber{0}{4.}{7}$

$$ \bu \_1=\frac 1{\sqrt {2}} \begin{pmatrix} 1\\i \end {pmatrix} $$

For the eigenvalue $2$, we can proceed in two ways: either solve $A\by =2\by$ or simply seek $\by$ such that $\by \perp \bx$. Either way, we get

$\seteqnumber{0}{4.}{7}$

$$ \bu \_{2}=\frac 1{\sqrt {2}} \begin{pmatrix} i\\1 \end {pmatrix} $$

so that

$\seteqnumber{0}{4.}{7}$

$$ P=\frac 1{\sqrt {2}} \begin{pmatrix} 1&i\\i&1 \end {pmatrix} $$

and

$\seteqnumber{0}{4.}{7}$

$$ P^{-1}AP= \begin{pmatrix} 0&0\\0&2 \end {pmatrix}. $$

We got all this by following the proof of [[Theorem 4.17]] but we can check our answer directly. First we check that $P$ really is unitary:

$\seteqnumber{0}{4.}{7}$

$$ P^{\dagger }P=\frac 12 \begin{pmatrix} 1&-i\\-i&1 \end {pmatrix} \begin{pmatrix} 1&i\\i&1 \end {pmatrix}=\frac 12 \begin{pmatrix} 2&0\\0&2 \end {pmatrix}=\I . $$

Second we check that $P^{-1}AP$ really is the diagonal matrix we say it is:

$\seteqnumber{0}{4.}{7}$

$$ \begin{align*} P^{-1}AP=P^{\dagger }AP &=\half \begin{pmatrix} 1&-i\\-i&1 \end {pmatrix} \begin{pmatrix} 1&i\\-i&1 \end {pmatrix} \begin{pmatrix} 1&i\\i&1 \end {pmatrix}\\ &=\half \begin{pmatrix} 0&0\\-2i&2 \end {pmatrix} \begin{pmatrix} 1&i\\i&1 \end {pmatrix}= \begin{pmatrix} 0&0\\0&2 \end {pmatrix}. \end{align*} $$

##### 4.2.6 Singular value decomposition

Here is an application of the spectral theorem to general linear operators.

Let $V$ be a finite-dimensional inner product space and $\phi \in L(V)$ a linear operator.

Let us first consider the situation where $\phi$ is self-adjoint. By the spectral theorem, $V$ has an orthonormal basis $\lst {u}1n$ of eigenvectors with _real_ eigenvalues $\lst {\lambda }1n$ where each distinct eigenvalue $\lambda$ appears $\dim E_{\phi }(\lambda )$ times. For $v\in V$, [[Lemma 3.3]] says

$\seteqnumber{0}{4.}{7}$

$$ v=\sum \_{i=1}^n\ip {u_i,v}u_i $$

and taking $\phi$ of this and using $\phi (u_i)=\lambda _iu_i$ yields:

$\seteqnumber{0}{4.}{7}$

$$ \label {eq:23} \phi (v)=\sum \_{i=1}^n\lambda \_i\ip {u_i,v}u_i. $$

But what can be said if $\phi$ is not self-adjoint or even diagonalisable? One answer is to consider $\phi ^{*}\circ \phi$ instead! Recall from §4.1.2 that $\phi ^{*}\circ \phi$ is always self-adjoint. Moreover

- [[Lemma 4.18]]. Let $V$ be a finite-dimensional inner product space and $\phi \in L(V)$. Then:

  - (1) All eigenvalues of $\phi ^{*}\circ \phi$ are non-negative.
  - (2) $\ker (\phi ^{*}\circ \phi )=\ker \phi$.

- Proof. If $\phi ^{*}(\phi (u))=\lambda u$ with $u\neq 0$ (thus $u$ is an eigenvector of $\phi ^{*}\circ \phi$ with eigenvalue $\lambda$), then

  $\seteqnumber{0}{4.}{8}$

  $$ \lambda \ip {u,u}=\ip {u,\lambda u}=\ip {u,\phi ^{\*}(\phi (u))} =\ip {\phi (u),\phi (u)} $$

  so that

  $\seteqnumber{0}{4.}{8}$

  $$ \lambda =\frac {\norm {\phi (u)}^2}{\norm {u}^2}\geq 0. $$

  This settles (1). For (2), this formula says that $\lambda =0$ if and only if $\phi (u)=0$ so that the $0$-eigenspace of $\phi ^{*}\circ \phi$ (that is, $\ker \phi ^{*}\circ \phi$) coincides with $\ker \phi$. □

###### Definition

. Let $V$ be a finite-dimensional inner product space and $\phi \in L(V)$. The _singular values of $\phi$_ are $\lst \sigma 1n$ where $\sigma _i=\sqrt {\mu _i}\geq 0$ and $\lst \mu 1n$ are the eigenvalues of $\phi ^{*}\circ \phi$ listed with multiplicity (thus each distinct $\mu$ appears $\dim E_{\phi ^{*}\circ \phi }(\mu )$ times).

- Exercise.9 If $\phi$ is self-adjoint with eigenvalues $\lambda _i$ then the singular values of $\phi$ are the $\abs {\lambda _i}$.

We use these singular values to get a nice analogue of (4.8) for general $\phi$ at the cost of employing _two_ orthonormal bases of $V$.

- [[Theorem 4.19]] (Singular value decomposition). Let $V$ be a finite-dimensional inner product space and $\phi \in L(V)$ a linear operator with singular values $\lst \sigma 1n$.

  Then there are orthonormal bases $\lst {u}1n$ and $\lst {w}1n$ of $V$ such that

  $\seteqnumber{0}{4.}{8}$

  $$ \label {eq:24} \phi (v)=\sum \_{i=1}^n\sigma \_i\ip {u_i,v}w_i, $$

  for all $v\in V$.

- Proof. Let $\lst {u}1n$ be an orthonormal basis of eigenvectors of $\phi ^{*}\circ \phi$ ordered so that $\sigma _1\geq \sigma _2\geq \cdots \geq \sigma _n$ and let $k$ be the largest index for which $\sigma _k\neq 0$. Thus, $\sigma _j=0$ for $k<j\leq n$ so that, by [[Lemma 4.18]](2), $\phi (u_j)=0$.

  For $\bw 1ik$, set $w_i:=\phi (u_i)/\sigma _i$. We show that $\lst {w}1k$ is orthonormal:

  $\seteqnumber{0}{4.}{9}$

  $$ \ip {w*i,w_j}=\frac 1{\sigma \_i\sigma \_j}\ip {\phi (u_i),\phi (u_j)}= \frac 1{\sigma \_i\sigma \_j}\ip {u_i,\phi ^{\*}\circ \phi (u_j)}= \frac {\sigma \_j^2}{\sigma \_i\sigma \_j}\delta *{ij}=\delta \_{ij}, $$

  for $\bw 1{i,j}k$. Now extend $\lst {w}1k$ to an orthonormal basis of $V$ by choosing an orthonormal basis $\lst {w}{k+1}n$ of $\Span {\lst {w}1k}^{\perp }$. Remark that for $\bw 1in$ we have

  $\seteqnumber{0}{4.}{9}$

  $$ \phi (u_i)=\sigma \_iw_i. $$

  Indeed, for $i\leq k$, this is how we defined $w_i$ while, for $i>k$, both $\sigma _i=0$ and $\phi (u_i)=0$.

  Thus, for $v\in V$,

  $\seteqnumber{0}{4.}{9}$

  $$ \phi (v)=\phi (\sum _{i=1}^{n}\ip {u_i,v}u_i) =\sum _{i=1}^{n}\ip {u*i,v}\phi (u_i)=\sum *{i=1}^n\sigma \_i\ip {u_i,v}w_i $$

  as required. □

- Remark. This result has real life applications to, among other things, image compression. The idea is that you can approximate a complicated matrix by the terms in the singular value decomposition with the largest singular values.

9 Question 2 on sheet 8.
