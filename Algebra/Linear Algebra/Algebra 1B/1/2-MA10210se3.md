[[next](MA10210se4.html)] [[prev](MA10210se2.html)] [[prev-tail](MA10210se2.html#tailMA10210se2.html)] [[tail](#tailMA10210se3.html)] [[up](MA10210ch1.html#MA10210se3.html)]

### 1.3 [Matrices and linear maps](MA10210.html#QQ2-6-7)

A field, generally denoted $ğ”½$ , is a commutative ring in which every non-zero element is invertible. For $ğ”½$ we have,

* * *

* * *

* * *

* * *

operation

axioms

* * *

* * *

* * *

$\left( ğ”½ , + \right)$  
additive group

$x+y$  
$-x$ (inverse of $x$)  
$0$ (additive Id)

$x+y=y+x$  
$x+\left( y + z \right)=\left( x + y \right)+z$  
$x+\left( - x \right)=0$  
$x+0=x$

* * *

* * *

* * *

$\left( ğ”½ \backslash \left\{ 0 \right\} , \cdot \right)$  
also an abelian group

$x\cdot y$ or $xy$  
$x^{- 1}$ (inverse of $x$)  
$1\neq 0$ (mult. Id)

$xy=yx$  
$x\left( y z \right)=\left( x y \right)z$  
$x\cdot 1=x$  
$x\cdot x^{- 1}=1$

* * *

* * *

* * *

$x\cdot$ is a homomorphism  
$\left( ğ”½ , + \right)\rightarrow\left( ğ”½ , + \right)$

$x\cdot\left( y + z \right)=x\cdot y+x\cdot z$

* * *

* * *

* * *

* * *

Example 1.  
Rationals $â„š$ , reals $â„$ , complex numbers $â„‚$ , finite fields $\left(ğ”½\right)_{p}=â„¤_{p}$ , integers $modp$ for $p$ prime, e.g. $\left(ğ”½\right)_{2}=\left\{ 0 , 1 \right\}$.

Integers $â„¤$ and $â„¤_{6}$ are not fields, as any nonzero integer different from $\pm1$ has no multiplicative inverse and in $â„¤_{6}$ , $2\cdot 3=0$ and so $2^{- 1},3^{- 1}$ does not exist.

To any field $ğ”½$ can associate

1.

the space of (coordinate) vectors $\left(ğ”½\right)^{n}=\underset{\text{ }n\text{ times}}{\underset{ï¸¸}{ğ”½ \times \dots  \times ğ”½}}$ .

This is an additive group with operations on components

$$
\begin{align}
\left( x_{1} , \dots  , x_{n} \right)+\left( y_{1} , \dots  , y_{n} \right) & = & \left( x_{1} + y_{1} , \dots  , x_{n} + y_{n} \right) & \text{} \\ -\left( x_{1} , \dots  , x_{n} \right) & = & \left( - x_{1} , \dots  , - x_{n} \right) & \text{} \\ 0 & = & \left( 0 , \dots  , 0 \right) & \text{}
\end{align}
$$

and the operation of scalar multiplication by $\lambda\inğ”½$

$$
\lambda \left( x_{1} , \dots  , x_{n} \right) = \left( \lambda x_{1} , \dots  , \lambda x_{n} \right)
$$

making $\left(ğ”½\right)^{n}$ the prototype vector space [treated axiomatically in ChapterÂ 2].

2.

the space $M_{m , n}\left( ğ”½ \right)$ of $m\times n$ matrices over $ğ”½$

$$
\begin{align}
= \begin{Bmatrix} A = \begin{pmatrix} a_{1 1} & \dots  & a_{1 n} \\ \vdots & & \vdots \\ a_{n 1} & \dots  & a_{m n} \end{pmatrix} : a_{i j} \in ğ”½ \end{Bmatrix} .
\end{align}
$$

Write $A=\left( a_{i j} \right)$ for short. Each $M_{m , n}\left( ğ”½ \right)$ is an additive group with scalar multiplication; all operations are defined component wise as in $\left(ğ”½\right)^{n}$ . That is, for $A=\left( a_{i j} \right)$ $B=\left( b_{i j} \right)$ and $\lambda\inğ”½$ ,

$$
A + B = \left( a_{i j} + b_{i j} \right) \text{ and } \lambda A = \left( \lambda a_{i j} \right) .
$$

#### [Matrix Multiplication](MA10210li1.html#QQ2-6-8)

We can multiply an $m\times n$ matrix $A=\left( a_{i j} \right)$ and an $n\times p$ matrix $B=\left( b_{i j} \right)$ to get an $m\times p$ matrix $AB$ defined by

$$
\left(\left( A B \right)\right)_{i k} = \sum_{j = 1}^{n} a_{i j} b_{j k}
$$

Lemma 2.  
Properties of matrix multiplication:

(i)

associativity: for $A\in M_{l , m}\left( ğ”½ \right),B\in M_{m , n}\left( ğ”½ \right)$ and $C\in M_{n , p}\left( ğ”½ \right)$ ,  

$$
\left( A B \right) C = A \left( B C \right) ,
$$

(ii)

â€˜bilinearityâ€™: for $A\in M_{l , m}\left( ğ”½ \right),B,C\in M_{m , n}\left( ğ”½ \right)$ , $D\in M_{n , p}\left( ğ”½ \right)$ and $\lambda,\mu\inğ”½$ , 
$$
\begin{align}
 & & A\left( \lambda B + \mu C \right)=\lambda AB+\mu AC & \text{} \\ & & \left( \lambda B + \mu C \right)D=\lambda BD+\mu CD. & \text{} \\ & & & \text{}
\end{align}
$$

Proof. Compare the $\left( i , j \right)$ -entries both on sides in each case.

(i)

$$
\begin{align}
\left(\left( \left( A B \right) C \right)\right)_{i j}= & \sum_{s = 1}^{n}\left(\left( A B \right)\right)_{i s}c_{s j} & & \text{} \\ = & \sum_{s = 1}^{n}\left( \sum_{t = 1}^{m} a_{i t} b_{t s} \right)c_{s j} & & \text{} \\ = & \sum_{s = 1}^{n}\sum_{t = 1}^{m}a_{i t}b_{t s}c_{s j} & & \text{} \\ = & \sum_{t = 1}^{m}a_{i t}\sum_{s = 1}^{n}b_{t s}c_{s j} & & \text{} \\ = & \left(\left( A \left( B C \right) \right)\right)_{i j}. & & \text{}
\end{align}
$$

So $\left( A B \right)C=A\left( B C \right)$ .

(ii)

$$
\left(\left( A \left( \lambda B + \mu C \right) \right)\right)_{i j} = \sum_{s = 1}^{m} a_{i s} \left( \lambda b_{s j} + \mu c_{s j} \right) = \lambda \sum_{s = 1}^{m} a_{i s} b_{s j} + \mu \sum_{s = 1}^{m} a_{i s} c_{s j} = \left(\left( \lambda A B \right)\right)_{i j} + \left(\left( \mu A C \right)\right)_{i j} .
$$

So $A\left( \lambda B + \mu C \right)=\lambda AB+\mu AC$ . Similary $\left( \lambda B + \mu C \right)D=\lambda BD+\mu CD$ . â–¡

Further properties

*   Matrix multiplication is usually not commutative: $AB\neq BA$ . For instance, $A=\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}$ and $B=\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}$ , $AB=\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$ , but $BA=\begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}.$
*   Identity matrix $I=I_{n}\in M_{n , n}\left( ğ”½ \right)$ given by
    
    $$
    \begin{align}
    I = \begin{pmatrix} 1 & & 0 \\ & \ddots & \\ 0 & & 1 \end{pmatrix}
    \end{align}
    $$
    
    is multiplicative identity: $IA=A$ , $BI=B$ , $\forall A,B\in M_{n , n}\left( ğ”½ \right)$ .
    
*   Any $A\in M_{m , n}\left( ğ”½ \right)$ has a transpose $A^{T}\in M_{n , m}\left( ğ”½ \right)$ given by
    
    $$
    A^{T} = \left( a_{i j}^{â€²} \right) \in M_{n , m} , a_{i j}^{â€²} = a_{j i} .
    $$
    
    This satisfies
    
    $$
    \left(\left( A B \right)\right)^{T} = B^{T} A^{T} .
    $$
    
    The equality can be checked by comparing the $\left( i , j \right)$ -entries on both, we skip the details.
    

We can also multiply an $m\times n$ matrix $A$ and a vector $x\in\left(ğ”½\right)^{n}$ to get a vector $Ax\in\left(ğ”½\right)^{m}$ , defined by

$$
\left(\left( A x \right)\right)_{i} = \underset{j}{\sum} A_{i j} x_{j} .
$$

Remark: (i) If we identify $\left(ğ”½\right)^{m}$ with $M_{m , 1}\left( ğ”½ \right)$

$$
x = \left( x_{1} , \dots  , x_{m} \right) \Leftrightarrow \begin{pmatrix} x_{1} \\ \vdots \\ x_{m} \end{pmatrix}
$$

then the product $Ax$ is exactly the same as matrix multiplication.

(ii) We can view each column (respectively row) as a column vector (respectively row vector). Then Column $k$ of the product $AB$ of matrices $A$ and $B$ is the same as multiplying column $k$ of $B$ by $A$ on the left:

$$
\left(col\right)_{k} \left( A B \right) = A \left(col\right)_{k} \left( B \right) .
$$

Similarly,

$$
\left(row\right)_{k} \left( A B \right) = \left(row\right)_{k} \left( A \right) B .
$$

#### [Linear maps between coordinate spaces](MA10210li1.html#QQ2-6-9)

Given $A\in M_{m , n}\left( ğ”½ \right)$ , let

$$
\left(\phi\right)_{A} : \left(ğ”½\right)^{n} \rightarrow \left(ğ”½\right)^{m} , x \rightarrow tail A x .
$$

E.g.Â if $A=\begin{pmatrix} 2 & 3 \\ 1 & 5 \end{pmatrix}$ then

$$
\left(\phi\right)_{A} : x \rightarrow tail A \begin{pmatrix} x_{1} \\ x_{2} \end{pmatrix} = \begin{pmatrix} 2x_{1}+3x_{2} \\ x_{1}+5x_{2} \end{pmatrix} .
$$

Remark: In the example above, $Ax$ is also equal to

$$
x_{1} \begin{pmatrix} 2 \\ 1 \end{pmatrix} + x_{2} \begin{pmatrix} 3 \\ 5 \end{pmatrix} = x_{1} \left(col\right)_{1} \left( A \right) + x_{2} \left(col\right)_{2} \left( A \right) .
$$

In generally, for $x\in\left(ğ”½\right)^{n}$ , $A\in M_{m , n}\left( ğ”½ \right)$ ,

$$
\begin{align}
Ax & = & \sum_{j = 1}^{n}x_{j}\left(col\right)_{j}\left( A \right), & \text{}
\end{align}
$$

a linear combination of column vectors of $A$ : $\left(col\right)_{1}\left( A \right),\dots ,\left(col\right)_{n}\left( A \right)$ .

Definition 3.  
A map $\psi:\left(ğ”½\right)^{n}\rightarrow\left(ğ”½\right)^{m}$ is linear if $\forall v,w\in\left(ğ”½\right)^{n}$ , $\lambda,\mu\inğ”½$ ,

$$
\psi \left( \lambda v + \mu w \right) = \lambda \psi \left( v \right) + \mu \psi \left( w \right) .
$$

Lemma 4.  

(a)

For any $A\in M_{m , n}$ , the multiplication map $\left(\phi\right)_{A}$ is linear.

(b)

For any $A\in M_{m , n}\left( ğ”½ \right)$ and $B\in M_{n , p}\left( ğ”½ \right)$ ,

$$
\left(\phi\right)_{A B} = \left(\phi\right)_{A} \circ \left(\phi\right)_{B} : \left(ğ”½\right)^{p} \rightarrow \left(ğ”½\right)^{m} .
$$

Proof. [(a)](#x6-90041) amounts to saying that $A\left( \lambda x + \mu y \right)=\lambda Ax+\mu By$ , which is part of the bilinearity of matrix multiplication.

[(b)](#x6-90052) says $A\left( B x \right)=\left( A B \right)x$ , which follows from associativity of matrix multiplication. â–¡

Note.  
$\psi$ linear implies

$$
\psi \left(\sum_{i = 1}^{n} \left(\lambda\right)_{i} v_{i}\right) = \sum_{i = 1}^{n} \left(\lambda\right)_{i} \psi \left( v_{i} \right) ,
$$

i.e. $\psi$ preserves all linear combinations.

Proposition 5.  
Every linear map $\psi:\left(ğ”½\right)^{n}\rightarrow\left(ğ”½\right)^{m}$ is of the form $\phi=\left(\phi\right)_{A}$ for a unique matrix $A\in M_{m , n}\left( ğ”½ \right)$ .

Proof. Consider â€œelementary vectorsâ€

$$
e_{k} = \left(col\right)_{k} \left( I \right) \in \left(ğ”½\right)^{n} ,
$$

i.e.

$$
e_{1} = \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix} , \dots  , e_{n} = \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{pmatrix} .
$$

Every

$$
x = \begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix} \in \left(ğ”½\right)^{n}
$$

is a linear combination

$$
x = x_{1} e_{1} + \dots  + x_{n} e_{n} .
$$

Since $\psi$ is linear,

$$
\begin{align}
\psi\left( x \right) & = & x_{1}\psi\left( e_{1} \right)+\dots +x_{n}\psi\left( e_{n} \right) & \text{} \\ & = & Ax & \text{}
\end{align}
$$

for $A\in M_{m , n}\left( ğ”½ \right)$ with

$$
\left(col\right)_{k} \left( A \right) = \psi \left( e_{k} \right) \in \left(ğ”½\right)^{m} .
$$

As $A$ is determined by $\psi$ , it is unique. More explicitly,

$$
\begin{align}
\left(\phi\right)_{A}=\left(\phi\right)_{B} & \Rightarrow\left(\phi\right)_{A}\left( e_{k} \right)=\left(\phi\right)_{B}\left( e_{k} \right)\forall k & & \\ & \text{i.e.}\left(col\right)_{k}\left( A \right)=\left(col\right)_{k}\left( B \right)\forall k & & \\ & \Rightarrow A=B. & \square
\end{align}
$$

[[next](MA10210se4.html)] [[prev](MA10210se2.html)] [[prev-tail](MA10210se2.html#tailMA10210se2.html)] [[front](MA10210se3.html)] [[up](MA10210ch1.html#MA10210se3.html)]