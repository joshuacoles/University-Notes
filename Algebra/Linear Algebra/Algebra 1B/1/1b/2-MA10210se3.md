# Field

A [[Field]], generally denoted $\F$, is a [[Commutative Ring]] in which every non-zero element is invertible. For $\F$ we have,

- $(\F, +)$ is an additive [[Abelian Group]].
- $(\F \setminus \set{0}, \cdot)$ is a multiplicative [[Abelian Group]].
- $x \cdot$ is a [[Homomorphisms|Homomorphism]] $(\F, +) \to (\F, +)$.

## Examples
Rationals $\Q$, reals $\R$, complex numbers $\C$, finite fields $(\F)_p=\Z_p$, integers $modp$ for $p$ prime, e.g. $\F_2=\set{0, 1}$.

Integers $\Z$ and $\Z_6$ are not fields, as any nonzero integer different from $\pm1$ has no multiplicative inverse and in $\Z_6$, $2 \cdot 3=0$ and so $2^{-1},3^{-1}$ does not exist.

## Associated constructs

To any field $\F$ can additional constructs such as.

### Tuples

The space of (coordinate) vectors

$$
\F^n = \F \cp \underbrace{\dots}_{\text{\(n\) times}} \cp \F. $$

This is an additive group with operations on components

$$
\begin{align}
(x_1, \dots, x_n) + (y_1, \dots, y_n) &= (x_1 + y_1, \dots, x_n + y_n) \\
-(x_1, \dots, x_n) &= (-x_1, \dots, -x_n) \\
0 &= (0, \dots, 0)
\end{align}
$$

and the operation of scalar multiplication by $\lambda \in \F$

$$
\lambda(x_1, \dots, x_n) = (\lambda x_1, \dots, \lambda x_n)
$$

making $\F^n$ the prototype vector space [treated axiomatically in Chapter 2].

### Matrices

The space $M_{m, n}(\F)$ of $m\times n$ matrices over $\F$

$$
A = \pmat{
	a_{1 1} & \dots  & a_{1 n} \\
	\vdots  & \ddots & \vdots  \\
	a_{n 1} & \dots  & a_{m n}
}
\qquad \Forall a_{i j} \in \F 
$$

Write $A = (a_{i j})$ for short. Each $M_{m, n}(\F)$ is an additive group with scalar multiplication; all operations are defined component wise as in $(\F)^n$ . That is, for $A=(a_{i j})$ $B=(b_{i j})$ and $\lambda\in\F$,

$$
A + B = (a_{i j} + b_{i j}) \text{ and } \lambda A = (\lambda a_{i j}) .
$$

# Matrix Multiplication

We can multiply an $m \times n$ matrix $A = (a_{i j})$ and an $n \times p$ matrix $B=(b_{i j})$ to get an $m\times p$ matrix $AB$ defined by

$$
(A B)_{i k} = \sum_{j = 1}^n a_{i j} b_{j k}
$$

## Lemma 2. 

Properties of matrix multiplication:

1. [[Associativity]]: for $A\in M_{l, m}(\F),B\in M_{m, n}(\F)$ and $C\in M_{n, p}(\F)$,  

$$
(A B) C = A (B C),
$$

2. [[bilinearity]]: for $A\in M_{l, m}(\F),B,C\in M_{m, n}(\F)$, $D\in M_{n, p}(\F)$ and $\lambda,\mu\in\F$, 
$$
\begin{align}
 A(\lambda B + \mu C)=\lambda AB+\mu AC  \\ (\lambda B + \mu C)D=\lambda BD+\mu CD.  \\ 
\end{align}
$$

### Proof.

Compare the $(i, j)$ -entries both on sides in each case.

1. $$
\begin{align}
(((A B) C))_{i j}= \sum_{s = 1}^n((A B))_{i s}c_{s j}  \\ = \sum_{s = 1}^n(\sum_{t = 1}^m a_{i t} b_{t s})c_{s j}  \\ = \sum_{s = 1}^n\sum_{t = 1}^ma_{i t}b_{t s}c_{s j}  \\ = \sum_{t = 1}^ma_{i t}\sum_{s = 1}^nb_{t s}c_{s j}  \\ = ((A (B C)))_{i j}. 
\end{align}
$$

So $(A B)C=A(B C)$ .

2. $$
((A (\lambda B + \mu C)))_{i j} = \sum_{s = 1}^m a_{i s} (\lambda b_{s j} + \mu c_{s j}) = \lambda \sum_{s = 1}^m a_{i s} b_{s j} + \mu \sum_{s = 1}^m a_{i s} c_{s j} = ((\lambda A B))_{i j} + ((\mu A C))_{i j} .
$$

So $A(\lambda B + \mu C)=\lambda AB+\mu AC$ . Similary $(\lambda B + \mu C)D=\lambda BD+\mu CD$ . □

# Further properties

*   Matrix multiplication is usually not commutative: $AB\neq BA$ . For instance, $A=\begin{pmatrix} 0 1 \\ 0 0 \end{pmatrix}$ and $B=\begin{pmatrix} 0 0 \\ 1 0 \end{pmatrix}$, $AB=\begin{pmatrix} 1 0 \\ 0 0 \end{pmatrix}$, but $BA=\begin{pmatrix} 0 0 \\ 0 1 \end{pmatrix}.$
*   Identity matrix $I=I_n\in M_{n, n}(\F)$ given by
    
    $$
    \begin{align}
    I = \begin{pmatrix} 1 0 \\ \ddots \\ 0 1 \end{pmatrix}
    \end{align}
    $$
    
    is multiplicative identity: $IA=A$, $BI=B$, $\forall A,B\in M_{n, n}(\F)$ .
    
*   Any $A\in M_{m, n}(\F)$ has a transpose $A^T\in M_{n, m}(\F)$ given by
    
    $$
    A^T = (a_{i j}^{′}) \in M_{n, m}, a_{i j}^{′} = a_{j i} .
    $$
    
    This satisfies
    
    $$
    ((A B))^T = B^T A^T .
    $$
    
    The equality can be checked by comparing the $(i, j)$ -entries on both, we skip the details.
    

We can also multiply an $m\times n$ matrix $A$ and a vector $x\in(\F)^n$ to get a vector $Ax\in(\F)^m$, defined by

$$
((A x))_i = \underset{j}{\sum} A_{i j} x_j .
$$

## Remark

- (i) If we identify $(\F)^m$ with $M_{m, 1}(\F)$

$$
x = (x_1, \dots, x_m) \Leftrightarrow \begin{pmatrix} x_1 \\ \vdots \\ x_m \end{pmatrix}
$$

then the product $Ax$ is exactly the same as matrix multiplication.

- (ii) We can view each column (respectively row) as a column vector (respectively row vector). Then Column $k$ of the product $AB$ of matrices $A$ and $B$ is the same as multiplying column $k$ of $B$ by $A$ on the left:

$$
\col_k (A B) = A \col_k (B) .
$$

Similarly,

$$
(row)_k (A B) = (row)_k (A) B .
$$

# Linear maps between coordinate spaces

Given $A\in M_{m, n}(\F)$, let

$$
(\phi)_A : \F^n \to \F^m, x \mapsto A x .
$$

E.g. if $A = \pmat{2 3 \\ 1 5}$ then

$$
(\phi)_A : x \mapsto A \pmat{x_1 \\ x_2} = \pmat{2 x_1 + 3 x_2 \\ x_1 + 5 x_2}.
$$

Remark: In the example above, $Ax$ is also equal to

$$
x_1 \pmat{2 \\ 1} + x_2 \pmat{3 \\ 5} = x_1 \col_1 (A) + x_2 \col_2(A) .
$$

In generally, for $x\in(\F)^n$, $A\in M_{m, n}(\F)$,

$$
\begin{align}
Ax &= \sum_{j = 1}^nx_j \col_j(A), 
\end{align}
$$

a linear combination of column vectors of $A$ : $\col_1(A),\dots,\col_n(A)$ .

## Definition 3.  
A map $\psi:(\F)^narrow(\F)^m$ is linear if $\forall v,w\in(\F)^n$, $\lambda,\mu\in\F$,

$$
\psi (\lambda v + \mu w) = \lambda \psi (v) + \mu \psi (w) .
$$

## Lemma 4.  

a.

For any $A\in M_{m, n}$, the multiplication map $(\phi)_A$ is linear.

b.

For any $A\in M_{m, n}(\F)$ and $B\in M_{n, p}(\F)$,

$$
(\phi)_{A B} = (\phi)_A \circ (\phi)_B : (\F)^p arrow (\F)^m .
$$

### Proof
[(a)](#x6-90041) amounts to saying that $A(\lambda x + \mu y)=\lambda Ax+\mu By$, which is part of the bilinearity of matrix multiplication.

[(b)](#x6-90052) says $A(B x)=(A B)x$, which follows from associativity of matrix multiplication. □

Note.  
$\psi$ linear implies

$$
\psi (\sum_{i = 1}^n (\lambda)_i v_i) = \sum_{i = 1}^n (\lambda)_i \psi (v_i),
$$

i.e. $\psi$ preserves all linear combinations.

## Proposition 5  
Every linear map $\psi:(\F)^narrow(\F)^m$ is of the form $\phi=(\phi)_A$ for a unique matrix $A\in M_{m, n}(\F)$ .

### Proof Consider “elementary vectors”

$$
e_k = \col_k (I) \in (\F)^n,
$$

i.e.

$$
e_1 = \pmat{1 \\ 0 \\ \vdots \\ 0},
\dots,
e_n = \pmat{0 \\ \vdots \\ 0 \\ 1}.
$$

Every

$$
x = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \in (\F)^n
$$

is a linear combination

$$
x = x_1 e_1 + \dots  + x_n e_n .
$$

Since $\psi$ is linear,

$$
\begin{align}
\psi(x) &= x_1\psi(e_1)+\dots +x_n\psi(e_n)  \\ &= Ax 
\end{align}
$$

for $A\in M_{m, n}(\F)$ with

$$
\col_k (A) = \psi (e_k) \in (\F)^m .
$$

As $A$ is determined by $\psi$, it is unique. More explicitly,

$$
\begin{align}
(\phi)_A=(\phi)_B \Rightarrow(\phi)_A(e_k)=(\phi)_B(e_k)\forall k \\ \text{i.e.}\col_k(A)=\col_k(B)\forall k \\ \Rightarrow A=B. \square
\end{align}
$$
