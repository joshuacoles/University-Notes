[[prev](MA10210se4.html)] [[prev-tail](MA10210se4.html#tailMA10210se4.html)] [[tail](#tailMA10210se5.html)] [[up](MA10210ch1.html#MA10210se5.html)]

### 1.5 [Invertible matrices](MA10210.html#QQ2-8-13)

Definition 1.  
A matrix $A\in M_{m , n}\left( ùîΩ \right)$ is invertible if $\exists$ matrix $X\in M_{n , m}\left( ùîΩ \right)$ such that

$$
AX=I_{m}\text{and}XA=I_{n}.
$$

(1)

E.g., the identity $n\times n$ is invertible, as $IX=X=XI$ for any $n\times n$ matrix $X$ .

Remark 2.  
We will see in Lemma [1.5.4](#x8-14001r4) that such a matrix $X$ is unique and it is called the $inverse$ of $A$ and is denoted by $A^{- 1}$ .

Example 3.  
Matrices of EROs and their inverses. The matrix of an ERO is obtained by applying the ERO to the identity matrix. Applying an ERO to a matrix $X$ is then the same as multiplying $X$ from the left by the matrix of the ERO. The examples given below are $3\times 3$ matrices, but all these matrices exist in any $M_{n , n}\left( ùîΩ \right)$ . (I)

$$
\begin{align}
 & P=\begin{pmatrix} \begin{matrix}1 & 0 & 0 \\ 0 & \lambda & 0 \\ 0 & 0 & 1\end{matrix} \end{pmatrix} & P^{- 1}=\begin{pmatrix} \begin{matrix}1 & 0 & 0 \\ 0 & \left(\lambda\right)^{- 1} & 0 \\ 0 & 0 & 1\end{matrix} \end{pmatrix} & \text{} \\ & R_{2}\rightarrow tail\lambda R_{2} & R_{2}\rightarrow tail\left(\lambda\right)^{- 1}R_{2} & \text{}
\end{align}
$$

(II)

$$
\begin{align}
 & P=\begin{pmatrix} \begin{matrix}1 & 0 & \lambda \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{matrix} \end{pmatrix} & P^{- 1}=\begin{pmatrix} \begin{matrix}1 & 0 & -\lambda \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{matrix} \end{pmatrix} & \text{} \\ & R_{1}\rightarrow tailR_{1}+\lambda R_{3} & R_{1}\rightarrow tailR_{1}-\lambda R_{3} & \text{}
\end{align}
$$

(III)

$$
\begin{align}
 & P=\begin{pmatrix} \begin{matrix}0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1\end{matrix} \end{pmatrix} & P^{- 1}=\begin{pmatrix} \begin{matrix}0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1\end{matrix} \end{pmatrix} & \text{} \\ & R_{1}\leftrightarrowR_{2} & R_{1}\leftrightarrowR_{2} & \text{}
\end{align}
$$

#### [Properties of invertible matrices](MA10210li1.html#QQ2-8-14)

Lemma 4.  
Let $A\in M_{m , n}\left( ùîΩ \right)$ .

(a)

If $X^{‚Ä≤}A=I_{n}$ and $AX^{''}=I_{m}$ for some $X^{‚Ä≤},X^{''}\in M_{n , m}\left( ùîΩ \right)$ then $X^{‚Ä≤}=X^{''}$ , i.e.¬†if $A$ has both a right inverse and a left inverse then they are equal, so $A$ is invertible.

In particular, if $A$ is invertible then the matrix $X$ is uniquely determined by ([1.5.1](#x8-13002r1)) and so we write $X=A^{- 1}$ .

Proof.

$$
\begin{align}
X^{‚Ä≤} & =X^{‚Ä≤}I & & \\ & =X^{‚Ä≤}\left( A X^{''} \right) & & \\ & =\left( X^{‚Ä≤} A \right)X^{''} & & \\ & =IX^{''}=X^{''}. & \square
\end{align}
$$

(b)

If $A$ is invertible then so is $A^{- 1}$ , and $\left(\left( A^{- 1} \right)\right)^{- 1}=A$ .

Proof. ([1.5.1](#x8-13002r1)) also says $X^{- 1}=A$ . ‚ñ°

(c)

If $A$ is invertible, then $A$ has no zero rows and no zero columns. (Equivalently, if $A$ has a zero row or column, then $A$ is not invertible.)

Proof. If $XA=I$ then

$$
X \left(col\right)_{i} \left( A \right) = \left(col\right)_{i} \left( I \right) \neq 0 ,
$$

so $\left(col\right)_{i}\left( A \right)$ cannot be zero. Similarly if $AX=I$ then

$$
\left(row\right)_{i} \left( A \right) X = \left(row\right)_{i} \left( I \right) \neq 0 
$$

so $\left(row\right)_{i}\left( A \right)$ cannot be zero. ‚ñ°

(d)

$A$ invertible $\Rightarrow$ $\left(\phi\right)_{A}$ is a bijection

Proof. Since $\left(\phi\right)_{I}$ is the identity map and matrix multiplication corresponds to composition, if $A^{- 1}$ exists then $\left(\phi\right)_{A^{- 1}}$ is inverse to $\left(\phi\right)_{A}$ . ‚ñ°

(e)

$A\in M_{m , n}\left( ùîΩ \right)$ invertible $\Rightarrow$ $A$ square, i.e.¬† $m=n$

Proof. Suppose $A\in M_{m , n}\left( ùîΩ \right)$ is invertible. If $n>m$ , then $Ax=0$ has non-trivial solutions by the Fundamental Lemma [1.2.6](MA10210se2.html#x5-6001r6), contradicting that $\left(\phi\right)_{A}$ is injective according to [(d)](#x8-140064). On the other hand, if $m>n$ then $A^{- 1}x=0$ would have a non-trivial solution, contradicting that $A^{- 1}$ is invertible. So $m=n$ is the only possibility. ‚ñ°

(f)

If $A$ and $B$ are $n\times n$ invertibel matrices, then so is $AB$ and 
$$
\begin{align}
\left(\left( A B \right)\right)^{- 1}=B^{- 1}A^{- 1}.\left( \text{ (See Q2.3)} \right) & & & \text{}
\end{align}
$$

More properties of invertible matrices will be discussed later.

(f)

$\left(\phi\right)_{A}$ is a bijection $\Rightarrow$ $A$ invertible (uses that $\left(\left( \left(\phi\right)_{A} \right)\right)^{- 1}$ is linear, see Lemma [2.2.6](MA10210se7.html#x11-19009r6))

(g)

If $A$ is square and $AX=I$ , then also $XA=I$ , i.e.¬† $A$ is invertible (Proposition [4.4.10](MA10210se19.html#x25-38018r10))

(h)

If $A$ is square, then $A$ invertible $\Leftrightarrow$ $ \det A\neq 0$ (see Chapter 4).

Note that the associativity of matrix multiplication implies that the set of invertible $n\times n$ matrices form a group under multiplication! (The identity matrix is the identity element.)

Definition 5.  
The general linear group $GL_{n}\left( ùîΩ \right)$ is the set of invertible matrices in $M_{n , n}\left( ùîΩ \right)$ equipped with matrix multiplication.

Note: $GL_{n}\left( ùîΩ \right)$ is an important group in algebra and geometry.

#### [Why Gaussian elimination works](MA10210li1.html#QQ2-8-15)

(I.e.¬†why EROs give equivalent linear system.) Key observation:

Lemma 6.  
If $P$ is an invertible $m\times m$ matrix and $A\in M_{m , n}\left( ùîΩ \right)$ , then

$$
A x = b \Leftrightarrow \left( P A \right) x = P b ,
$$

i.e. the two linear systems have the same solutions.

Proof. First,

$$
\begin{align}
 & & Ax=b & \text{} \\ & \Rightarrow & \left( P A \right)x=P\left( A x \right)=Pb. & \text{}
\end{align}
$$

Conversely, if $P^{- 1}$ exists, then

$$
\begin{align}
\left( P A \right)x=Pb & \Rightarrow P^{- 1}PAx=P^{- 1}Pb & & \\ & \Rightarrow IAx=Ib & & \\ & \Rightarrow Ax=b. & \square
\end{align}
$$

ERO‚Äôs give an equivalent system because they convert $Ax=b$ into

$$
P A x = P b ,
$$

where $P$ is the product of the matrices of the ERO‚Äôs applied on to the augmented matrix $\left( A \left| b \right)$ . So Gauss elimination works.

#### [Computation of the inverse of a matrix: Gauss-Jordan method](MA10210li1.html#QQ2-8-16)

When we convert any matrix $A$ to REF or RREF, we can keep track of the $P$ that represents the composition of the EROs by applying the EROs to the augmented matrix $\left( A \left| I \right)$ : the result will be $\left( P A \left| P \right)$ , so $P$ can be read off from the right hand side. In particular, this leads to the Gauss-Jordan method for finding the inverse of a square matrix $A$ :

1.

Write augmented matrix $\left( A \left| I \right)$ .

2.

Use EROs to convert to $\left( P A \left| P \right)$ , with $PA$ in RREF.

3.

Now

either

There exists a pivot in every row (and hence in every column, because $A$ is square). That is, $PA=I$ . As $P$ is invertible, $P^{- 1}=A$ and so $A$ is invertible and we have found $A^{- 1}$ , i.e. $=P$ .

or

RREF has a zero row 
$$
\begin{align}
 & \Rightarrow & \text{}PA\text{ is not invertible (though }P\text{ is) [Lemma }\text{1.5.4}\text{}\text{(c)}\text{]} & \text{} \\ & \Rightarrow & \text{}A\text{ is not invertible [see Q2.3]} & \text{}
\end{align}
$$

Example 7.  
To find inverse of $A=\begin{pmatrix} 2 & 3 \\ 1 & 2 \end{pmatrix}$

$$
\begin{align}
 & & \begin{pmatrix} \begin{matrix}2 & 3 & 1 & 0 \\ 1 & 2 & 0 & 1\end{matrix} \end{pmatrix} & \text{} \\ & \rightarrow _{}^{R_{1} \leftrightarrow R_{2}} & \begin{pmatrix} \begin{matrix}1 & 2 & 0 & 1 \\ 2 & 3 & 1 & 0\end{matrix} \end{pmatrix} & \text{} \\ & \rightarrow _{}^{R_{2} - 2 R_{1}} & \begin{pmatrix} \begin{matrix}1 & 2 & 0 & 1 \\ 0 & -1 & 1 & -2\end{matrix} \end{pmatrix} & \text{} \\ & \rightarrow _{}^{- R_{2}} & \begin{pmatrix} \begin{matrix}1 & 2 & 0 & 1 \\ 0 & 1 & -1 & 2\end{matrix} \end{pmatrix} & \text{} \\ & \rightarrow _{}^{R_{1} - 2 R_{2}} & \begin{pmatrix} \begin{matrix}1 & 0 & 2 & -3 \\ 0 & 1 & -1 & 2\end{matrix} \end{pmatrix}, & \text{}
\end{align}
$$

so

$$
\begin{align}
A^{- 1} = \begin{pmatrix} 2 & -3 \\ -1 & 2 \end{pmatrix} .
\end{align}
$$

Check

$$
\begin{align}
 & & \begin{pmatrix} \begin{matrix}2 & 3 \\ 1 & 2\end{matrix} \end{pmatrix}\begin{pmatrix} \begin{matrix}2 & -3 \\ -1 & 2\end{matrix} \end{pmatrix}=\begin{pmatrix} \begin{matrix}1 & 0 \\ 0 & 1\end{matrix} \end{pmatrix} & \text{} \\ & & \begin{pmatrix} \begin{matrix}2 & -3 \\ -1 & 2\end{matrix} \end{pmatrix}\begin{pmatrix} \begin{matrix}2 & 3 \\ 1 & 2\end{matrix} \end{pmatrix}=\begin{pmatrix} \begin{matrix}1 & 0 \\ 0 & 1\end{matrix} \end{pmatrix}. & \text{}
\end{align}
$$

[[prev](MA10210se4.html)] [[prev-tail](MA10210se4.html#tailMA10210se4.html)] [[front](MA10210se5.html)] [[up](MA10210ch1.html#MA10210se5.html)]