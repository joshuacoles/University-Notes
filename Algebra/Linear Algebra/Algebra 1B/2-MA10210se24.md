[[prev](MA10210se23.html)] [[prev-tail](MA10210se23.html#tailMA10210se23.html)] [[tail](#tailMA10210se24.html)] [[up](MA10210ch5.html#MA10210se24.html)]

### 5.4 [Jordan normal form (optional)](MA10210.html#QQ2-31-47)

Definition 1.  
(1) A $k\times k$ Jordan block is a matrix of the form

$$
\begin{align}
J_{k} \left( \lambda \right) = \begin{pmatrix} \lambda & 1 & 0 & \dots  & 0 & 0 \\ 0 & \lambda & 1 & 0 & \dots  & 0 \\ \vdots \\ 0 & 0 & 0 & \dots  & \lambda & 1 \\ 0 & 0 & 0 & \dots  & 0 & \lambda \end{pmatrix} \in M_{k \times k} \left( ùîΩ \right) ,
\end{align}
$$

i.e., the diagonal entries are $\lambda$ , the ‚Äúsuperdiagonal entries‚Äù are 1, and all other entries are 0.  

(2) A matrix $A$ is said to be in Jordan normal form if it is of the form

$$
\begin{align}
\begin{pmatrix} J_{k_{1}}\left( \left(\lambda\right)_{1} \right) \\ & J_{k_{2}}\left( \left(\lambda\right)_{2} \right) \\ & & \ddots \\ & & & J_{k_{m}}\left( \left(\lambda\right)_{m} \right) \end{pmatrix}
\end{align}
$$

i.e., with Jordan blocks along the diagonal and zeros elsewhere.

Remark 2.  

*   By definition, the eigenvalues of a matrix in JNF are the diagonal entries, $\left(\lambda\right)_{1},\dots ,\left(\lambda\right)_{m}$ . Note that these values are not necessarily distinct.
*   Any diagonal matrix is in Jordan normal form with all Jordan blocks of size $1\times 1$ .

Theorem 3.  
Let $V$ be an $n$ -dimensional vector space over $‚ÑÇ$ and $f:V\rightarrow V$ a linear operator. Then there exists a basis $\alpha$ such that with respect to this basis $f$ is represented by a matrix in JNF.

Remark 4.  

*   The JNF of $f$ in the theorem is unique, up to reordering of the Jordan blocks. First, the diagonal entries are eigenvalues of $f$ . Second, the sizes of the Jordan blocks are determined by $ \rank \left(\left( f - \left(\lambda\right)_{i} \left(Id\right)_{V} \right)\right)^{s}$ , for all $\left(\lambda\right)_{i}$ and $1\leq s\leq k_{i}$ .
*   If we work over an arbitrary fields, the JNF of a linear operator may not exist. See for instance Example, where the matrix has no eigenvalues in $‚Ñù$ and so it has no JNF over $‚Ñù$ .
*   Any matrix $A\in M_{n \times n}\left( ‚ÑÇ \right)$ is viewed as a linear operator $\left(ùîΩ\right)^{n}\rightarrow\left(ùîΩ\right)^{n}$ . So the theorem says that any $n\times n$ matrix over $‚ÑÇ$ is similar to a JNF matrix.

Proof of Theorem [5.4.3](#x31-47003r3). We prove the theorem by induction on $n$ . When $n=0,1$ , it is clear. Assume that the statement holds for linear operators on spaces with dimension smaller than $n$ .

Let $\lambda$ be an eigenvalue of $f$ . Note that $A$ has at least an eigenvalue, because its characteristic polynomial has at least one root in $‚ÑÇ$ . Let $f_{\lambda}$ be the linear operator

$$
f_{\lambda} = f - \lambda \left(Id\right)_{V} : V \rightarrow V , v \rightarrow tail f \left( v \right) - \lambda v .
$$

As $\lambda$ is a eigenvalue of $f$ , $kerf_{\lambda}\neq\left\{ 0 \right\}$ and by Rank-Nullity-Theorem,

$$
dim  \Im  f < dim V .
$$

Denote the image of $f_{\lambda}$ by $W$ . Note that

$$
f \left( f - \lambda Id \right) = \left( f - \lambda Id \right) f
$$

and so

$$
f \left(\left|\right)_{W} : W \rightarrow W \text{ is a linear operator} .
$$

By induction, there exists a basis

$$
\beta : v_{1}^{1} , \dots  , v_{k_{1}}^{1} , v_{1}^{2} , \dots  , v_{k_{2}}^{2} , \dots  , v_{1}^{m} , \dots  , v_{k_{m}}^{m}
$$

such that $f\left(\left|\right)_{W}$ is represented by the JNF matrix

$$
\begin{align}
\begin{pmatrix} J_{k_{1}}\left( \left(\lambda\right)_{1} \right) \\ & J_{k_{2}}\left( \left(\lambda\right)_{2} \right) \\ & & \ddots \\ & & & J_{k_{m}}\left( \left(\lambda\right)_{m} \right) \end{pmatrix}
\end{align}
$$

Then

*   $f_{\lambda}\left(\left|\right)_{W}$ is a linear operator $W\rightarrow W$ and with respect to $\beta$ is represented by
    
    $$
    \begin{align}
    \begin{pmatrix} J_{k_{1}}\left( \left(\lambda\right)_{1} - \lambda \right) \\ & J_{k_{2}}\left( \left(\lambda\right)_{2} - \lambda \right) \\ & & \ddots \\ & & & J_{k_{m}}\left( \left(\lambda\right)_{m} - \lambda \right) \end{pmatrix}
    \end{align}
    $$
    
*   $kerf_{\lambda}\left(\left|\right)_{W}=span\left\{ v_{1}^{i} : \left(\lambda\right)_{i} = \lambda \right\}$.

When $\left(\lambda\right)_{i}=\lambda$ , we let

$$
v_{k_{i} + 1}^{i} \text{ such that } f_{\lambda} \left( v_{k_{i} + 1}^{i} \right) = v_{k_{i}}^{i} ,
$$

where $v_{k_{i} + 1}^{i}$ exists because $v_{k_{i}}^{i}\in W= \Im f_{\lambda}$ .  

Let $U$ be a subspace of $kerf_{\lambda}$ such that

$$
U \oplus ker f_{\lambda} \left(\left|\right)_{W} = ker f_{\lambda} .
$$

and let

$$
u_{1} , \dots  , u_{t} \text{ be a basis of U} .
$$

Denote by $\left(\beta\right)^{i}$ the sublist $v_{1}^{i},\dots v_{k_{i}}^{i}$ of $\beta$ and define  

$$
\left(\alpha\right)^{i} = \left\{\right)
$$

Œ≤i, if Œª i‚â†Œª; Œ≤i ‚à™ v ki+1i,otherwise.

Let $\alpha$ be the list:

$$
\left(\alpha\right)^{1} , \dots  , \left(\alpha\right)^{m} , u_{1} , \dots  , u_{t} .
$$

We claim that the list $\alpha$ is a basis of $V$ . Note that the number of element in $\alpha$ is $n$ , so it suffices to prove that $\alpha$ is linearly independent. Suppose that

$$
\underset{i , j \neq k_{i} + 1}{\sum} a_{i , j} v_{j}^{i} + \underset{i}{\sum} b_{i} v_{k_{i} + 1}^{i} + \underset{i}{\sum} c_{i} u_{i} = 0 . \left( * \right)
$$

Applying $f_{\lambda}$ gives,

$$
\underset{i , j \neq k_{i} + 1}{\sum} a_{i , j} \left( \left( \left(\lambda\right)_{i} - \lambda \right) v_{j}^{i} + v_{j - 1}^{i} \right) + \underset{i}{\sum} b_{i} v_{k_{i}}^{i} = 0
$$

Since $\beta$ is a basis,

*   when $\left(\lambda\right)_{i}\neq\lambda$ , $a_{i j}=0$ for all $j$ ;  
    
*   when $\left(\lambda\right)_{i}=\lambda$ , $a_{i , 1}=0$ for $j\geq 2$ ;  
    
*   $b_{i}=0$ , for all $i$ .

So (*) becomes

$$
\underset{i : \left(\lambda\right)_{i} = \lambda}{\sum} a_{i , 1} v_{1}^{i} + \underset{i}{\sum} c_{i} u_{i} = 0 .
$$

Note that $\left\{ v_{1}^{i} | \left(\lambda\right)_{i} = \lambda \right\}\cup\left\{ u_{1} , \dots  , u_{t} \right\}$ is a basis of $kerf_{\lambda}$ . So all the coefficients $a_{i , 1},c_{i}$ are $0$ . This proves the independence of $\alpha$ , as required.  

Finally, direct computation shows that with respect to the basis $\alpha$ , the linear operator $f$ is represented by the JNF matrix

$$
\begin{align}
\begin{pmatrix} J_{s_{1}}\left( \left(\lambda\right)_{1} \right) \\ & J_{s_{2}}\left( \left(\lambda\right)_{2} \right) \\ & & \ddots \\ & & & J_{s_{m}}\left( \left(\lambda\right)_{m} \right) \\ & & & & J_{1}\left( \lambda \right) \\ & & & & & \ddots \\ & & & & & & J_{1}\left( \lambda \right) \\ \end{pmatrix}
\end{align}
$$

where

$$
s_{i} = \left\{\right)
$$

ki, if Œªi‚â†Œª; ki + 1,otherwise.

‚ñ°

Example 5.  
Let $A=\begin{pmatrix} 2 & 1 \\ -1 & 0 \end{pmatrix}$ .

*   Compute the eigenvalues by solving
    
    $$
    \left(\Delta\right)_{A} \left( t \right) = \left( 2 - t \right) \left( - t \right) + 1 = t^{2} - 2 t + 1 = \left(\left( t - 1 \right)\right)^{2} = 0 .
    $$
    
    So $1$ is the unique eigenvalue and
    
    $$
    \begin{align}
    A_{1} = A - I = \begin{pmatrix} 1 & 1 \\ -1 & -1 \end{pmatrix} .
    \end{align}
    $$
    
*   $ \Im A_{1}=\langle e_{1} - e_{2} \rangle$ and $kerA_{1}=\langle e_{1} + e_{2} \rangle$ .  
    
*   $A\left( e_{1} - e_{2} \right)=e_{1}-e_{2}$ and with respect to $e_{1}-e_{2}$ , $A\left(\left|\right)_{ \Im  A_{1}}$ is represented by the $1\times 1$ matrix (1).  
    
*   $A_{1}e_{1}=e_{1}-e_{2}$ .  
    
*   Let $\alpha$ be the basis $e_{1}-e_{2},e_{1}$ . With respect to $\alpha$ , $A$ is represented by the matrix $\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}.$

Example 6.  
$A=\begin{pmatrix} 1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 2 \end{pmatrix}$ .

*   Observe that $1$ and $2$ are the eigenvalues. Choose $\lambda=1$ . Then
    
    $$
    \begin{align}
    A_{1} = \begin{pmatrix} 0 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix} .
    \end{align}
    $$
    
*   $ \Im A_{1}=\langle \beta : e_{1} + e_{2} , e_{3} \rangle$ and $kerA_{1}=\langle e_{1} \rangle$, which intersects $ \Im A_{1}$ trivially and is the space $U$ in the proof the theorem for $A_{1}$ .
*   With respect to $\beta$ , $A$ is represented by $\begin{pmatrix} 2 & 1 \\ 0 & 2 \end{pmatrix}$ , which is in JNF.
*   Let $\alpha=\beta\cup\left\{ e_{1} \right\}:e_{1}+e_{2},e_{3},e_{1}$ . With respect to $\alpha$ , $A$ is represented by
    
    $$
    \begin{align}
    \begin{pmatrix} 2 & 1 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1 \end{pmatrix} .
    \end{align}
    $$
    

Remark 7.  
Compared with Example [5.4.5](#x31-47005r5), Example [5.4.6](#x31-47006r6)¬† has one step less in finding the JNF. This is because the eigenvalue of $A\left(\left|\right)_{ \Im  A_{1}}$ is $2$ , different from $\lambda=1$ , we don‚Äôt need to find a vector that is mapped to the last basis element in $\beta$ under $A_{1}$ . A question one could ask here is how to find the basis $\beta$ ? In this example, $\beta$ can be obtained by observation and can also be computed following as in Example [5.4.5](#x31-47005r5).

Remark 8.  
In general, the proof of Theorem [5.4.3](#x31-47003r3) can be unfolded as follows to find the basis $\alpha$ and compute the JNF of a given matrix as follows.

*   Compute the eigenvalues of $A$ .  
    
*   Find a basis $\beta$ for $kerA_{\lambda}$ for each eigenvalue $\lambda$ by solving the equation $A_{\lambda}x=0,$ where $A_{\lambda}=A-\lambda I$ . We can require further that the basis $\beta$ is a disjoint union of two lists $\gamma$ and $\mu$ such that $\gamma$ is a basis of $kerA_{\lambda}\cap \Im A_{\lambda}$ .  
    
*   Then for each basis element $v$ in $\gamma$ , denoted by $b_{1}$ . Recursively finding a vector $b_{i}$ by solving the equation
    
    $$
    A_{\lambda} b_{i + 1} = b_{i} ,
    $$
    
    which implies that $Ab_{i + 1}=\lambda b_{i + 1}+b_{i}.$  
    
*   Now each such a string $b_{1},\dots b_{k}$ with $k$ maximal gives us a Jordan block $J_{\lambda}\left( k \right)$ . The basis $\alpha$ is then the union of all such strings and all the lists $\mu$ produced in (ii). With respect to $\alpha$ , we find a JNF of $A$ with the diagonal blocks $J_{\left(\lambda\right)_{1}}\left( k_{1} \right),\dots ,J_{\left(\lambda\right)_{m}}\left( k_{m} \right)$ and the $1\times 1$ Jordan blocks determined by the union of $\mu$ s from (ii). We can reorder them, so that the sizes of the blocks weakly decrease.

Remark 9.  
Jordan Normal Forms are to be discussed again in Algebra 2B from a different approach. We include it here, because it is a good example to apply some of the main techniques in this course and it has important applications in other subjects, for instance MA20220.

[[prev](MA10210se23.html)] [[prev-tail](MA10210se23.html#tailMA10210se23.html)] [[front](MA10210se24.html)] [[up](MA10210ch5.html#MA10210se24.html)]