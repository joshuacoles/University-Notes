[[next](MA10210se23.html)] [[prev](MA10210se21.html)] [[prev-tail](MA10210se21.html#tailMA10210se21.html)] [[tail](#tailMA10210se22.html)] [[up](MA10210ch5.html#MA10210se22.html)]

### 5.2 [Diagonalisation](MA10210.html#QQ2-29-42)

Observation: Let $\phi:V\rightarrow V$ be a linear operator, and let $A\in M_{n , n}\left( ùîΩ \right)$ be the matrix that represents it with respect to a basis $v_{1},\dots ,v_{n}$ . Then $A$ is a diagonal matrix

$$
\begin{align}
A = diag \left( \left(\lambda\right)_{1} , \dots  , \left(\lambda\right)_{n} \right) = \begin{pmatrix} \left(\lambda\right)_{1} \\ & \ddots \\ & & \left(\lambda\right)_{n} \end{pmatrix}
\end{align}
$$

if and only if each $v_{i}$ is an eigenvector of $\phi$ (with eigenvalue $\left(\lambda\right)_{i}$ ).

Definition 1.  

(a)

A linear operator $\phi:V\rightarrow V$ is diagonalisable iff there is a basis $\alpha$ of $V$ consisting of eigenvectors of $\phi$ .

(b)

A matrix $A\in M_{n , n}\left( ùîΩ \right)$ is diagonalisable over $ùîΩ$ (or simply ‚Äòdiagonalisable‚Äô if there is no risk of confusion) iff there is an invertible $P\in M_{n , n}\left( ùîΩ \right)$ such that $P^{- 1}AP$ is a diagonal matrix. Then $A$ is said to be diagonalised by $P$ .

Proposition 2.  

*   In Defintion [5.2.1](#x29-42001r1), $A$ satisfies (b) if and only if $\left(\phi\right)_{A}:\left(ùîΩ\right)^{n}\rightarrow\left(ùîΩ\right)^{n}$ , $x\rightarrow tailAx$ , satisfies (a).
*   Suppose that $A$ represents $\phi$ with respect to some basis $\beta$ of $V$ . Then $\phi$ is diagonalisable if and only if $A$ is diagonalisable.

Proof. (1) First note that $P$ is invertible $\Leftrightarrow$ $\left(col\right)_{1}\left( P \right),\dots \left(col\right)_{n}\left( P \right)$ is a basis of $\left(ùîΩ\right)^{n}$ .

Next,

$$
\begin{align}
 & P^{- 1}AP=D=diag\left( \left(\lambda\right)_{1} , \dots  , \left(\lambda\right)_{n} \right) & & \text{} \\ \Leftrightarrow & AP=PD,\left( \text{when } P \text{ is invertible} . \right) & & \text{} \\ \Leftrightarrow & A\left(col\right)_{j}\left( P \right)=P\left(col\right)_{j}\left( D \right)=\left(\lambda\right)_{j}\left(col\right)_{j}\left( P \right), & & \text{} \\ \Leftrightarrow & \left(\phi\right)_{A}\left( \left(col\right)_{j} \left( P \right) \right)=\left(\lambda\right)_{j}\left(col\right)_{j}\left( P \right),\forall j. & & \text{}
\end{align}
$$

So $A$ is diagonalisable if and only if $\left(\phi\right)_{A}$ is diagonalisable.  

(2) By assumption,

$$
\left(\phi\right)_{A} = \phi_{\beta}^{- 1} \circ \phi \circ \left(\phi\right)_{\beta} .
$$

So if $v$ is an eigenvector of $\phi$ with eigenvalue $\lambda$ , then

$$
\left(\phi\right)_{A} \left( \phi_{\beta}^{- 1} \left( v \right) \right) = \phi_{\beta}^{- 1} \circ \phi \circ \left(\phi\right)_{\beta} \left( \phi_{\beta}^{- 1} \left( v \right) \right) = \phi_{\beta}^{- 1} \left( \phi \left( v \right) \right) = \phi_{\beta}^{- 1} \left( \lambda v \right) = \lambda \phi_{\beta}^{- 1} \left( v \right) .
$$

That is $\phi_{\beta}^{- 1}\left( v \right)$ is an eigenvector of $\left(\phi\right)_{A}$ with eigenvalue $\lambda$ . Therefore, that $\phi$ is diagonalisable implies that $\left(\phi\right)_{A}$ is diagonalisable. Similarly, using $\phi=\left(\phi\right)_{\beta}\left(\phi\right)_{A}\phi_{\beta}^{- 1}$ , that $\left(\phi\right)_{A}$ is diagonalisable implies $\phi$ is diagonalisable. Therefore,

$\phi$ is diagonalisable $\Leftrightarrow$ $\left(\phi\right)_{A}$ is diagonalisable $\Leftrightarrow$ $A$ is diagonalisable, by (1). ‚ñ°

Example 3.  
Let $A=\begin{pmatrix} 1 & 3 \\ 3 & 1 \end{pmatrix}\in M_{2 , 2}\left( ‚Ñù \right)$ . In Example [4.5.7](MA10210se20.html#x26-39011r7), computed that

$$
A\begin{pmatrix} 1 \\ 1 \end{pmatrix}=4\begin{pmatrix} 1 \\ 1 \end{pmatrix}\text{ and }A\begin{pmatrix} 1 \\ -1 \end{pmatrix}=-2\begin{pmatrix} 1 \\ -1 \end{pmatrix}.
$$

(1)

Thus the basis $\begin{pmatrix} 1 \\ 1 \end{pmatrix},\begin{pmatrix} 1 \\ -1 \end{pmatrix}$ of $‚Ñù^{2}$ consists of eigenvectors of $\left(\phi\right)_{A}:‚Ñù^{2}\rightarrow‚Ñù^{2}$ , so $\left(\phi\right)_{A}$ is diagonalisable.

([5.2.1](#x29-42007r1))¬†can also be written as

$$
\begin{align}
A \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} = \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} 4 & 0 \\ 0 & -2 \end{pmatrix} .
\end{align}
$$

So if we set $P=\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}$ then $P^{- 1}AP$ is the diagonal matrix $\begin{pmatrix} 4 & 0 \\ 0 & -2 \end{pmatrix}$ , so $A$ is diagonalisable.

Note.  
Why could ‚Äòconfusion arise‚Äô in (b)? Let $ùîΩ$ be a subfield of $ùïÇ$ . Then $M_{n , n}\left( ùîΩ \right)\subseteq M_{n , n}\left( ùïÇ \right)$ . Thus for $A\in M_{n , n}\left( ùîΩ \right)$ , it makes sense to ask whether it is diagonalisable over $ùïÇ$ as well as whether it is diagonalisable over $ùîΩ$ . The following example shows a matrix may be diagonalisable over $ùïÇ$ , but not over $ùîΩ$ .

Example 4.  
Consider

$$
\begin{align}
 & & A=\begin{pmatrix} \begin{matrix}0 & -1 \\ 1 & 0\end{matrix} \end{pmatrix}\in M_{2 , 2}\left( ‚Ñù \right). & \text{}
\end{align}
$$

The corresponding operator $‚Ñù^{2}\rightarrow‚Ñù^{2}$ is rotation by $\frac{\pi}{2}$ so all vectors in $‚Ñù^{2}$ change direction, and hence there are no eigenvectors. More formally,

$$
\begin{align}
\left(\Delta\right)_{A} \left( t \right) = \begin{vmatrix} -t & -1 \\ 1 & -t \end{vmatrix} = t^{2} + 1
\end{align}
$$

has no roots in $‚Ñù$ . Hence $A$ is not diagonalisable over $‚Ñù$ .

However, $\left(\Delta\right)_{A}\left( t \right)$ does have roots $\lambda=\pmi$ in $‚ÑÇ$ . Considering $A$ as an element of $M_{n , n}\left( ‚ÑÇ \right)$ , it has independent eigenvectors

$$
\begin{pmatrix} 1 \\ -i \end{pmatrix} , \begin{pmatrix} 1 \\ i \end{pmatrix} \in ‚ÑÇ^{2} .
$$

So $A$ is diagonalisable over $‚ÑÇ$ .

#### [Application](MA10210ch5.html#QQ2-29-43)

Calculation of powers of matrices.

If $A$ is diagonalisable with

$$
P^{- 1} A P = D = diag \left( \left(\lambda\right)_{1} , \dots  , \left(\lambda\right)_{n} \right) ,
$$

then $A=PDP^{- 1}$ and so

$$
\begin{align}
A^{k} & = & \left( P D P^{- 1} \right)\dots \left( P D P^{- 1} \right) & \text{} \\ & = & PD^{k}P^{- 1} & \text{}
\end{align}
$$

with

$$
D^{k} = diag \left( \lambda _{1}^{k} , \dots  , \lambda _{n}^{k} \right) .
$$

So we can obtain an explicit formula for $A^{k}$ .

Note that the equality $A^{k}=PD^{k}P^{- 1}$ also implies that $A^{k}$ and $A$ have the same eigenvectors $\left(col\right)_{1}\left( P \right)$ , $\dots $ , $\left(col\right)_{n}\left( P \right)$ .  

Example 5 (Fibonacci Sequence).  
The famous Fibonacci sequence

$$
0 , 1 , 1 , 2 , 3 , 5 , 8 , 1 3 , 2 1 , 3 4 , \dots 
$$

is generated from $x_{0}=0$ , $x_{1}=1$ by the recurrence relation

$$
x_{n + 2} = x_{n + 1} + x_{n} .
$$

Setting $v_{n}=\begin{pmatrix} x_{n} \\ x_{n + 1} \end{pmatrix}$ converts it to a matrix recurrence

$$
\begin{align}
\begin{pmatrix} x_{n + 1} \\ x_{n + 2} \end{pmatrix} = \begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} x_{n} \\ x_{n + 1} \end{pmatrix}
\end{align}
$$

i.e. $v_{n + 1}=Av_{n}$ for $A=\begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix}$ with initial condition $v_{0}=\begin{pmatrix} 0 \\ 1 \end{pmatrix}$ .

Thus the solution is $v_{n}=A^{n}v_{0}$ or

$$
x_{n} = \left( 1 0 \right) v_{n} = \left( 1 0 \right) A^{n} \begin{pmatrix} 0 \\ 1 \end{pmatrix} .
$$

Find $A^{n}$ by diagonalisation, i.e. seeking a basis of eigenvectors. First, we compute the eigenvalues of $A$ , which are roots of the characteristic polynomial

$$
\begin{align}
\left(\Delta\right)_{A}\left( t \right) & = &  \det \left( A - t I \right) & \text{} \\ & = & \begin{vmatrix} \begin{matrix}-t & 1 \\ 1 & 1-t\end{matrix} \end{vmatrix} & \text{} \\ & = & t^{2}-t-1 & \text{}
\end{align}
$$

So the eigenvalues are

$$
\begin{align}
\left(\lambda\right)_{+} & = & \frac{1 + \sqrt{5}}{2}\approx1.618 & \text{} \\ & & \text{‚Äùgolden ratio‚Äù} & \text{} \\ \left(\lambda\right)_{-} & = & \frac{1 - \sqrt{5}}{2}\approx-0.618 & \text{}
\end{align}
$$

Next, for $\lambda$ -eigenvectors solve $\left( A - \lambda I \right)v=0$ , i.e.

$$
\begin{align}
 & & \begin{pmatrix} \begin{matrix}-\lambda & 1 \\ 1 & 1-\lambda\end{matrix} \end{pmatrix}\begin{pmatrix} \begin{matrix}a \\ b\end{matrix} \end{pmatrix}=0 & \text{} \\ & \Rightarrow & b=\lambda a & \text{}
\end{align}
$$

(from row 1 and it is the same from row 2, as $rk\left( A - \lambda I \right)=1$ when $\lambda$ is an eigenvalue). Thus we have a $\lambda$ -eigenvector $\begin{pmatrix} 1 \\ \lambda \end{pmatrix}$ and an eigenbasis, i.e. column vectors of

$$
\begin{align}
 & & P=\begin{pmatrix} \begin{matrix}1 & 1 \\ \left(\lambda\right)_{-} & \left(\lambda\right)_{+}\end{matrix} \end{pmatrix}. & \text{} \\ & & & \text{}
\end{align}
$$

We have $ \det P=\left(\lambda\right)_{+}-\left(\lambda\right)_{-}=\sqrt{5}\neq 0$ and so

$$
\begin{align}
P^{- 1} = \frac{1}{\sqrt{5}} \begin{pmatrix} \left(\lambda\right)_{+} & -1 \\ -\left(\lambda\right)_{-} & 1 \end{pmatrix} .
\end{align}
$$

Then

$$
\begin{align}
P^{- 1} A P = D = \begin{pmatrix} \left(\lambda\right)_{-} & 0 \\ 0 & \left(\lambda\right)_{+} \end{pmatrix} , \text{i. e. } A = P D P^{- 1} .
\end{align}
$$

Thus

$$
\begin{align}
A^{n} & = & PD^{n}P^{- 1} & \text{} \\ & = & \frac{1}{\sqrt{5}}\begin{pmatrix} \begin{matrix}1 & 1 \\ \left(\lambda\right)_{-} & \left(\lambda\right)_{+}\end{matrix} \end{pmatrix}\begin{pmatrix} \begin{matrix}\lambda _{-}^{n} & 0 \\ 0 & \lambda _{+}^{n}\end{matrix} \end{pmatrix}\begin{pmatrix} \begin{matrix}\left(\lambda\right)_{+} & -1 \\ -\left(\lambda\right)_{-} & 1\end{matrix} \end{pmatrix} & \text{}
\end{align}
$$

and

$$
\begin{align}
x_{n} & = & \left( 1 0 \right)A^{n}\begin{pmatrix} \begin{matrix}0 \\ 1\end{matrix} \end{pmatrix} & \text{} \\ & = & \frac{1}{\sqrt{5}}\left( 1 1 \right)\begin{pmatrix} \begin{matrix}\lambda _{-}^{n} & 0 \\ 0 & \lambda _{+}^{n}\end{matrix} \end{pmatrix}\begin{pmatrix} \begin{matrix}-1 \\ 1\end{matrix} \end{pmatrix} & \text{} \\ & = & \frac{1}{\sqrt{5}}\left( \lambda _{+}^{n} - \lambda _{-}^{n} \right). & \text{}
\end{align}
$$

Observe for large $n$

$$
\left| \lambda _{+}^{n} \left| \gg 1 \gg \left| \lambda _{-}^{n} \left| ,
$$

so $x_{n}\simeq\lambda _{+}^{n}$ , more precisely,

$$
\frac{x_{n + 1}}{x_{n}} \rightarrow \left(\lambda\right)_{+} \text{as  }n\rightarrow\in fty\text{} .
$$

Thus largest eigenvalue determines long-term growth of solutions.

[[next](MA10210se23.html)] [[prev](MA10210se21.html)] [[prev-tail](MA10210se21.html#tailMA10210se21.html)] [[front](MA10210se22.html)] [[up](MA10210ch5.html#MA10210se22.html)]