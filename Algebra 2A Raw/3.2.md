#### 3.2 Orthogonality

##### 3.2.1 Orthonormal bases

###### Definition

. A list of vectors $\lst {u}1k$ in an inner product space $V$ is _orthonormal_ if, for all $\bw 1{i,j}k$,

$\seteqnumber{0}{3.}{3}$

$$ \ip {u*i,u_j}=\delta *{ij}:= \begin{cases} 1&\text {if $i=j$;}\\0&\text {if $i\neq j$.} \end {cases} $$

If $\lst {u}1k$ is also a basis, we call it an _orthonormal basis_.

###### Example

. The standard basis $\lst {e}1n$ of $\F ^n$ is orthonormal for the dot product.

Orthonormal bases are very cool. Here is why: if $\lst {u}1k$ is orthonormal and $v\in \Span {\lst {u}1k}$ then we can write

$\seteqnumber{0}{3.}{3}$

$$ v=\lc \lambda {u}1k. $$

How can we compute the coordinates $\lambda _i$? In general, this amounts to solving a system of linear equations and so involves something tedious and lengthy like Gaussian elimination. However, in our case, things are _much_ easier. Observe:

$\seteqnumber{0}{3.}{3}$

$$ \ip {u*i,v}=\ip {u_i,\sum \_j\lambda \_ju_j}=\sum \_j\lambda \_j\ip {u_i,u_j} =\sum \_j\lambda \_j\delta *{ij}=\lambda \_i. $$

Thus

$\seteqnumber{0}{3.}{3}$

$$ \label {eq:11} \lambda \_i=\ip {u_i,v} $$

which is very easy to compute.

Let us enshrine this analysis into the following lemma:

- [[Lemma 3.3]]. Let $V$ be an inner product space with orthonormal basis $\lst {u}1n$ and let $v\in V$. Then

  $\seteqnumber{0}{3.}{4}$

  $$ v=\sum \_{i=1}^n\ip {u_i,v}u_i. $$

As an immediate consequence of (3.4):

- [[Lemma 3.4]]. Any orthonormal list of vectors $\lst {u}1k$ is linearly independent.

- Proof. If $\lc {\lambda }u1k=0$ then (3.4) gives $\lambda _i=\ip {u_i,0}=0$. □

What is more, these coordinates $\lambda _i$ are all you need to compute inner products.

- [[Proposition 3.5]]. Let $\lst {u}1n$ be an orthonormal basis of an inner product space $V$.

  Let $v=\lc {x}{u}1n$ and $w=\lc {y}u1n$. Then

  $\seteqnumber{0}{3.}{4}$

  $$ \ip {v,w}=\sum \_{i=1}^n\bar {x}\_iy_i=x\cdot y. $$

  Thus the inner product of two vectors is the dot product of their coordinates with respect to an orthonormal basis.

- Proof. We simply expand out $\ip {v,w}$ by sesquilinearity:

  $\seteqnumber{0}{3.}{4}$

  $$ \ip {v,w}=\ip {\sum _ix_iu_i,\sum \_jy_ju_j}=\sum _{i,j}\bar {x}_iy_j\ip {u_i,u_j} =\sum _{i,j}\bar {x}_iy_j\delta _{ij}=\sum \_i\bar {x}\_iy_i=x\cdot y. $$

  □

To put it another way:

- [[Proposition 3.6]]. Let $\lst {u}1n$ be an orthonormal basis of an inner product space $V$ and $v,w\in V$. Then:

  - (1) **Parseval’s identity**: $\ip {v,w}=\sum _{i=1}^n\ip {v,u_i}\ip {u_i,w}$.
  - (2) **Bessel’s equality**: $\norm {v}^2=\sum _{i=1}^n\abs {\ip {v,u_i}}^2$.

- Proof.
- - (1) This comes straight from [[Proposition 3.5]], using conjugate symmetry of the inner product to get $\bar {x}_i=\overline {\ip {u_i,v}}=\ip {v,u_i}$.
  - (2) Put $v=w$ in (1).

  □

All of this should make us eager to get our hands on orthonormal bases and so we would like to know if they always exist. To see that they do, we need the following construction:

- [[Theorem 3.7]] (Gram–Schmidt orthogonalisation). Let $\lst {v}1m$ be linearly independent vectors in an inner product space $V$.

  Then there is an orthonormal list $\lst {u}1m$ such that

  $\seteqnumber{0}{3.}{4}$

  $$ \Span {\lst {u}1k}=\Span {\lst {v}1k}, $$

  for all $\bw 1{k}m$, defined inductively by:

  $\seteqnumber{0}{3.}{4}$

  $$ u_k:=w_k/\norm {w_k} $$

  where

  $\seteqnumber{0}{3.}{4}$

  $$ w_1:=v_1 $$

  and, for $k>1$,

  $\seteqnumber{0}{3.}{4}$

  $$ w*k:=v_k-\sum *{j=1}^{k-1}\ip {u*j,v_k}u_j =v_k-\sum *{j=1}^{k-1}\frac {\ip {w_j,v_k}}{\norm {w_j}^2}w_j. $$

- Proof. We induct with inductive hypothesis at $k$ that $\lst {u}1k$ is orthonormal and that, for $\bw 1\ell {k}$, $\Span {\lst {u}1\ell }=\Span {\lst {v}1\ell }$.

  At $k=1$, this reads $\norm {u_1}=1$ and $\Span {u_1}=\Span {v_1}$ which is certainly true.

  Now assume the hypothesis is true at $k-1$ so that $\lst {u}1{k-1}$ is orthonormal and $\Span {\lst {u}1{k-1}}=\Span {\lst {v}1{k-1}}$. Then

  $\seteqnumber{0}{3.}{4}$

  $$ \Span {\lst {u}1k}=\Span {\lst {v}1{k-1},u_k}= \Span {\lst {v}1{k-1},w_k}=\Span {\lst {v}1k}, $$

  where the first equality comes from the induction hypothesis and the last from the definition of $w_k$. Moreover, for any $i<k$,

  $\seteqnumber{0}{3.}{4}$

  $$ \ip {u*i,w_k}=\ip {u_i,v_k}-\sum *{j<k}\ip {u*j,v_k}\ip {u_i,u_j}= \ip {u_i,v_k}-\sum *{j<k}\ip {u*j,v_k}\delta *{ij}=\ip {u_i,v_k}-\ip {u_i,v_k}=0 $$

  Thus $w_k\perp \lst {u}1{k-1}$ so that $u_k$ is also whence $\lst {u}1k$ is orthonormal. Thus the inductive hypothesis is true at $k$ and so at $m$ by induction. □

- [[Corollary 3.8]]. Any finite-dimensional inner product space $V$ has an orthonormal basis.

- Proof. Let $\lst {v}1n$ be any basis of $V$ and apply [[Theorem 3.7]] to get an orthonormal (and so linearly independent by [[Lemma 3.4]]) list $\lst {u}1n$ with

  $\seteqnumber{0}{3.}{4}$

  $$ \Span {\lst {u}1n}=\Span {\lst {v}1n}=V. $$

  Thus the $\lst {u}1n$ span also and so are an orthonormal basis. □

- Remark. For practical purposes such as computations, it is easiest to phrase the Gram–Schmidt algorithm in terms of the $w_k$: we set

  $\seteqnumber{0}{3.}{4}$

  $$ \begin{align*} w*1&:=v_1\\ w_k&:=v_k-\sum *{j=1}^{k-1}\frac {\ip {w_j,v_k}}{\norm {w_j}^{2}} w_j \end{align*} $$ (which can be computed without introducing square roots) and then finally set $u_k=w_k/\norm {w_k}$.

###### Example

. Let $U\leq \R ^3$ be given by $\set {x\in \R ^3\st x_1+x_2+x_3=0}$. Let us find an orthonormal basis for $U$.

First we need a basis of $U$ to start with: $\dim U=2$ (why?) with basis $v_1=(1,0,-1)$, $v_2=(0,1,-1)$ (of course, there are many other bases).

First, $\norm {w_1}^2=\norm {v_1}^2=2$ while $\ip {w_1,v_2}=\ip {v_1,v_2}=1$ so that

$\seteqnumber{0}{3.}{4}$

$$ \begin{align*} w_2&=v_2-\frac {\ip {w_1,v_2}}{\ip {w_1,w_1}}w_1 \\ &=(0,1,-1)-\half (1,0,-1)=(-\half ,1,-\half ). \end{align*} $$ This means that $\norm {w_2}^2={1/4+1+1/4}=3/2$ so that

$\seteqnumber{0}{3.}{4}$

$$ \begin{align*} u_1&=w_1/\sqrt {2}=(\tfrac 1{\sqrt {2}},0,-\tfrac 1{\sqrt {2}})\\ u_2&=\sqrt {\tfrac 23}w_2=\sqrt {\tfrac 23}(-\half ,1,-\half )= (-\tfrac 1{\sqrt {6}},\sqrt {\tfrac 23},\tfrac {-1}{\sqrt {6}}). \end{align*} $$

Let us conclude our discussion of orthonormal bases with an application of Gram–Schmidt which has uses in Statistics (see MA20227) and elsewhere. First, a definition:

###### Definition

. A matrix $Q\in M_{n\times n}(\R )$ is _orthogonal_ if

$\seteqnumber{0}{3.}{4}$

$$ Q^TQ=\I \_n, $$

or, equivalently, $Q$ has orthonormal columns with respect to the dot product. Here $\I _n$ is the $n\times n$ identity matrix.

- Remark. The two conditions in this definition are indeed equivalent: if $\bq _i$ is the $i$-th column of $Q$ then

  $\seteqnumber{0}{3.}{4}$

  $$ (Q^TQ)\_{ij}=\bq ^T_i\bq \_j. $$

- [[Theorem 3.9]] (QR decomposition). Let $A\in M_{n\times n}(\R )$ be an invertible matrix. Then we can write

  $\seteqnumber{0}{3.}{4}$

  $$ A=QR, $$

  where $Q$ is orthogonal and $R$ is upper triangular ($R_{ij}=0$ if $i>j$) with positive entries on the diagonal.

We apply [[Theorem 3.7]] to the columns of $A$ to get the columns of $Q$.

So let $\lst {\bv }1n$ be the columns of $A$. Since $A$ is invertible, these are a basis so we can apply [[Theorem 3.7]] to get an orthonormal basis $\lst {\bu }1n$. Let $Q$ be the orthogonal matrix whose columns are the $\bu _i$. Unravelling the formulae of the Gram–Schmidt procedure, we have

$\seteqnumber{0}{3.}{4}$

$$ \begin{align*} \bv \_1&=\norm {\bv \_1}\bu \_1 \\ \bv \_2&=\norm {\mathbf {w}\_2}\bu \_2+\ip {\bu \_1,\bv \_2}\bu \_1 \end{align*} $$ and, more generally,

$\seteqnumber{0}{3.}{4}$

$$ \bv _k=\norm {\mathbf {w}\_k}\bu \_k+\sum _{j<k}\ip {\bu \_j,\bv \_k}\bu \_j. $$

Otherwise said, $A=QR$ where $R_{kk}=\norm {\mathbf {w}_k}$, $R_{jk}=\ip {\bu _j,\bv _k}$, for $j<k$, and $R_{ij}=0$ if $i>j$.

To compute $Q$ and $R$ in practice, first do Gram–Schmidt orthogonalisation on the columns of $A$ to get $Q$ and then note that $Q^TA=Q^TQR=\I _nR=R$ so that

$\seteqnumber{0}{3.}{4}$

$$ R=Q^TA $$

which is probably easier to compute than keeping track of intermediate coefficients in the orthogonalisation!

- Remarks.
- - 1. In pure mathematics, the QR decomposition is a special case of the _Iwasawa decomposition_.
  - 2. We shall have more to say about orthogonal matrices in the next chapter, see §4.1.3.

##### 3.2.2 Orthogonal complements and orthogonal projection

###### Definition

. Let $V$ be an inner product space and $U\leq V$. The _orthogonal complement $U^{\perp }$ of $U$ (in $V$)_ is given by

$\seteqnumber{0}{3.}{4}$

$$ U^{\perp }:=\set {v\in V\st \text {$\ip {u,v}=0$, for all $u\in U$}}. $$

![(Orthogonal complements in the plane.)](M216-notes-images/image-10.svg)

Figure 3.3: Orthogonal complements in $\R ^2$

- [[Proposition 3.10]]. Let $V$ be an inner product space and $U\leq V$. Then

  - (1) $U^{\perp }\leq V$;
  - (2) $U\cap U^{\perp }=\set 0$;
  - (3) $U\leq (U^{\perp })^{\perp }$.

- Proof.

  - (1) This is a straightforward exercise using the second slot linearity of the inner product.
  - (2) If $u\in U\cap U^{\perp }$, $\ip {u,u}=0$ so that $u=0$ by positive-definiteness of the inner product.
  - (3) If $u\in U$ and $w\in U^{\perp }$ then

    $\seteqnumber{0}{3.}{4}$

    $$ \ip {w,u}=\overline {\ip {u,w}}=0 $$

    so that $u\in (U^{\perp })^{\perp }$.

  □

If $U$ is finite-dimensional then $U^{\perp }$ is a complement to $U$ in the sense of §2.2 (even if $V$ is infinite-dimensional!):

- [[Theorem 3.11]]. Let $U$ be a finite-dimensional subspace of an inner product space $V$. Then $V$ is an internal direct sum:

  $\seteqnumber{0}{3.}{4}$

  $$ V=U\oplus U^{\perp }. $$

- Proof.
- By [[Proposition 2.2]] and [[Proposition 3.10]](2), we just need to prove that $V=U+U^{\perp }$. For this, let $\lst {u}1k$ be an orthonormal basis of $U$ and let $v\in V$. We write

  $\seteqnumber{0}{3.}{4}$

  $$ v=\bigl (\sum _{i=1}^k\ip {u_i,v}u_i\bigr )+\bigl (v-\sum _{i=1}^k\ip {u_i,v}u_i\bigr ) =:v_1+v_2. $$

  Now $v_1\in U$ being in the span of the $u_i$ while, for $\bw 1jk$,

  $\seteqnumber{0}{3.}{4}$

  $$ \begin{align*} \ip {u*j,v_2} & =\ip {u_j,v}-\sum *{i=1}^k\ip {u_i,v}\ip {u_j,u_i} \\ & =\ip {u_j,v}-\ip {u_j,v}=0. \end{align*} $$ Thus, for $u=\lc {\lambda }u1k\in U$,

  $\seteqnumber{0}{3.}{4}$

  $$ \ip {u,v*2}=\sum *{j=1}^k\bar {\lambda }\_j\ip {u_j,v_2}=0 $$

  so that $v_2\in U^{\perp }$. □

- [[Corollary 3.12]]. Let $V$ be a finite-dimensional inner product space and $U\leq V$. Then

  - (1) $\dim U^{\perp }=\dim V-\dim U$.
  - (2) $U=(U^{\perp })^{\perp }$.

- Proof.
- - (1) This is immediate from [[Proposition 2.5]].
  - (2) We give two proofs of this.

    - • Two applications of (1) give

      $\seteqnumber{0}{3.}{4}$

      $$ \dim (U^{\perp })^{\perp }=\dim V-\dim U^{\perp }=\dim U $$

      while [[Proposition 3.10]](3) gives $U\leq (U^{\perp })^{\perp }$. We conclude that we have equality by [[Lemma 1.4]].

    - • Alternatively, let $v\in (U^{\perp })^{\perp }$ and write $v=v_1+v_2$ with $v_1\in U$ and $v_2\in U^{\perp }$. Then

      $\seteqnumber{0}{3.}{4}$

      $$ 0=\ip {v_2,v}=\ip {v_2,v_1}+\ip {v_2,v_2}=\norm {v_2}^2 $$

      so that $v_2=0$ giving $v=v_1\in U$. Thus $(U^{\perp })^{\perp }\leq U$ which, with [[Proposition 3.10]](3) yields $U=(U^{\perp })^{\perp }$. This argument works when $V$, and even $U$, are infinite-dimensional so long as $V=U\oplus U^{\perp }$.

  □

- Remark. Both [[Theorem 3.11]] and [[Corollary 3.12]](2) can fail when $U$ is infinite-dimensional.

When $V=U\oplus U^{\perp }$, we can apply the results of §2.2.1 to get projections onto $U$ and $U^{\perp }$.

###### Definition

. Let $V$ be an inner product space and $U\leq V$ such that $V=U\oplus U^{\perp }$. The projection $\pi _U:V\to V$ with image $U$ and kernel $U^{\perp }$ is called the _orthogonal projection onto $U$_.

- Remark. $\pi _{U^{\perp }}=\id _V-\pi _U$. The situation is illustrated in Figure 3.4.

![(Two orthogonal projections.)](M216-notes-images/image-11.svg)

Figure 3.4: Orthogonal projections

- [[Lemma 3.13]]. Let $V$ be an inner product space and $U\leq V$ a finite-dimensional subspace with orthonormal basis $\lst {u}1k$ then, for all $v\in V$,

  $\seteqnumber{0}{3.}{4}$

  $$ \pi _U(v)=\sum _{i=1}^k\ip {u_i,v}u_i. $$

- Proof. This is the proof of [[Theorem 3.11]]. □

Let us conclude this chapter with an application to a minimisation problem that, among other things, underlies much of Fourier analysis (see MA20223).

- [[Theorem 3.14]]. Let $V$ be an inner product space and $U\leq V$ such that $V=U\oplus U^{\perp }$.

  For $v\in V$, $\pi _U(v)$ is the nearest point of $U$ to $v$: for all $u\in U$,

  $\seteqnumber{0}{3.}{4}$

  $$ \norm {v-\pi \_U(v)}\leq \norm {v-u}. $$

![(Orthogonal projection minimises distance.)](M216-notes-images/image-12.svg)

Figure 3.5: The orthogonal projection minimises distance to $U$

- Proof. As we see in Figure 3.5, this is just the Pythagoras theorem ([[Proposition 3.2]]). Indeed, for $u\in U$, note that $\pi _U(v)-u\in U$ while $v-\pi _U(v)=\pi _{U^{\perp }}(v)\in U^{\perp }$. Thus,

  $\seteqnumber{0}{3.}{4}$

  $$ \begin{align*} \norm {v-u}^2&=\norm {v-\pi \_U(v)+\pi \_U(v)-u}^2= \norm {v-\pi \_U(v)}^2+\norm {\pi \_U(v)-u}^2\\ &\geq \norm {v-\pi \_U(v)}^2. \end{align*} $$ Now take square roots! □

- Exercise. Read [[Example 6.58]] on pages 199–200 of Axler’s Linear Algebra Done Right to see a beautiful application of this result. He takes $V=C^0[-\pi ,\pi ]$ and $U$ to be the space of polynomials of degree at most $5$ to get an astonishingly accurate polynomial approximation to $\sin$.
