#### 6.3 Application: Quadratic forms

###### Convention

. We continue working with a field $\F$ where $1+1\neq 0$.

We can construct a function on $V$ from a bilinear form $B$ (which is a function on $V\times V$).

###### Definition

. A _quadratic form_ on a vector space $V$ over $\F$ is a function $Q:V\to \F$ of the form

$\seteqnumber{0}{6.}{0}$

$$ Q(v)=B(v,v), $$

for all $v\in V$, where $B:V\times V\to \F$ is a symmetric bilinear form.

- Remark. For $v\in V$ and $\lambda \in \F$, $Q(\lambda v)=B(\lambda v,\lambda v)=\lambda ^2Q(v)$ so $Q$ is emphatically not a linear function!

###### Examples

. Here are two quadratic forms on $\F ^3$:

- 1. $Q(x)=x_1^2+x_2^2-x_3^2=B_A(x,x)$ where

  $\seteqnumber{0}{6.}{0}$

  $$ A= \begin{pmatrix} 1&0&0\\0&1&0\\0&0&-1 \end {pmatrix}. $$

- 2. $Q(x)=x_1x_2=B_A(x,x)$ where

  $\seteqnumber{0}{6.}{0}$

  $$ A= \begin{pmatrix} 0&\half &0\\\half &0&0\\0&0&0 \end {pmatrix}. $$

We can recover the symmetric bilinear form $B$ from its quadratic form $Q$:

- [[Lemma 6.8]]. Let $Q:V\to \F$ be a quadratic form with $Q(v)=B(v,v)$ for a symmetric bilinear form $B$. Then

  $\seteqnumber{0}{6.}{0}$

  $$ B(v,w)=\half \bigl (Q(v+w)-Q(v)-Q(w)\bigr ), $$

  for all $v,w\in V$.

  $B$ is called the _polarisation of $Q$_.

- Proof. Expand out to get

  $\seteqnumber{0}{6.}{0}$

  $$ Q(v+w)-Q(v)-Q(w)=B(v,w)+B(w,v)=2B(v,w). $$

  □

Here is how to do polarisation in practice: any quadratic form $Q:\F ^n\to \F$ is of the form

$\seteqnumber{0}{6.}{0}$

$$ Q(x)=\sum _{1\leq i\leq j\leq n}q_{ij}x*ix_j=\bx ^{T} \begin{pmatrix} q*{11}&&\half q*{ji}\\&\ddots &\\\half q*{ij}&&q\_{nn} \end {pmatrix}\bx $$

so that the polarisation is $B_A$ where

$\seteqnumber{0}{6.}{0}$

$$ A*{ij}=A*{ji}= \begin{cases} q*{ii}&\text {if $i=j$;}\\ \half q*{ij}&\text {if $i<j$}. \end {cases} $$

###### Example

. Let $Q:\R ^3\to \R$ be given by

$\seteqnumber{0}{6.}{0}$

$$ Q(x)=x_1^2+2x_2^2+2x_1x_2+x_1x_3. $$

Let us find the polarisation $B$ of $Q$, that is, we find $A$ so that $B=B_A$: we have $q_{11}=1$. $q_{22}=2$, $q_{12}=2$ and $q_{13}=1$ with all other $q_{ij}$ vanishing so

$\seteqnumber{0}{6.}{0}$

$$ A= \begin{pmatrix} 1&1&\half \\1&2&0\\\half &0&0 \end {pmatrix}. $$

###### Definitions

. Let $Q$ be a quadratic form on a finite-dimensional vector space $V$ over $\F$.

The _rank_ of $Q$ is the rank of its polarisation.

If $\F =\R$, the _signature_ of $Q$ is the signature of its polarisation.

What does the diagonalisation theorem mean for a quadratic form $Q$? Observe:

- • Any $\alpha \in V^{*}$ can be squared to give a quadratic form: $\alpha ^2:V\to \F$ given by $\alpha ^2(v)=\alpha (v)^2$. Note that this is indeed a quadratic form with polarisation $B(v,w)=\alpha (v)\alpha (w)$.
- • If $\lst {v}1n$ diagonalises the polarisation $B$ of $Q$ then $Q(\sum _i\lambda _iv_i)=\sum _iB(v_i,v_i)\lambda _i^2$ so that

  $\seteqnumber{0}{6.}{0}$

  $$ Q=\sum \_{i=1}^nQ(v_i)(v_i^{\*})^2. $$

  That is, we have written $Q$ as a linear combination of $n$ _linearly independent_ squares.

Let us now apply the classification results of §6.2 and summarise the situation for quadratic forms on vector spaces over our favourite fields:

- [[Theorem 6.9]]. Let $Q$ be a quadratic form with rank $r$ polarisation on a finite-dimensional vector space over $\F$.

  - (1) When $\F =\C$, there is a basis $\lst {v}1n$ of $V$ such that

    $\seteqnumber{0}{6.}{0}$

    $$ Q(\sum \_{i=1}^nx_iv_i)=\plst {x^2}1r. $$

  - (2) When $\F =\R$ and $Q$ has signature $(p,q)$, there is a basis $\lst {v}1n$ of $V$ such that

    $\seteqnumber{0}{6.}{0}$

    $$ Q(\sum _{i=1}^nx_iv_i)=\plst {x^2}1p-x_{p+1}^2-\dots -x_r^2. $$

###### Example

. Find the signature of $Q:\R ^3\to \R$ given by

$\seteqnumber{0}{6.}{0}$

$$ Q(x)=x_1^2+x_2^2+x_3^2+2x_1x_3+4x_2x_3. $$

$Q$ has polarisation $B=B_A$ with

$\seteqnumber{0}{6.}{0}$

$$ A= \begin{pmatrix} 1&0&1\\0&1&2\\1&2&1 \end {pmatrix}. $$

Solution: exploit the zero in the $(1,2)$-slot of $A$ to see that $e_1,e_2,y=(-1,-2,1)$ is a diagonalising basis and so gives us a diagonal matrix representing $B$ with $Q(e_1)=Q(e_2)=1>0$ and $Q(y)=-4<0$ along the diagonal. So the signature is $(2,1)$.

Here are two alternative techniques:

- 1. Orthogonal diagonalisation yields a diagonal matrix representing $B$ with the eigenvalues of $A$ down the diagonal so we just count how many positive and negative eigenvalues there are.

  In fact, $A$ has eigenvalues $1$ and $1\pm \sqrt {5}$. Since $\sqrt {5}>2$, $1-\sqrt {5}<0$ and we again conclude that the signature is $(2,1)$.

  **Danger**: this method needed us to solve a cubic equation which is already difficult. For an $n\times n$ $A$ with $n\geq 5$, this could be impossible!

- 2. Finally, we could try and write $Q$ as a linear combination of linearly independent squares and then count the number of positive and negative coefficients. In fact,

  $\seteqnumber{0}{6.}{0}$

  $$ \begin{align*} Q(x)&=x_1^2+x_2^2+x_3^2+2x_1x_3+4x_2x_3\\ &=(x_1+x_3)^2+x_2^2+4x_2x_3=(x_1+x_3)^2+(x_2+2x_3)^2-4x_3^2. \end{align*} $$ But now we need to check that $x_1+x_3, x_2+2x_3,x_3$ are linearly independent linear functionals on $\R ^3$. Here [[Corollary 5.8]] comes to the rescue and says we only need show that $(\ker x_1+x_2)\cap (\ker x_2+2x_3)\cap (\ker x_3)=\set {0}$. But $x_3=0=x_1+x_3=x_2+2x_3$ rapidly implies that each $x_i=0$ and we are done. The coefficients of these squares are $1,1,-4$ and so, once more, we get that the signature is $(2,1)$.
