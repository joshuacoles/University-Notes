# The Calculus of Probabilities

We can use [[1.4 Kolmogorov's Axioms of Probability]] to build up properties of a probability measure that will be useful when calculating complicated probabilities.

## Theorem 2 (The probability of the empty set is zero)

For any [[1.4 Kolmogorov's Axioms of Probability|Probability Space]] $(\Omega, \calF, \P)$,

$$
\P(\varnothing) = 0.
$$

### Proof

Consider the infinite collection of disjoint sets,

$$
E = (\Omega, \emptyset, \dots)
$$

by considering their union and applying [[1.4 Kolmogorov's Axioms of Probability#^ka3|Kolmogorov's third axiom]] we get,

$$
\begin{align}
\P(\Omega) &= \P\(\bigcup_{i = 0}^{\infty} E_i\)
\\

&= \sum_{i = 0}^{\infty} \P(E_i)
\tag{Kolmogorov Axiom 3}\\

&= \P(\Omega) + \sum_{i = 1}^{\infty} \P(\emptyset) \\

0 &= \sum_{i = 1}^{\infty} \P(\emptyset) \\
0 &= \P(\emptyset)
\end{align}
$$

hence proving the conjecture.

## Theorem 3 (**Finite** additivity of $\P$ for disjoint events)

For any $(\Omega, \calF, \P)$, if $\lst E1n \in \calF$ are disjoint then

$$
\P\(\bigcup_{i = 0}^n E_i\) = \sum_{i = 1}^n \P(E_i)
$$

### Proof

Consider the extension of the finite list $\lst E1n$ to,

$$
H_i = \begin{cases}
E_i & \text{if}~ i \le n \\
\emptyset & \text{otherwise}
\end{cases}
$$

since $A \cap \emptyset = \emptyset$ for all $A$ then the collection remains disjoint and further since $\emptyset$ is the identity of union their union remains the same,

$$
\bigcup_{i = 0}^n E_i = \bigcup_{i = 0}^\infty H_i
$$

hence by considering in the same manner as before,

$$
\begin{align}


\P\(\bigcup_{i = 0}^\infty H_i\) &=
\sum_{i = 1}^\infty \P(H_i)
\tag{Kolmogorov Axiom 3}
\\


&=
\sum_{i = 1}^n \P(H_i) + \sum_{i = n + 1}^\infty \P(H_i)
\\


&=
\sum_{i = 1}^n \P(E_i) + \sum_{i = n + 1}^\infty \P(\emptyset)
\\


&=
\sum_{i = 1}^n \P(E_i) + 0
\tag{By $\P(\emptyset) = 0$ from above}
\\


&= \sum_{i = 1}^n \P(E_i) \\
&= \P\(\bigcup_{i = 0}^n E_i\) \tag{Kolmogorov Axiom 3}
\end{align}
$$

hence showing us we can apply Axiom 3 over finite sets.

## Corollary 1 (Probability of complements)

For any $(\Omega, \calF, \P)$ and $E \in \calF$

$$
\P(E^c) = 1 - \P(E)
$$

### Proof

From [[1.3 Event Space#^sigma-2|the closure of an Event Space under complement]] we know $\E \in \calF \implies E^c \in \calF$ and hence when considering their disjoint union, $E \cup E^c = \Omega$ and applying [[1.4 Kolmogorov's Axioms of Probability#^ka2|(A2)]] and [[1.4 Kolmogorov's Axioms of Probability#^ka3|(A3)]] we obtain,

$$
\begin{align}
\P(E \cup E^c) &= \P(\Omega) \\
\P(E) + \P(E^c) &= 1 \\
\P(E^c) &= 1 - \P(E) \\
\end{align}
$$

## Corollary 2 (Probabilities are between zero and one)

For any $(\Omega, \calF, \P)$ and $E \in \calF$

$$
0 \le \P(E) \le 1
$$

and thus $\P : \calF \to [0, 1]$.

### Proof

From [[1.4 Kolmogorov's Axioms of Probability#^ka1|(A1)]] we know that $\P(E) \ge 0 \land \P(E^c) \ge 0$ which applying the latter to [[#Corollary 1 Probability of complements]] discussed above we obtain,

$$
1 - \P(E) \ge 0 \implies \P(E) \le 0
$$

as needed.

## Corollary 3 (Partition rule)

For any $(\Omega, \calF, \P)$ and $E, F \in \calF$

$$
\P(F) = \P(F \cap E) + \P(F \cap E^c).
$$

### Proof

#Exercise


## Corollary 4 (Inclusion-exclusion rule)

For any $(\Omega, \calF, \P)$ and $E, F \in \calF$

$$
\P(E \cup F) = \P(E) + \P(F) - \P(E \cap F).
$$

### Proof

#Exercise

## Corollary 5 (Containment rule)

For any $(\Omega, \calF, \P)$ and $E \sub F \in \calF$

$$
\P(F) = \P(E) + \P(F \cap E^c)
$$
^c5s1

so that

$$
\P(F) \geq \P(E).
$$
^c5s2

### Proof

As $E \subset F$ then $F \cap E = E$ substituting this into [[#Corollary 3 Partition rule]] gives us the [[#^c5s1|first statement]]  and noting that $P(F \cap E^c) \ge 0$ by [[1.4 Kolmogorov's Axioms of Probability#^ka1|(A1)]] gives us the [[#^c5s2|second]]. This is shown graphically below.

![[MA10211_all-9.png]]

## Theorem 4 (Boole's inequalities: the union and intersection bounds)

For any $(\Omega, \calF, \P)$ and events $\lst E1n \in\calF$,

$$
\P\(\bigcup_{i = 1}^n E_i\) \le \sum_{i = 1}^n\P(E_i),

\qquad

\P\(\bigcap_{i = 1}^n E_i\) \ge 1 - \sum_{i = 1}^n \P(E_i^c).
$$

These results also hold in the infinite case.

### Proof

We use proof by induction to prove the finite case. Let $P(k)$ be the statement that

$$
P(k): \quad \P\(\bigcup_{i = 1}^k E_i\) \le \sum_{i = 1}^k \P(E_i).
$$

The case $P(1)$ is trivially true with $\P(E_1) \leq \P(E_1)$.

Inducting, we assume $P(k)$ is true then by [[#Corollary 4 Inclusion-exclusion rule]] and [[#Corollary 5 Containment rule]],

$$
\begin{align}
\P\(\bigcup_{i = 1}^{k + 1} E_i\)
&=
\P\(\bigcup_{i = 1}^k E_i\)
+ \P(E_{k + 1})
- \P\(\(\bigcup_{i = 1}^k E_i\) \cap E_{k+1}\)
\\

&\le
\P\(\bigcup_{i = 1}^k E_i\)
+ \P(E_{k + 1})
\tag{By (A1), $\P(E) \ge 0$}
\\

&\le
\sum_{i = 1}^k \P(E_i)
+ \P(E_{k + 1})
\tag{By IH: $P(k)$}
\\

&\le
\sum_{i = 1}^{k + 1} \P(E_i)
\end{align}
$$

Hence proving $P(k + 1) \Longleftarrow P(k)$ and hence by induction, $\Forall k \in \N : P(k)$.

For the second form we can apply De Morgan's Laws for finite collections of events and obtain,

$$
\begin{align}
\P\(\bigcup_{i = 1}^n E_i\)
&= \P\(\(\bigcup_{i = 1}^n E_i^c\)^c\) \\
&= 1 - \P\(\bigcup_{i = 1}^n E_i^c\) \\
&â‰¥ 1 - \sum_{i = 1}^n \P\(E_i^c\)
\end{align}
$$

as needed.
