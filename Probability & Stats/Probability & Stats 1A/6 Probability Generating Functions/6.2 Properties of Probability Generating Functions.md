# 6.2 Properties of Probability Generating Functions

The $k$th moment of a random variable $X$ is $𝔼(X^{k} )$.
Typically we are interested in the first two moments,
$𝔼(X)$ and $𝔼(X^{2} )$ as from these we can
deduce the expectation and variance of $X$. We can exploit the structure
of $G_{X}$ to find these.

Theorem 28 (Generating the $k$th factorial moment)
Let $X$ be a random variable with probability generating function
$G_{X}(s )$. Then

(a) 

$G_{X}(1 )=1$.

(b) 

The $k$th factorial moment
$𝔼(X (X - 1) \dots (X - k + 1) )$
is given by

$$
\begin{align}
𝔼(X (X - 1) \dots  (X - k + 1))=G_{X}^{(k)}(1)=((\frac{d^{k}}{d s^{k}} G_{X} (s)|)_{s = 1}. & & & text{}
\end{align}
$$

Proof:

(a) 

$$
\begin{align}
G_{X}(1)=\sum_{x = 0}^{\infty}1^{x}ℙ(X = x)=\sum_{x = 0}^{\infty}ℙ(X = x)=1. & & & text{}
\end{align}
$$

(b) 

It can be shown that if $s\in[ 0, 1 right]$ we can exchange the
differential and the summation so that

$$
\begin{align}
\frac{d^{k}}{d s^{k}}G_{X}(s) & = & \sum_{x = 0}^{\infty}(\frac{d^{k}}{d s^{k}} s^{x})ℙ(X = x) & \text{} \\ & = & \sum_{x = k}^{\infty}x(x - 1)\dots (x - k + 1)s^{x - k}ℙ(X = x). & \text{(6.1)}\text{}text{}
\end{align}
$$

Evaluating at $s=1$ gives

$$
\begin{align}
G_{X}^{(k)}(1) & = & \sum_{x = k}^{\infty}x(x - 1)\dots (x - k + 1)ℙ(X = x) & \text{} \\ & = & \sum_{x = 0}^{\infty}x(x - 1)\dots (x - k + 1)ℙ(X = x)=𝔼(X (X - 1) \dots  (X - k + 1)) & text{}
\end{align}
$$

since the first $k$ terms are zero.

$square$

To generate $𝔼(X)$ and $𝔼(X^{2} )$ we use the
cases $k=1$ and $k=2$ of Theorem [28](#x34-5900128)(b) since

$$
\begin{align}
𝔼(X) & = & ((\frac{d}{d s} G_{X} (s)|)_{s = 1} & \text{} \\ 𝔼(X (X - 1)) & = & ((\frac{d^{2}}{d s^{2}} G_{X} (s)|)_{s = 1} & text{}
\end{align}
$$

so that
$𝔼(X^{2})=𝔼(X (X - 1))+𝔼(X )$.

Example 63 Suppose that $XsimGeom(p )$. Use
$G_{X}(s)$ to find $𝔼(X )$ and
$Var(X )$.

From Example [62](nose20.htm#x33-5800562),
$G_{X}(s)=\frac{s p}{1 - s (1 - p )}$ so that

$$
\begin{align}
\frac{d}{d s}G_{X}(s) & = & \frac{p (1 - s (1 - p)) + p s (1 - p)}{((1 - s (1 - p)))^{2}}=\frac{p}{((1 - s (1 - p)))^{2}}, & \text{} \\ \frac{d^{2}}{d s^{2}}G_{X}(s) & = & \frac{2 p (1 - p)}{((1 - s (1 - p)))^{3}}. & text{}
\end{align}
$$

Hence,

$$
\begin{align}
𝔼(X) & = & ((\frac{d}{d s} G_{X} (s)|)_{s = 1}=\frac{p}{((1 - (1 - p)))^{2}}=\frac{1}{p}, & \text{} \\ 𝔼(X (X - 1)) & = & ((\frac{d^{2}}{d s^{2}} G_{X} (s)|)_{s = 1}=\frac{2 p (1 - p)}{((1 - (1 - p)))^{3}}=\frac{2 (1 - p)}{p^{2}}. & text{}
\end{align}
$$

Thus,

$$
\begin{align}
𝔼(X^{2}) & = & \frac{2 (1 - p)}{p^{2}}+\frac{1}{p}=\frac{2 - p}{p^{2}} & text{}
\end{align}
$$

so that

$$
\begin{align}
Var(X) & = & \frac{2 - p}{p^{2}}-((\frac{1}{p}))^{2}=\frac{1 - p}{p^{2}}. & text{}
\end{align}
$$

Notice that these answers agree with those obtained directly: see
Examples [49](nose17.htm#x29-4900949) and [53](nose18.htm#x30-5201153).

\


