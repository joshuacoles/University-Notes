# 3.4 Bayes' theorem

Suppose that $\P(E)>0$ and $\P(F)>0$ then using
the multiplication law,

$$
\begin{align}
\P(E \cap F)=\P(E | F)\P(F)=\P(F | E)\P(E) 
\end{align}
$$

so that

$$
\begin{align}
\P(E | F) &= \frac{\P (F | E) \P (E)}{\P (F)} \text{(3.9)}
\end{align}
$$

which gives a formula for reversing the conditioning. In many cases,
$\P(F | E)$ might be straightforward to obtain but we
are interested in $\P(E | F)$. For example, as a
patient visiting the doctor we are interested in
$\P(\text{disease} | \text{symptoms})$ whilst medical
evidence typically knows
$\P(\text{symptoms} | \text{disease})$.

Equation ([3.9](#x20-32002r3.9)) is often called Bayes'
theorem[1](#fn1x3) but there is a more general form which applies to
partitions of $Omega$.

## Theorem 1 (Bayes' theorem)

If $\set{ E_{1}, … ⁡, E_{n} }$ form a partition of $Omega$
and $\P(E_{i})>0$ for all $i$ then, for any $j=1,…⁡,n$,

$$
\begin{align}
\P(E_{j} | F) &= \frac{\P (F | E_{j}) \P (E_{j})}{\sum_{i = 1}^{n} \P (F | E_{i}) \P (E_{i})}. \text{(3.10)}
\end{align}
$$

### Proof

Equation ([3.10](#x20-32005r3.10)) follows from equation
([3.9](#x20-32002r3.9)) by taking $E=E_{j}$ and then using the law of
total probability, Theorem [10](nose9.htm#x19-3100710), for
$\P(F)$. $square$

Notes:

1.

Recall that for any event $E$, $\set{ E, E^{c} }$ forms a
partition so that for any event $E$,

$$
\begin{align}
\P(E | F) &= \frac{\P (F | E) \P (E)}{\P (F | E) \P (E) + \P (F | E^{c}) \P (E^{c})}. 
\end{align}
$$

2.

As with the law of total probability, the result can be extended to the
case where $E_{1},E_{2},…⁡$ form a partition and also that where
$E_{1},…⁡,E_{n}$ are disjoint with
$F\subset(\cup ⁡)_{i = 1}^{n}E_{i}$.

## Example 3 We revisit Example [29](nose9.htm#x19-3101529). Suppose

$1/4$, $1/8$ and $1/16$ of Buster, Rich and Owen's own deals made a
loss, respectively. What is the probability a randomly chosen deal that
made a loss is one of Buster's?

Let $L$ be the event that the deal made a loss. Then we know

$$
\begin{align}
\P(L | B) &= \frac{1}{4},\P(B)=\frac{3}{1 0}\text{(Buster)}  \\ \P(L | R) &= \frac{1}{8},\P(R)=\frac{5}{1 0}\text{(Rich)}  \\ \P(L | W) &= \frac{1}{1 6},\P(W)=\frac{2}{1 0}\text{(Owen)} 
\end{align}
$$

and, so by Bayes' Theorem

$$
\begin{align}
\P(B | L) &= \frac{\P (L | B) \P (B)}{\P (L | B) \P (B) + \P (L | R) \P (R) + \P (L | W) \P (W)}  \\  \\ &= \frac{(\frac{1}{4} \times \frac{3}{1 0})}{(\frac{1}{4} \times \frac{3}{1 0}) + (\frac{1}{8} \times \frac{5}{1 0}) + (\frac{1}{1 6} \times \frac{2}{1 0})}  \\  \\ &= \frac{6}{6 + 5 + 1}=\frac{1}{2}. 
\end{align}
$$

[1](#fn1x3-bk)[Thomas Bayes
(1701-1761)](https://en.wikipedia.org/wiki/Thomas_Bayes).

\
