# Continuity of Partial Derivatives in a Open Region => Differentiability within Region

Let $U \sub \R^n$ be an [[1.1 Analytic Space]] and $f : U \to \R^m$ a function. 

If there exists an [[Open Set]] $V \sub U$ such that:

- $x \in V$
	- Note $\set{x}$ is **not open**.
- The [[1.5 Jacobian Matrix|Jacobian]] $Jf(p)$ is fully defined across the whole of $p \in V$.
- All [[1.2 Partial Derivative|Partial Derivatives]] $\partial_if_j$ are [[Continuous]] at $x$[^1].

Then $f$ is [[2.1 Derivative|Differentiable]] at $x$.

[^1]: Question time! Is this equivalent to saying that the Jacobian exists and $J : f \mapsto Jf$ is a continuous function with respect to some matrix norm? #Further-Reading 

## Proof

Assume the premise that $V \sub U$ is open, containing $x$ and all partial derivatives $\partial_if_j$ are continuous in $V$.

Let,
- $r > 0$ such that $B_{2r}(x) \sub V$ (thus inheriting all properties define in premise).

> #Question Why do we choose $2r$ here?
- $v \in B_r(0)$.

First defining some notation, given the vector $v \in \R_N$ we define $v_{i>j}$ for some $j \in J_n = \set{1, \dots, n}$ to be,

$$
v_{i>j} = \set{0, \dots, v_{j+1}, v_{j+2}, \dots, v_{n}},
$$

thus for a given component in $f$, $l \in J_m$ we express the difference of $f_l(x + v) - f_l(x)$ as,

$$
\begin{align}
f_l(x + v) - f_l(x)
&= f_l(x + v) 				&&- f_l(x + v_{i>1}) \\
&+ f_l(x + v_{i>1}) 		&&- f_l(x + v_{i>2}) \\
&+ f_l(x + v_{i>2}) 		&&- \cdots \\
&+ \cdots 					&&- f_l(x + v_{i > n - 1}) \\
&+ f_l(x + v_{i> n - 1})	&&- f_l(x + 0). \\
\end{align}
$$

^a0f6d3

> Noting diagonal terms cancel and that both $v = v_{i > 0}$ and $0 =v_{i>n}$, completing the pattern.

Further for a given $j \in J_n$ we can define the restriction of a term of this form to a line parallel to $e_j$ (in the same vein as that seen in the definition of the partial derivative).

$$
\begin{gather}
g_{lj} : [-r, r] \to \R \\
g_{lj}(t) = f_l(x + v_{i > j} + t e_j).
\end{gather}
$$

> Remembering that $v_{i>j} = \set{0, \dots, v_{j+1}, \dots, v_n}$ and hence the $j$'th component of the term $x + v_{i > j} + t e_j$ is only $x_j + t$.

> #Question why restraint on $t$ to $r$ not $2r$ as it is continuous even in that region.
> Is it so that the maximum magnitude of the total delta $\delta = v_{i > j} + t e_j$ is bound to be $\|\delta\|^2 = \|v_{i > j}\|^2 + t^2$ (note that $v_{i>j} \perp e_j$ by definition hence we can apply [[Pythagoras' Theorem]]) and hence, $\|\delta\|^2 = \|v_{i > j}\|^2 + t^2 < r^2 + r^2 = 2r^2 \implies \|\delta\| < \sqrt2r$. 
> $2r$ is still a stronger condition than is needed but it at least makes sense now.

Computing the derivative of which gives us,

$$
\dd{g_{lj}}{t} = \parfrac{f_l}{x_j}(x + v_{i > j} + te_j)
$$

Since by the premise we know that $\parfrac{f_l}{x_j}$ is continuous at $x + v_{i > j} + te_j$ by the note on the magnitude above, we can apply the [[Mean Value Theorem]],

$$
\Exists s_{lj} \in [0, v_j] : g_{lj}(v_j) - g_{lj}(0) = v_j g_{lj}'(s_{lj}),
$$

which applying our definition of $g$ and $g'$ from above gives us,

$$
\begin{align}
g_{lj}(v_j) - g_{lj}(0)
&= f_l(x + v_{i > j} + v_j \dp e_j) - f_l(x + v_{i > j} + 0 \dp e_j) \\
&= f_l(x + v_{i > j - 1}) - f_l(x - v_{i > j}) \\

v_j g_{lj}'(s_{lj}) &= f_l(x + v_{i > j - 1}) - f_l(x - v_{i > j}) \\\\
v_j \parfrac{f_l}{x_j}(x + v_{i > j} + s_j e_j) &= f_l(x + v_{i > j - 1}) - f_l(x - v_{i > j})\\\\
\end{align}
$$

the RHS of which we can see is the repeating pattern seen in the original re-expression of the $f_l$ difference ([[#^a0f6d3|eq]]). Hence we can substitute this to obtain,

$$
f_l(x + v) - f_l(x) = \sum v_j \parfrac{f_l}{x_j}(x + v_{i > j} + s_j e_j)
$$

^16a31c

Now, using our infinite wisdom of someone having done the problem before, we construct the number,

> This is actually what makes proofs difficult. Simplifying, is easy. Building up complexity is a fumbling around in the darkness.

$$
h_{lj} = \parfrac{f_l}{x_j}(x + v_{i > j} + s_j e_j) - \parfrac{f_l}{x_j}(x),
$$

expressing the [[#^16a31c|previous equation for the difference]] as,

$$
f_l(x + v) - f_l(x) = \sum_j v_j h_{lj} +\sum_j v_j \parfrac{f_l}{x_j}(x).
$$

of which we can see the rightmost term is equal to the $l$'th component of $Jf(x)$.

If we then take $v \to 0 \implies v_j \to 0$ this will lead to $s_j \to 0 \impliedby s_j \in [0, v_j]$ which by the continuity of the partial derivatives as per the premise, implies that $h_{lj} \to 0$ as the two terms become equal. This combined with the obvious vanishing of the first term on the RHS gives us also that the LHS goes to zero by their equality[^2]

[^2]: Note we cannot justify this by continuity in $f_l$ as I do not believe that we can assume it, we may however have just proved it.

On the other hand the quantities $\dfrac{v_j}{\norm{v}}$ will remain bounded. So re-arranging slightly (for a given $l$) we get,

$$
\lim_{v \to 0}~\frac{
	f_l(x + v) - f_l(x) - (Jf(x))_l
}{\norm{v_j}}
=
\lim_{v \to 0}~ \sum \frac{v_j}{\norm{v_j}} h_{lj}.
$$

which we see vanishes to zero on *both sides*. Further since each component vanishes to zero, we can take that the their so does the vector they form[^3] giving us the final result of:

[^3]: Proof of $x_i \to 0 \implies x \to 0$ #todo.

$$
\lim_{v \to 0}~\frac{
	f(x + v) - f(x) - Jf(x)
}{\norm{v_j}} = 0. \qquad\square
$$
