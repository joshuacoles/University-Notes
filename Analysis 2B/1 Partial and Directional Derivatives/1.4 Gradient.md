# Gradient

Let $U \subseteq \mathbb{R}^n$ be an [[1.1 Analytic Space]] $f : U \to \R$ a function that has [[1.2 Partial Derivative|Partial Derivatives]] at a point $x \in U$. Then the vector 

$$
\nab f(x) = \begin{pmatrix}
	\parfrac{f}{x_1}(x) \\
	\vdots \\
	\parfrac{f}{x_n}(x)
\end{pmatrix}
$$ 

is called the **Gradient** of $f$ at $x$. This creates a *vector-valued* function $\nab f$ given by,

$$
(f : U \to R) \mapsto (\nab f : (U \sub \R^n) \to \R^n).
$$

^7d9bab

## Example

For the function given in [[1.2 Partial Derivative#Example]],

![[1.2 Partial Derivative#^example]]

we have the gradient

$$
\nab f(x) = \begin{pmatrix}
	2x_1 x_2 \\
	x_1^2 \\
	1
\end{pmatrix}.
$$

## Relation to [[1.5 Jacobian Matrix|Jacobian]]

The Gradient of a function $f : \R \to \R^n$ can be seen as the transpose of the [[1.5 Jacobian Matrix|Jacobian]] said function,

$$
\nab f(x) = (Jf(x))^T
$$

as the matrix given by $Jf(x)$ will be a row vector of size $n$.

```ad-question
title: Deeper meaning of the shape of the Jacbian for single variable function

Does the shape of the [[1.5 Jacobian Matrix]] as a row vector have any connection with the concepts of [[Dual Space]] and [[Tensor|Tensors]]? Ie is it more "natural" as it were...

What happens if we combine $Jf(x)$ and $x$ in the prescribed manner for [[Dual Space|Dual Spaces]]?

#Further-Reading 
```
