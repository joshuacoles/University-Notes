# 4.6 Second Derivative Test

The first [[2.1 Derivative|Derivative]] on its own doesn't tell us whether a [[4.3 Critical Point|Critical Point]] is a [[4.2 Local Minima|Local Minima]], a [[4.1 Local Maxima|Local Maxima]], or a [[4.4 Saddle Point|Saddle Point]]. With the help of the [[3.1 Hessian Matrix|Hessian Matrix]], however, we can often find out.

> #todo move this into a Alg 1B/Alg 2A note.

We recall a few facts from linear algebra. Consider a symmetric $(n \times n)$-matrix $A$ with real entries. Then there exists a change of coordinates such that $A$ becomes diagonal. In particular, the following notions depend only on the eigenvalues.

1.  We say that $A$ is _positive definite_ if $v \cdot Av > 0$ for all $v \in \mathbb{R}^n \setminus \{0\}$. This is the case if, and only if, all eigenvalues of $A$ are positive. Indeed, if $\lambda_1 > 0$ is the smallest eigenvalue, then $v \cdot Av \ge \lambda_1 \|v\|^2$ for all $v \in \mathbb{R}^n$.
2.  We say that $A$ is _negative definite_ if $v \cdot Av < 0$ for all $v \in \mathbb{R}^n \setminus \{0\}$. This is the case if, and only if, all eigenvalues of $A$ are negative. Indeed, if $\lambda_n < 0$ is the greatest eigenvalue, then $v \cdot Av \le \lambda_n \|v\|^2$ for all $v \in \mathbb{R}^n$.
3.  We say that $A$ is _indefinite_ if there exists $v_1, v_2 \in \mathbb{R}^n$ such that $v_1 \cdot Av_1 > 0$ and $v_2 \cdot Av_2 < 0$. This is the case if, and only if, $A$ has positive and negative eigenvalues.

Note that these three categories do not cover all possible cases, as we may have $0$ as an eigenvalue.

## The Test

Let $U \sub \R^n$ be an [[1.1 Analytic Space]] and $f : U \to \R$ a function with [[Continuous]] [[1.2 Partial Derivative|Partial Derivatives]] up to **second order**. Suppose that $x \in U$ is a [[4.3 Critical Point|Critical Point]] of $f$.

1.  If $Hf(x)$ is [[Positive Definite]], then $x$ is a [[4.2 Local Minima|Local Minima]] of $f$.
2.  If $Hf(x)$ is [[Negative Definite]], then $x$ is a [[4.1 Local Maxima|Local Maxima]] of $f$.
3.  If $Hf(x)$ is [[Indefinite]], then $x$ is a [[4.4 Saddle Point|Saddle Point]] of $f$.

If $Hf(x)$ is neither positive definite nor negative define nor indefinite, then this test is **inconclusive**.

### Proof

Let $r > 0$ such that $B_r(x) \subseteq U$. By [[3.3 2nd order Taylor's Theorem in Higher Dimensions#Second Form]], there exists a function $R : B_r(x) \to \R$ such that

$$
f(y) = f(x) + \frac{1}{2} (y - x) \cdot Hf(x) (y - x) + R(y) \|y - x\|^2 \tag{4.1}
$$
^eq-4-1

for all $y \in B_r(x)$ and $\lim_{y \to x} R(y) = 0$.

If $Hf(x)$ is [[Positive Definite]], then choose $c > 0$ such that $v \cdot Hf(x) v \ge c\|v\|^2$ for all $v \in \R^n$. (For example, we may choose the smallest eigenvalue of $Hf(x)$.) 

Since $R(y) \to 0$ as $y \to x$, we may choose $s \in (0, r]$ such that $|R(y)| \le c/2$ for all $y \in B_s(x)$. Then

$$
\begin{align}
f(y)
&= 	f(x)
	+ \frac 12 (y - x) \cdot Hf(x) (y - x)
	+ R(y) \|y - x\|^2
\\
&\ge
	f(x)
	+ \frac c2 \|y - x\|^2 - \frac c2 \|y - x\|^2
\\
&\ge f(x)
\end{align}
$$

for all $y \in B_s(x)$. Therefore, we have a local minimum at $x$.

If $Hf(x)$ is negative definite, we can use the same arguments to conclude that we have a local maximum.

If $Hf(x)$ is indefinite, choose two eigenvectors $v_1, v_2 \in \mathbb{R}^n$ of $Hf(x)$ that belong to eigenvalues $\lambda_1 < 0$ and $\lambda_2 > 0$, respectively. Let $c = \min\{-\lambda_1, \lambda_2\}$ and choose $s \in (0, r]$ such that $|R(y)| \le c/4$ for all $y \in B_s(x)$.

Now insert $y = x + tv_1$ into [[#^eq-4-1]]. This gives

$$
f(y) = f(x) + \frac 12 t^2 \left(v_1 \cdot Hf(x) v_1 + 2R(y) \|v_1\|^2\right).
$$

If $|t|$ is chosen so small that $y \in B_s(x)$, then

$$
v_1 \cdot Hf(x) v_1 + 2R(y) \|v_1\|^2
\le
\lambda_1 \|v_1\|^2 + \frac{c}{2} \|v_1\|^2
\le
- \frac{c}{2} \|v_1\|^2 < 0.
$$

Hence $f(x + tv_1) < f(x)$ whenever $|t|$ is sufficiently small and $x$ cannot be a [[4.2 Local Minima]]. Further applying the same logic with $v_2$ we can show $x$ cannot be a [[4.1 Local Maxima|Local Maxima]].

## Example 1

Consider the function $f : \R^2 \to \R$ with $f(x) = x_1^2 - 3x_1 x_2 + 4x_2^2 + x_1 - 8x_2$ from [[4.5 Necessary Condition for Local Extrema#Example]]. We already know that there is a unique critical point at $x = (16/7, 13/7)$. Now we compute

$$
Hf(x) = \begin{pmatrix}
	2 & -3 \\
	-3 & 8
\end{pmatrix}.
$$

The eigenvalues are $5\pm 3\sqrt{2}$, which are both positive. Hence we have a local minimum.

## Example 2

Consider the function $f : \R^2 \to \R$ given by $f(x) = \cos(x_1) e^{x_2^2}$. We compute

$$
Jf(x) = e^{x_2^2} (-\sin(x_1), 2x_2 \cos(x_1))
$$

and

$$
Hf(x) = e^{x_2^2} \pmat{
	-\cos(x_1) & -2x_2 \sin(x_1) \\
	-2x_2\sin(x_1) & (4x_2^2 + 2) \cos(x_1)
}.
$$

The critical points satisfy the equations $-e^{x_2^2} \sin(x_1) = 0$ and $2e^{x_2^2} x_2 \cos(x_1) = 0$ simultaneously. As the exponential function has no zeroes and the functions $\sin$ and $\cos$ have no common zeroes, we must have $\sin(x_1) = 0$ and $x_2 = 0$. The solutions are of the form $(\ell \pi, 0)$ for $\ell \in \mathbb{Z}$.

At these critical points, we find that

$$
Hf(\ell \pi, 0) = (-1)^\ell \begin{pmatrix}
	-1 & 0 \\
	0 & 2
s\end{pmatrix}.
$$

The matrix $\left(\begin{smallmatrix} -1 & 0 \\ 0 & 2 \end{smallmatrix}\right)$ has a positive eigenvalue (namely $2$) and a negative one (namely $-1$). Hence it is indefinite, and the same applies when we multiply by $-1$. Therefore, all of the critical points are saddle points.
